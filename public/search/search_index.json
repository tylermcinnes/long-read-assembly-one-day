{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Long-read Assembly \u00b6 Workshop schedule Lesson overview Introduction to the workshop How are assemblies created? Familiarize Ourselves with the Data Assembly Assembly QC Assembly clean-up Phased assemblies in action Supplementary materials Supplementary Topic Supplementary 1 NeSI HPC Authentication Factor Setup and Jupyter Login Supplementary 2 NeSI File System, Working Directory, and Symlinks Supplementary 3 Jupyter Virtual Desktop Attribution notice Material used to prepare for the workshop was extracted from https://github.com/human-pangenomics/hprc-tutorials/tree/GA-workshop References Lorig-Roach, Ryan, et al. \"Phased nanopore assembly with Shasta and modular graph phasing with GFAse.\" bioRxiv (2023): 2023-02. Porubsky, David, et al. \"Fully phased human genome assembly without parental data using single-cell strand sequencing and long reads.\" Nature biotechnology 39.3 (2021): 302-308. Garg, Shilpa. \"Towards routine chromosome-scale haplotype-resolved reconstruction in cancer genomics.\" Nature Communications 14.1 (2023): 1358. Rautiainen, Mikko, et al. \"Telomere-to-telomere assembly of diploid chromosomes with Verkko.\" Nature Biotechnology (2023): 1-9. Rhie, Arang, et al. \"Merqury: reference-free quality, completeness, and phasing assessment for genome assemblies.\" Genome biology 21.1 (2020): 1-27.","title":"Home"},{"location":"#long-read-assembly","text":"Workshop schedule Lesson overview Introduction to the workshop How are assemblies created? Familiarize Ourselves with the Data Assembly Assembly QC Assembly clean-up Phased assemblies in action Supplementary materials Supplementary Topic Supplementary 1 NeSI HPC Authentication Factor Setup and Jupyter Login Supplementary 2 NeSI File System, Working Directory, and Symlinks Supplementary 3 Jupyter Virtual Desktop Attribution notice Material used to prepare for the workshop was extracted from https://github.com/human-pangenomics/hprc-tutorials/tree/GA-workshop References Lorig-Roach, Ryan, et al. \"Phased nanopore assembly with Shasta and modular graph phasing with GFAse.\" bioRxiv (2023): 2023-02. Porubsky, David, et al. \"Fully phased human genome assembly without parental data using single-cell strand sequencing and long reads.\" Nature biotechnology 39.3 (2021): 302-308. Garg, Shilpa. \"Towards routine chromosome-scale haplotype-resolved reconstruction in cancer genomics.\" Nature Communications 14.1 (2023): 1358. Rautiainen, Mikko, et al. \"Telomere-to-telomere assembly of diploid chromosomes with Verkko.\" Nature Biotechnology (2023): 1-9. Rhie, Arang, et al. \"Merqury: reference-free quality, completeness, and phasing assessment for genome assemblies.\" Genome biology 21.1 (2020): 1-27.","title":"Long-read Assembly"},{"location":"pages/0_intro/","text":"1. Introduction - Long Read Assembly Workshop \u00b6 This long read assembly workshop aims to work through an entire assembly workflow including data QC, assembly, and assembly QC. We Need More Human Genomes \u00b6 No single genome can represent the diversity in the human population. Using a single reference genome creates reference biases, adversely affecting variant discovery, gene\u2013disease association studies and the accuracy of genetic analyses. The Human Pangenome Reference Consortium (HPRC) is a project funded by the National Human Genome Research Institute (USA) to sequence and assemble genomes from individuals from diverse populations in order to better represent the genomic landscape of diverse human populations. Why Do Good Assemblies Matter? \u00b6 You can do things with assemblies that you can't with their constituent data. And the better the assembly, the more you can do. For example, the figure below shows a region of the genome with medically relevant loci (Prader Willi) that has regions of long repeats (segmental duplications) that have high sequence similarity. The assemblies on the left use older data and assembly methods can't resolve these loci (see all the gaps), while the assemblies on the right use PacBio HiFi data and Hifiasm with trio phasing to successfully resolve these loci, revealing a likely SV in a region affected by a lack of coverage with older sequencing technology. ( Porubsky et al. 2023 ) In this workshop, we will show you how to make assemblies with PacBio HiFi data, but we will also add in ultra-long Oxford Nanopore reads. The resulting assemblies will just as good as, if not better than, the assemblies shown on the right in the above image. Assembly Workflows & Our Approach \u00b6 The approach we take in this workshop will reflect a normal assembly workflow. Today we will learn about the type of data that goes into current assemblers, we will use two of the most popular and powerful assemblers currently available, and finally, we will take a look at the assemblies to assess their quality and see what we can do with them.","title":"1. Introduction - Long Read Assembly Workshop"},{"location":"pages/0_intro/#1-introduction-long-read-assembly-workshop","text":"This long read assembly workshop aims to work through an entire assembly workflow including data QC, assembly, and assembly QC.","title":"1. Introduction - Long Read Assembly Workshop"},{"location":"pages/0_intro/#we-need-more-human-genomes","text":"No single genome can represent the diversity in the human population. Using a single reference genome creates reference biases, adversely affecting variant discovery, gene\u2013disease association studies and the accuracy of genetic analyses. The Human Pangenome Reference Consortium (HPRC) is a project funded by the National Human Genome Research Institute (USA) to sequence and assemble genomes from individuals from diverse populations in order to better represent the genomic landscape of diverse human populations.","title":"We Need More Human Genomes"},{"location":"pages/0_intro/#why-do-good-assemblies-matter","text":"You can do things with assemblies that you can't with their constituent data. And the better the assembly, the more you can do. For example, the figure below shows a region of the genome with medically relevant loci (Prader Willi) that has regions of long repeats (segmental duplications) that have high sequence similarity. The assemblies on the left use older data and assembly methods can't resolve these loci (see all the gaps), while the assemblies on the right use PacBio HiFi data and Hifiasm with trio phasing to successfully resolve these loci, revealing a likely SV in a region affected by a lack of coverage with older sequencing technology. ( Porubsky et al. 2023 ) In this workshop, we will show you how to make assemblies with PacBio HiFi data, but we will also add in ultra-long Oxford Nanopore reads. The resulting assemblies will just as good as, if not better than, the assemblies shown on the right in the above image.","title":"Why Do Good Assemblies Matter?"},{"location":"pages/0_intro/#assembly-workflows-our-approach","text":"The approach we take in this workshop will reflect a normal assembly workflow. Today we will learn about the type of data that goes into current assemblers, we will use two of the most popular and powerful assemblers currently available, and finally, we will take a look at the assemblies to assess their quality and see what we can do with them.","title":"Assembly Workflows &amp; Our Approach"},{"location":"pages/1_read_inputs/","text":"2. Sequence Data \u00b6 Preamble: What Data Are We Using? Genome In A Bottle & HG002 In this workshop we will be using data from HG002, which is a reference sample from the Genome In A Bottle (GIAB) consortium. The GIAB project releases benchmark data for genomic characterization, and you may have seen their benchmark variant calls and regions out in the wild. As part of their benchmarking material generation, they release datasets for their reference samples. We will be using those in this workshop. Family Structure HG002 is actually part of a trio of reference samples. Below is the family listing, also known as the Ashkenazim trio: HG002: Son HG003: Father HG004: Mother If you'd like to use data from any of the Ashkenazim trio, there is a set of index files on the GIAB github . There is an excellent HG002 assembly available Since HG002 has so much data to compare against, it is pretty common for new technologies to benchmark on HG002\u2014making even more HG002 data. It has grown into a data ecosystem. This is part of the reason why it was chosen for the T2T consortium as the target for one of its next big pushes: a high-quality diploid T2T genome . This assembly has been worked on by many leading people in the field. And while it is undergoing polishing and is talked about as a draft, it is gapless outside of rDNA arrays and is very good. This makes HG002 a very good sample for testing assembly processes. You can always go back and compare your results against the draft from the T2T consortium. Data For Graph Building: PacBio & ONT Data \u00b6 We are going to start by introducing the two long read sequencing technologies that we will be using: PacBio's HiFi and Oxford Nanopore's Ultralong. These two technologies are complementary and each have their own strengths. You can answer some questions more easily with HiFi and some more easily with ONT UL. They can also be used together, and this is important for the concept of a hybrid assembly algorithm where accurate reads are used to create a draft assembly and long reads are used to extend that assembly. In this section, we will learn about both technologies and then create plots showing their characteristic read lengths and qualities. This will help us get a feel for what the data actually looks like in the wild. Lastly, we will prepare this data for use in our assembly section. As someone who is about to make an assembly, you have the most control over what type of data you put into the assembly algorithm. The more you know about the data, the better your assembly will be. PacBio Hifi: Illumina-Like Quality With Long Reads \u00b6 What is PacBio HiFi PacBio's high fidelity (or HiFi) reads are long (~15kb) and accurate (~99.9%). PacBio produces such high quality reads (with their single-molecule real-time, or SMRT, sequencing) by reading the same sequence over and over again in order to create a circular consensus sequence (or CCS) as shown below. PacBio's CCS Process Long, highly accurate reads allows for a number of analyses that were difficult or impossible in the context of short reads. For instance, variants can be more easily phased as you can just look for variants that are seen in the same sequencing read since HiFi read lengths span much more than short read lengths. In our context, long accurate reads allow assembly algorithms to build assembly graphs across difficult regions. But it turns out that HiFi reads aren't long enough to span exact repeats in regions like human centromeres. ONT Ultralong: Lower Quality But Really Long Reads \u00b6 Oxford Nanopore's ultralong (UL) sequencing has lower accuracy (~97%), but is really long (even longer than normal ONT). This is achieved though a different library prep\u2014as compared to normal DNA sequencing with ONT. UL library prep uses a transposase to cut DNA at non-specific sites where it can then be adapted for sequencing. ONT Ultralong Library Prep The time, transposase amount, and temperature are all factors that affect transposase activity. The more you cut, the shorter the reads. ONT's standard DNA library prep, on the other hand, shears DNA then ligates adapters. (If you've created DNA libraries using Illumina's TruSeq kits, then you get the idea.) These UL reads, while less accurate than HiFi, span tricky regions, which makes UL and HiFi data highly complementary, especially in the context of de novo assembly, as we will soon see.","title":"2. Sequence Data"},{"location":"pages/1_read_inputs/#2-sequence-data","text":"Preamble: What Data Are We Using? Genome In A Bottle & HG002 In this workshop we will be using data from HG002, which is a reference sample from the Genome In A Bottle (GIAB) consortium. The GIAB project releases benchmark data for genomic characterization, and you may have seen their benchmark variant calls and regions out in the wild. As part of their benchmarking material generation, they release datasets for their reference samples. We will be using those in this workshop. Family Structure HG002 is actually part of a trio of reference samples. Below is the family listing, also known as the Ashkenazim trio: HG002: Son HG003: Father HG004: Mother If you'd like to use data from any of the Ashkenazim trio, there is a set of index files on the GIAB github . There is an excellent HG002 assembly available Since HG002 has so much data to compare against, it is pretty common for new technologies to benchmark on HG002\u2014making even more HG002 data. It has grown into a data ecosystem. This is part of the reason why it was chosen for the T2T consortium as the target for one of its next big pushes: a high-quality diploid T2T genome . This assembly has been worked on by many leading people in the field. And while it is undergoing polishing and is talked about as a draft, it is gapless outside of rDNA arrays and is very good. This makes HG002 a very good sample for testing assembly processes. You can always go back and compare your results against the draft from the T2T consortium.","title":"2. Sequence Data"},{"location":"pages/1_read_inputs/#data-for-graph-building-pacbio-ont-data","text":"We are going to start by introducing the two long read sequencing technologies that we will be using: PacBio's HiFi and Oxford Nanopore's Ultralong. These two technologies are complementary and each have their own strengths. You can answer some questions more easily with HiFi and some more easily with ONT UL. They can also be used together, and this is important for the concept of a hybrid assembly algorithm where accurate reads are used to create a draft assembly and long reads are used to extend that assembly. In this section, we will learn about both technologies and then create plots showing their characteristic read lengths and qualities. This will help us get a feel for what the data actually looks like in the wild. Lastly, we will prepare this data for use in our assembly section. As someone who is about to make an assembly, you have the most control over what type of data you put into the assembly algorithm. The more you know about the data, the better your assembly will be.","title":"Data For Graph Building: PacBio &amp; ONT Data"},{"location":"pages/1_read_inputs/#pacbio-hifi-illumina-like-quality-with-long-reads","text":"What is PacBio HiFi PacBio's high fidelity (or HiFi) reads are long (~15kb) and accurate (~99.9%). PacBio produces such high quality reads (with their single-molecule real-time, or SMRT, sequencing) by reading the same sequence over and over again in order to create a circular consensus sequence (or CCS) as shown below. PacBio's CCS Process Long, highly accurate reads allows for a number of analyses that were difficult or impossible in the context of short reads. For instance, variants can be more easily phased as you can just look for variants that are seen in the same sequencing read since HiFi read lengths span much more than short read lengths. In our context, long accurate reads allow assembly algorithms to build assembly graphs across difficult regions. But it turns out that HiFi reads aren't long enough to span exact repeats in regions like human centromeres.","title":"PacBio Hifi: Illumina-Like Quality With Long Reads"},{"location":"pages/1_read_inputs/#ont-ultralong-lower-quality-but-really-long-reads","text":"Oxford Nanopore's ultralong (UL) sequencing has lower accuracy (~97%), but is really long (even longer than normal ONT). This is achieved though a different library prep\u2014as compared to normal DNA sequencing with ONT. UL library prep uses a transposase to cut DNA at non-specific sites where it can then be adapted for sequencing. ONT Ultralong Library Prep The time, transposase amount, and temperature are all factors that affect transposase activity. The more you cut, the shorter the reads. ONT's standard DNA library prep, on the other hand, shears DNA then ligates adapters. (If you've created DNA libraries using Illumina's TruSeq kits, then you get the idea.) These UL reads, while less accurate than HiFi, span tricky regions, which makes UL and HiFi data highly complementary, especially in the context of de novo assembly, as we will soon see.","title":"ONT Ultralong: Lower Quality But Really Long Reads"},{"location":"pages/2_familiarise_w_data/","text":"3. Familiarize Ourselves With The Data \u00b6 Let's get our hands on some data so we can see with our own eyes what HiFi and UL data look like. Start Into the Right Directory code cd ~/obss_2023/genome_assembly cd data Load modules code module load pigz/2.7 module load NanoComp/1.20.0-gimkl-2022a-Python-3.10.5 module load SAMtools/1.16.1-GCC-11.3.0 Subset Our Input Data In order to get a feel for the data, we only need a small portion of it. Pull the first few thousand reads of the HiFi reads and write them to new files. code zcat m64011_190830_220126.Q20.fastq.gz \\ | head -n 200000 \\ | pigz > hifi_50k_reads.fq.gz & Next, downsample the ONT UL reads, too. code samtools fastq -@2 \\ 03_08_22_R941_HG002_1_Guppy_6.1.2_5mc_cg_prom_sup.bam \\ | head -n 20000 \\ | pigz > ont_ul_5k_reads.fq.gz & Now let's compare the data We are going to use a tool called NanoComp. This tool can take in multiple FASTQs (or BAMs) and will create summary statistics and nice plots that show things like read length and quality scores. NanoComp has nano in the name, and has some ONT-specific functionality, but it can be used with PacBio data just fine. code NanoComp --fastq \\ hifi_50k_reads.fq.gz \\ ont_ul_5k_reads.fq.gz \\ --names PacBio_HiFi ONT_UL \\ --outdir nanocomp_hifi_vs_ul Once the run is complete (~2 minutes), navigate in your file browser to the folder that NanoComp just created and then click on the NanoComp-report.html file (near the bottom of the folder's contents) to open it. Take a look at the plots for log-transformed read lengths and basecall quality scores. (Note that you may have to click Trust HTML at the top of the page for the charts to display.) What is the range of Q-scores seen in HiFi data? The mean and median Q-scores are around 33 and 34, but there is a spread. The CCS process actually produces different data based on a number of different factors, including the number of times a molecule is read (also called subread passes). Raw CCS data is usually filtered for >Q20 reads at which point it is by convention called HiFi. (Note that some people use CCS data below Q20!) What percent of UL reads are over 100kb? This depends on the dataset but it is very common to see 30% of reads being over 100kb. The 100kb number gets passed around a lot because reads that are much longer than HiFi are when UL distinguishes itself. Cleaning Data For Assembly \u00b6 PacBio Adapter Trimming \u00b6 PacBio's CCS software attempts to identify adapters and remove them. This process is getting better all the time, but some datasets (especially older ones) can have adapters remaining. If this is the case, adapters can find their way into the assemblies. Run CutAdapt to check for adapter sequences in the downsampled data that we are currently using. (The results will print to stdout on your terminal screen.) code module load cutadapt/4.1-gimkl-2022a-Python-3.10.5 cutadapt \\ -b \"AAAAAAAAAAAAAAAAAATTAACGGAGGAGGAGGA;min_overlap=35\" \\ -b \"ATCTCTCTCTTTTCCTCCTCCTCCGTTGTTGTTGTTGAGAGAGAT;min_overlap=45\" \\ --discard-trimmed \\ -o /dev/null \\ hifi_50k_reads.fq.gz \\ -j 0 \\ --revcomp \\ -e 0.05 Notice that we are writing output to /dev/null . We are working on a subset of these reads so the runtime is reasonable. There is no need to hold onto the reads that we are filtering on, it is just a subset of the data. What do you think the two sequences that we are filtering out are? (hint: you can Google them) The first sequence is the primer and the second sequence is the hairpin adapter. You can see the hairpin by looking at the 5' and 3' ends and checking that they are reverse complements. Why can we get away with throwing away entire reads that contain adapter sequences? As you can see from the summary statistics from CutAdapt, not many reads in this dataset have adapters/primers. There is some concern about bias\u2014where we remove certain sequences from the genome assembly process. We've taken the filtered reads and aligned them to the genome and they didn't look like they were piling up in any one area. What would happen if we left adapter sequences in the reads? If there are enough adapters present, you can get entire contigs comprised of adapters. This is not the worst, actually, because they are easy to identify and remove wholesale. It is trickier (and this happens more often) when adapter sequences end up embedded in the final assemblies. If/when you upload assemblies to repositories like Genbank they check for these adapters and force you to mask them out with N's. This is confusing to users because it is common to use N's to signify gaps in scaffolded assemblies. So users don't know if they are looking at a scaffolded assembly or masked out sequence. ONT Read Length Filtering \u00b6 Hifiasm is often run with ONT data filtered to be over 50kb in length, so let's filter that data now to see how much of the data remains. code module load SeqKit/2.4.0 seqkit seq \\ -m 50000 \\ ont_ul_5k_reads.fq.gz \\ | pigz > ont_ul_5k_reads.50kb.fq.gz & Now we can quickly check how many reads are retained. code NanoComp --fastq \\ ont_ul_5k_reads.fq.gz \\ ont_ul_5k_reads.50kb.fq.gz \\ --names ONT_UL ONT_UL_50kb \\ --outdir nanocomp_ul_vs_ul_50kb When we filter for reads over 50kb, how many reads and total base pairs of DNA are filtered out? Over half of the reads are filtered out, but only about 25% of the data (see \"Total bases\") is filtered. This makes sense as the long reads contribute more bp per read. Verkko , a hybrid assembler, is typically run without having filtered by size, why do you think that is? Based on the answer to the last question, filtering an ultralong readset for >50kb reads does not reduce the overall size of the dataset very much. Phasing Data \u00b6 Now that we've introduced the data that creates the graphs, it's time to talk about data types that can phase them in order to produce fully phased diploid assemblies. At the moment the easiest and most effective way to phase human assemblies is with trio information. Meaning you sequence a sample, and then you also sequence its parents. You then look at which parts of the genome the sample inherited from one parent and not the other. This is done with kmer databases (DBs). In our case, we will use both Meryl (for Verkko) and yak (for hifiasm) so let's take a moment to learn about kmer DBs. Trio data: Meryl \u00b6 Meryl is a kmer counter that dates back to Celera. It creates kmer DBs, but it is also a toolset that you can use for finding kmers and manipulating kmer count sets. Meryl is to kmers what BedTools is to genomic regions. Today we want to use Meryl in the context of creating databases from PCR-free Illumina readsets. These can be used both during the assembly process and during the post-assembly QC. Helpful Background \u00b6 Meryl can be used to create hapmer DBs (haplotype + k-mer), which can be used as input for tools like Verkko and Merqury. Hapmer DBs are constructed from the k -mers that a child inherits from one parent and not the other. These k -mers are useful for phasing assemblies because if an assembler has two very similar sequences, it can look for maternal-specific k -mers and paternal-specific k -mers and use those to determine which haplotype to assign to each sequence. In the Venn diagram above, the maternal hapmer k -mers/DB are on the left-hand side (in the purple in red box). The paternal hapmer k -mers/DB are on the right-hand side (in the purple in blue box). Wait, what is phasing? Phasing is the process of saying two things are on the same haplotype ( i.e. , saying two blocks of sequence came from the maternal haplotype, or vice versa) One way you will hear us talk about phasing in this workshop is in the context of ultra long reads. In this case, we may have two heterozygous regions separated by a homozygous region. When an assembler is walking this graph, if there is no external information about haplotype, then the assembler doesn't have a way of knowing that certain blocks of sequence came from the same sequence. For example, in the bottom image, the assembler might walk from the top left block, into the homozygous block, and then down to the bottom right block, switching between the two haplotypes. However, if we can find a long read that maps to the top sequences in both, then we could say that these sequences come from the same haplotype. That is phasing. We can play a similar trick with parental data. If we find paternal markers in both sequences in the top, then we can say that they both come from the paternal haplotype. This is also phasing. Using Meryl \u00b6 Now create a small file to work with code zcat HG003_HiSeq30x_subsampled_R1.fastq.gz \\ | head -n 2000000 \\ | pigz > HG003_HiSeq30x_05M_reads_R1.fastq.gz Create a k -mer DB from an Illumina read set code module load Merqury/1.3-Miniconda3 meryl count \\ compress \\ k = 30 \\ threads = 2 \\ memory = 8 \\ HG003_HiSeq30x_05M_reads_R1.fastq.gz \\ output paternal_05M_compress.k30.meryl This should be pretty fast because we are just using a small amount of data to get a feel for the program. The output of Meryl is a folder that contains 64 index files and 64 data files. If you try and look at the data files you'll see that they aren't human readable. In order to look at the actual k -mers, you have to use Meryl to print them. Look at the k -mers code meryl print \\ greater-than 1 \\ paternal_05M_compress.k30.meryl \\ | head The first column is the k -mer and the second column is the count of that k -mer in the dataset. We are just looking at the first few here. Take a look at some statistics for the DB code meryl statistics \\ paternal_05M_compress.k30.meryl \\ | head -n 20 We see a lot of k -mers missing and the histogram (frequency column) has a ton of counts at 1. This makes sense for a heavily downsampled dataset. Great. We just got a feel for how to use Meryl in general on subset data. Now let's actually take a look at how to create Meryl DBs for Verkko assemblies. How would we run Meryl for Verkko? Here is what the Slurm script would look like: (Don't run this, it is slow! We have made these for you already.) code #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name meryl_run #SBATCH --cpus-per-task 32 #SBATCH --time 12:00:00 #SBATCH --mem 24G #SBATCH --partition milan #SBATCH --output slurmlogs/%x.%j.out #SBATCH --error slurmlogs/%x.%j.err module purge module load Merqury/1.3-Miniconda3 export MERQURY = $( dirname $( which merqury.sh )) ## Create mat/pat/child DBs meryl count compress k = 30 \\ threads = 32 memory = 96 \\ maternal.*fastq.gz \\ output maternal_compress.k30.meryl meryl count compress k = 30 \\ threads = 32 memory = 96 \\ paternal.*fastq.gz \\ output paternal_compress.k30.meryl meryl count compress k = 30 \\ threads = 32 memory = 96 \\ child.*fastq.gz output \\ child_compress.k30.meryl ## Create the hapmer DBs $MERQURY /trio/hapmers.sh \\ maternal_compress.k30.meryl \\ paternal_compress.k30.meryl \\ child_compress.k30.meryl Closing notes It should be noted that Meryl DBs used for assembly with Verkko and for base-level QC with Merqury are created differently. Here are the current recommendations for k -mer size and compression: Verkko: use k=30 and the compress command Merqury: use k=21 and do not include the compress command Why does Verkko use compressed Meryl DBs while Merqury does not? The biggest error type from long read sequencing comes from homopolymer repeats. So assembly graphs are typically constructed from homopolymer compressed data. After the assembly graph is created the homopolymers are added back in. Verkko compresses the HiFi reads for you, but you need to give it homopolymer compressed Meryl DBs so they play nicely together. Merqury on the other hand is used to assess the quality of the resultant assembly, so you want to keep those homopolymers in order to find errors in them. Why does Merqury use k=21 ? Larger K sizes give more conservative results, but this comes at a cost since you get lower effective coverage. For non-human species, if you know your genome size you can estimate an optimal K using Meryl itself . If you are wondering, Verkko uses k=30 in order to be \"conservative\". And at the time of writing this document, different species typically stick with k=30 . Though this hasn't been tested, so it may change in the future. Do Meryl DBs have to be created from Illumina data? Could HiFi data be used an an input to Meryl? They don't! You can create a Meryl DB from 10X data or HiFi data, for instance. The one caveat is that you want your input data to have a low error rate. So UL ONT data wouldn't work. Other things you could do with Meryl Here is an example of something you could do with Meryl: You can create a k -mer DB from an assembly You could then print all k -mers that are only present once (using meryl print equal-to 1 ) Then write those out to a bed file with meryl-lookup . Now you have \"painted\" all of the locations in the assembly with unique k -mers. That can be a handy thing to have lying around. Trio data: Yak \u00b6 Yak (Yet-Another Kmer Analyzer) is the kmer counter that we need for Hifiasm assemblies and to QC assemblies made with either assembler so let's learn about how to make yak dbs. In the Meryl section we subset R1, now subset R2 as well code zcat HG003_HiSeq30x_subsampled_R2.fastq.gz \\ | head -n 2000000 \\ | pigz > HG003_HiSeq30x_05M_reads_R2.fastq.gz Look up yak's github and figure out how to make a count/kmer db for this data Yak won't work on our Jupyter instances, so create a slurm script that has 32 cores and 96GB of memory. That way it will work on our subset data and it will also work on full size data -- you'd just have to extend the time variable in slurm. Click here for the answer Here is one way to call yak in a yak.sl script... #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name yak_run #SBATCH --cpus-per-task 32 #SBATCH --time 00:10:00 #SBATCH --mem 32G #SBATCH --partition milan #SBATCH --output slurmlogs/%x.%j.out #SBATCH --error slurmlogs/%x.%j.err module purge module load yak/0.1 yak count \\ -t32 \\ -b37 \\ -o HG003_subset.yak \\ < ( zcat HG003_HiSeq30x_05M_reads_R*.fastq.gz ) \\ < ( zcat HG003_HiSeq30x_05M_reads_R*.fastq.gz ) If you haven't already, execute your yak script using slurm (takes about 2 minutes). code sbatch yak.sl When you are done you get out a non-human readable file. It doesn't need to be compressed or decompressed, and nothing else needs to be done in order to use it. Closing remarks on yak If you have Illumina data for an entire trio (which we do), then you can use yak to make yak DBs for each parent separately to use hifiasm for trio assembly or yak trioeval for quality control (more on that later) You don't need to homopolymer compress yak dbs There is no need to create separate dbs for assembly and for QC yak can perform a variety of assembly QC tasks (as we will see) but it isn't really designed to play around with kmers like Meryl is Hi-C \u00b6 Hi-C is a proximity ligation method. It takes intact chromatin and locks it in place, cuts up the DNA, ligates strands that are nearby and then makes libraries from them. It's easiest to just take a look at a cartoon of the process. Given that Hi-C ligates molecules that are proximate (nearby) to each other, it can be used for spatial genomics applications. In assembly, we take advantage of the fact that most nearby molecules are on the same strand (or haplotype) of DNA. What are the advantages of trio phasing over Hi-C? Trio data is great for phasing because you can assign haplotypes to maternal and paternal bins. This has the added benefit of assigning all maternal contigs to the same assembly. Hi-C ensure that an entire chromosome is phased into one haplotype, but across chromosomes the assignment is random. So why wouldn't you always use trio data for phasing? It can be hard to get trio data. If a sample has already been collected it may be hard to go back and identify the parents and collect sample from them. In non-human samples, trios can also be difficult particularly with samples taken from the wild. Are there any difficulties in preparing Hi-C data? Yes! As you can see in the cartoon above Hi-C relies on having intact chromatin as an input, which means it needs whole, non-lysed cells. This means that cell lines are an excellent input source, but frozen blood is less good, for instance. Other (Phasing) Datatypes \u00b6 We should also mention that there are other datatypes that can be used for phasing, though they are less common. Pore-C Pore-C is a variant of Hi-C which retains the chromatin conformation capture aspect, but the sequencing is done on ONT. This allows long-read sequencing of concatemers. Where Hi-C typically has at most one \"contact\" per read, Pore-C can have many contacts per read. The libraries also do not need to be amplified, so Pore-C reads can carry base modification calls. StrandSeq StrandSeq is a technique that creates sparse Illumina datasets that are both cell- and strand-specific. Cell specificity is achieved by putting one cell per well into 384-well plates (often multiple). Strand specificity is achieved through selective fragmentation of nascent strands. (During DNA replication, BrdU is incorporated exclusively into nascent DNA strands. In the library preparation the BrdU strand is fragmented and only the other strand amplifies.) This strand specificity gives another way to identify haplotype-specific kmers and use them during assembly phasing. If you are interested in these phasing approaches, you can read more about them in the following articles:","title":"3. Familiarize Ourselves With The Data"},{"location":"pages/2_familiarise_w_data/#3-familiarize-ourselves-with-the-data","text":"Let's get our hands on some data so we can see with our own eyes what HiFi and UL data look like. Start Into the Right Directory code cd ~/obss_2023/genome_assembly cd data Load modules code module load pigz/2.7 module load NanoComp/1.20.0-gimkl-2022a-Python-3.10.5 module load SAMtools/1.16.1-GCC-11.3.0 Subset Our Input Data In order to get a feel for the data, we only need a small portion of it. Pull the first few thousand reads of the HiFi reads and write them to new files. code zcat m64011_190830_220126.Q20.fastq.gz \\ | head -n 200000 \\ | pigz > hifi_50k_reads.fq.gz & Next, downsample the ONT UL reads, too. code samtools fastq -@2 \\ 03_08_22_R941_HG002_1_Guppy_6.1.2_5mc_cg_prom_sup.bam \\ | head -n 20000 \\ | pigz > ont_ul_5k_reads.fq.gz & Now let's compare the data We are going to use a tool called NanoComp. This tool can take in multiple FASTQs (or BAMs) and will create summary statistics and nice plots that show things like read length and quality scores. NanoComp has nano in the name, and has some ONT-specific functionality, but it can be used with PacBio data just fine. code NanoComp --fastq \\ hifi_50k_reads.fq.gz \\ ont_ul_5k_reads.fq.gz \\ --names PacBio_HiFi ONT_UL \\ --outdir nanocomp_hifi_vs_ul Once the run is complete (~2 minutes), navigate in your file browser to the folder that NanoComp just created and then click on the NanoComp-report.html file (near the bottom of the folder's contents) to open it. Take a look at the plots for log-transformed read lengths and basecall quality scores. (Note that you may have to click Trust HTML at the top of the page for the charts to display.) What is the range of Q-scores seen in HiFi data? The mean and median Q-scores are around 33 and 34, but there is a spread. The CCS process actually produces different data based on a number of different factors, including the number of times a molecule is read (also called subread passes). Raw CCS data is usually filtered for >Q20 reads at which point it is by convention called HiFi. (Note that some people use CCS data below Q20!) What percent of UL reads are over 100kb? This depends on the dataset but it is very common to see 30% of reads being over 100kb. The 100kb number gets passed around a lot because reads that are much longer than HiFi are when UL distinguishes itself.","title":"3. Familiarize Ourselves With The Data"},{"location":"pages/2_familiarise_w_data/#cleaning-data-for-assembly","text":"","title":"Cleaning Data For Assembly"},{"location":"pages/2_familiarise_w_data/#pacbio-adapter-trimming","text":"PacBio's CCS software attempts to identify adapters and remove them. This process is getting better all the time, but some datasets (especially older ones) can have adapters remaining. If this is the case, adapters can find their way into the assemblies. Run CutAdapt to check for adapter sequences in the downsampled data that we are currently using. (The results will print to stdout on your terminal screen.) code module load cutadapt/4.1-gimkl-2022a-Python-3.10.5 cutadapt \\ -b \"AAAAAAAAAAAAAAAAAATTAACGGAGGAGGAGGA;min_overlap=35\" \\ -b \"ATCTCTCTCTTTTCCTCCTCCTCCGTTGTTGTTGTTGAGAGAGAT;min_overlap=45\" \\ --discard-trimmed \\ -o /dev/null \\ hifi_50k_reads.fq.gz \\ -j 0 \\ --revcomp \\ -e 0.05 Notice that we are writing output to /dev/null . We are working on a subset of these reads so the runtime is reasonable. There is no need to hold onto the reads that we are filtering on, it is just a subset of the data. What do you think the two sequences that we are filtering out are? (hint: you can Google them) The first sequence is the primer and the second sequence is the hairpin adapter. You can see the hairpin by looking at the 5' and 3' ends and checking that they are reverse complements. Why can we get away with throwing away entire reads that contain adapter sequences? As you can see from the summary statistics from CutAdapt, not many reads in this dataset have adapters/primers. There is some concern about bias\u2014where we remove certain sequences from the genome assembly process. We've taken the filtered reads and aligned them to the genome and they didn't look like they were piling up in any one area. What would happen if we left adapter sequences in the reads? If there are enough adapters present, you can get entire contigs comprised of adapters. This is not the worst, actually, because they are easy to identify and remove wholesale. It is trickier (and this happens more often) when adapter sequences end up embedded in the final assemblies. If/when you upload assemblies to repositories like Genbank they check for these adapters and force you to mask them out with N's. This is confusing to users because it is common to use N's to signify gaps in scaffolded assemblies. So users don't know if they are looking at a scaffolded assembly or masked out sequence.","title":"PacBio Adapter Trimming"},{"location":"pages/2_familiarise_w_data/#ont-read-length-filtering","text":"Hifiasm is often run with ONT data filtered to be over 50kb in length, so let's filter that data now to see how much of the data remains. code module load SeqKit/2.4.0 seqkit seq \\ -m 50000 \\ ont_ul_5k_reads.fq.gz \\ | pigz > ont_ul_5k_reads.50kb.fq.gz & Now we can quickly check how many reads are retained. code NanoComp --fastq \\ ont_ul_5k_reads.fq.gz \\ ont_ul_5k_reads.50kb.fq.gz \\ --names ONT_UL ONT_UL_50kb \\ --outdir nanocomp_ul_vs_ul_50kb When we filter for reads over 50kb, how many reads and total base pairs of DNA are filtered out? Over half of the reads are filtered out, but only about 25% of the data (see \"Total bases\") is filtered. This makes sense as the long reads contribute more bp per read. Verkko , a hybrid assembler, is typically run without having filtered by size, why do you think that is? Based on the answer to the last question, filtering an ultralong readset for >50kb reads does not reduce the overall size of the dataset very much.","title":"ONT Read Length Filtering"},{"location":"pages/2_familiarise_w_data/#phasing-data","text":"Now that we've introduced the data that creates the graphs, it's time to talk about data types that can phase them in order to produce fully phased diploid assemblies. At the moment the easiest and most effective way to phase human assemblies is with trio information. Meaning you sequence a sample, and then you also sequence its parents. You then look at which parts of the genome the sample inherited from one parent and not the other. This is done with kmer databases (DBs). In our case, we will use both Meryl (for Verkko) and yak (for hifiasm) so let's take a moment to learn about kmer DBs.","title":"Phasing Data"},{"location":"pages/2_familiarise_w_data/#trio-data-meryl","text":"Meryl is a kmer counter that dates back to Celera. It creates kmer DBs, but it is also a toolset that you can use for finding kmers and manipulating kmer count sets. Meryl is to kmers what BedTools is to genomic regions. Today we want to use Meryl in the context of creating databases from PCR-free Illumina readsets. These can be used both during the assembly process and during the post-assembly QC.","title":"Trio data: Meryl"},{"location":"pages/2_familiarise_w_data/#helpful-background","text":"Meryl can be used to create hapmer DBs (haplotype + k-mer), which can be used as input for tools like Verkko and Merqury. Hapmer DBs are constructed from the k -mers that a child inherits from one parent and not the other. These k -mers are useful for phasing assemblies because if an assembler has two very similar sequences, it can look for maternal-specific k -mers and paternal-specific k -mers and use those to determine which haplotype to assign to each sequence. In the Venn diagram above, the maternal hapmer k -mers/DB are on the left-hand side (in the purple in red box). The paternal hapmer k -mers/DB are on the right-hand side (in the purple in blue box). Wait, what is phasing? Phasing is the process of saying two things are on the same haplotype ( i.e. , saying two blocks of sequence came from the maternal haplotype, or vice versa) One way you will hear us talk about phasing in this workshop is in the context of ultra long reads. In this case, we may have two heterozygous regions separated by a homozygous region. When an assembler is walking this graph, if there is no external information about haplotype, then the assembler doesn't have a way of knowing that certain blocks of sequence came from the same sequence. For example, in the bottom image, the assembler might walk from the top left block, into the homozygous block, and then down to the bottom right block, switching between the two haplotypes. However, if we can find a long read that maps to the top sequences in both, then we could say that these sequences come from the same haplotype. That is phasing. We can play a similar trick with parental data. If we find paternal markers in both sequences in the top, then we can say that they both come from the paternal haplotype. This is also phasing.","title":"Helpful Background"},{"location":"pages/2_familiarise_w_data/#using-meryl","text":"Now create a small file to work with code zcat HG003_HiSeq30x_subsampled_R1.fastq.gz \\ | head -n 2000000 \\ | pigz > HG003_HiSeq30x_05M_reads_R1.fastq.gz Create a k -mer DB from an Illumina read set code module load Merqury/1.3-Miniconda3 meryl count \\ compress \\ k = 30 \\ threads = 2 \\ memory = 8 \\ HG003_HiSeq30x_05M_reads_R1.fastq.gz \\ output paternal_05M_compress.k30.meryl This should be pretty fast because we are just using a small amount of data to get a feel for the program. The output of Meryl is a folder that contains 64 index files and 64 data files. If you try and look at the data files you'll see that they aren't human readable. In order to look at the actual k -mers, you have to use Meryl to print them. Look at the k -mers code meryl print \\ greater-than 1 \\ paternal_05M_compress.k30.meryl \\ | head The first column is the k -mer and the second column is the count of that k -mer in the dataset. We are just looking at the first few here. Take a look at some statistics for the DB code meryl statistics \\ paternal_05M_compress.k30.meryl \\ | head -n 20 We see a lot of k -mers missing and the histogram (frequency column) has a ton of counts at 1. This makes sense for a heavily downsampled dataset. Great. We just got a feel for how to use Meryl in general on subset data. Now let's actually take a look at how to create Meryl DBs for Verkko assemblies. How would we run Meryl for Verkko? Here is what the Slurm script would look like: (Don't run this, it is slow! We have made these for you already.) code #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name meryl_run #SBATCH --cpus-per-task 32 #SBATCH --time 12:00:00 #SBATCH --mem 24G #SBATCH --partition milan #SBATCH --output slurmlogs/%x.%j.out #SBATCH --error slurmlogs/%x.%j.err module purge module load Merqury/1.3-Miniconda3 export MERQURY = $( dirname $( which merqury.sh )) ## Create mat/pat/child DBs meryl count compress k = 30 \\ threads = 32 memory = 96 \\ maternal.*fastq.gz \\ output maternal_compress.k30.meryl meryl count compress k = 30 \\ threads = 32 memory = 96 \\ paternal.*fastq.gz \\ output paternal_compress.k30.meryl meryl count compress k = 30 \\ threads = 32 memory = 96 \\ child.*fastq.gz output \\ child_compress.k30.meryl ## Create the hapmer DBs $MERQURY /trio/hapmers.sh \\ maternal_compress.k30.meryl \\ paternal_compress.k30.meryl \\ child_compress.k30.meryl Closing notes It should be noted that Meryl DBs used for assembly with Verkko and for base-level QC with Merqury are created differently. Here are the current recommendations for k -mer size and compression: Verkko: use k=30 and the compress command Merqury: use k=21 and do not include the compress command Why does Verkko use compressed Meryl DBs while Merqury does not? The biggest error type from long read sequencing comes from homopolymer repeats. So assembly graphs are typically constructed from homopolymer compressed data. After the assembly graph is created the homopolymers are added back in. Verkko compresses the HiFi reads for you, but you need to give it homopolymer compressed Meryl DBs so they play nicely together. Merqury on the other hand is used to assess the quality of the resultant assembly, so you want to keep those homopolymers in order to find errors in them. Why does Merqury use k=21 ? Larger K sizes give more conservative results, but this comes at a cost since you get lower effective coverage. For non-human species, if you know your genome size you can estimate an optimal K using Meryl itself . If you are wondering, Verkko uses k=30 in order to be \"conservative\". And at the time of writing this document, different species typically stick with k=30 . Though this hasn't been tested, so it may change in the future. Do Meryl DBs have to be created from Illumina data? Could HiFi data be used an an input to Meryl? They don't! You can create a Meryl DB from 10X data or HiFi data, for instance. The one caveat is that you want your input data to have a low error rate. So UL ONT data wouldn't work. Other things you could do with Meryl Here is an example of something you could do with Meryl: You can create a k -mer DB from an assembly You could then print all k -mers that are only present once (using meryl print equal-to 1 ) Then write those out to a bed file with meryl-lookup . Now you have \"painted\" all of the locations in the assembly with unique k -mers. That can be a handy thing to have lying around.","title":"Using Meryl"},{"location":"pages/2_familiarise_w_data/#trio-data-yak","text":"Yak (Yet-Another Kmer Analyzer) is the kmer counter that we need for Hifiasm assemblies and to QC assemblies made with either assembler so let's learn about how to make yak dbs. In the Meryl section we subset R1, now subset R2 as well code zcat HG003_HiSeq30x_subsampled_R2.fastq.gz \\ | head -n 2000000 \\ | pigz > HG003_HiSeq30x_05M_reads_R2.fastq.gz Look up yak's github and figure out how to make a count/kmer db for this data Yak won't work on our Jupyter instances, so create a slurm script that has 32 cores and 96GB of memory. That way it will work on our subset data and it will also work on full size data -- you'd just have to extend the time variable in slurm. Click here for the answer Here is one way to call yak in a yak.sl script... #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name yak_run #SBATCH --cpus-per-task 32 #SBATCH --time 00:10:00 #SBATCH --mem 32G #SBATCH --partition milan #SBATCH --output slurmlogs/%x.%j.out #SBATCH --error slurmlogs/%x.%j.err module purge module load yak/0.1 yak count \\ -t32 \\ -b37 \\ -o HG003_subset.yak \\ < ( zcat HG003_HiSeq30x_05M_reads_R*.fastq.gz ) \\ < ( zcat HG003_HiSeq30x_05M_reads_R*.fastq.gz ) If you haven't already, execute your yak script using slurm (takes about 2 minutes). code sbatch yak.sl When you are done you get out a non-human readable file. It doesn't need to be compressed or decompressed, and nothing else needs to be done in order to use it. Closing remarks on yak If you have Illumina data for an entire trio (which we do), then you can use yak to make yak DBs for each parent separately to use hifiasm for trio assembly or yak trioeval for quality control (more on that later) You don't need to homopolymer compress yak dbs There is no need to create separate dbs for assembly and for QC yak can perform a variety of assembly QC tasks (as we will see) but it isn't really designed to play around with kmers like Meryl is","title":"Trio data: Yak"},{"location":"pages/2_familiarise_w_data/#hi-c","text":"Hi-C is a proximity ligation method. It takes intact chromatin and locks it in place, cuts up the DNA, ligates strands that are nearby and then makes libraries from them. It's easiest to just take a look at a cartoon of the process. Given that Hi-C ligates molecules that are proximate (nearby) to each other, it can be used for spatial genomics applications. In assembly, we take advantage of the fact that most nearby molecules are on the same strand (or haplotype) of DNA. What are the advantages of trio phasing over Hi-C? Trio data is great for phasing because you can assign haplotypes to maternal and paternal bins. This has the added benefit of assigning all maternal contigs to the same assembly. Hi-C ensure that an entire chromosome is phased into one haplotype, but across chromosomes the assignment is random. So why wouldn't you always use trio data for phasing? It can be hard to get trio data. If a sample has already been collected it may be hard to go back and identify the parents and collect sample from them. In non-human samples, trios can also be difficult particularly with samples taken from the wild. Are there any difficulties in preparing Hi-C data? Yes! As you can see in the cartoon above Hi-C relies on having intact chromatin as an input, which means it needs whole, non-lysed cells. This means that cell lines are an excellent input source, but frozen blood is less good, for instance.","title":"Hi-C"},{"location":"pages/2_familiarise_w_data/#other-phasing-datatypes","text":"We should also mention that there are other datatypes that can be used for phasing, though they are less common. Pore-C Pore-C is a variant of Hi-C which retains the chromatin conformation capture aspect, but the sequencing is done on ONT. This allows long-read sequencing of concatemers. Where Hi-C typically has at most one \"contact\" per read, Pore-C can have many contacts per read. The libraries also do not need to be amplified, so Pore-C reads can carry base modification calls. StrandSeq StrandSeq is a technique that creates sparse Illumina datasets that are both cell- and strand-specific. Cell specificity is achieved by putting one cell per well into 384-well plates (often multiple). Strand specificity is achieved through selective fragmentation of nascent strands. (During DNA replication, BrdU is incorporated exclusively into nascent DNA strands. In the library preparation the BrdU strand is fragmented and only the other strand amplifies.) This strand specificity gives another way to identify haplotype-specific kmers and use them during assembly phasing. If you are interested in these phasing approaches, you can read more about them in the following articles:","title":"Other (Phasing) Datatypes"},{"location":"pages/3_assembly/","text":"4. Assembly \u00b6 Here is a rundown of what we will do: Learn about how Verkko creates assemblies Run Hifiasm with test data (HiFi only) Run Verkko with test data (HiFi + ONT) See how to run both with full datasets Compare the two assemblers Talk about how much data we need At the end of the day you will hopefully have a feel for how to actually run each assembler, what data to give them, and when to choose one over the other. Theoretical Walkthrough Of The Assembly Process \u00b6 Verkko \u00b6 In this section we will go over the rough outline of Verkko's approach to assembly. Hopefully this will help put the data types from yesterday in context. Knowing how each data type is used also helps you to make better decisions when planning your sequencing runs. Both Verkko and Hifiasm can use a variety of data sources: PacBio HiFi: >10kbp, around 99.9% accuracy Oxford Nanopore Ultralong: >100kb, around 97% accuracy Phasing data from Hi-C or trio Illumina data PacBio HiFi data is required for both assemblers. Other data types are optional\u2014but they lead to much better assemblies. So let's jump ahead a bit and take a peek at how Verkko creates assemblies using figure 1 from the recent Verkko paper (Rautiainen, Mikko, et al.). It's ok if this is a bit confusing, you don't need to know the inner workings of Verkko in order to make great assemblies. PacBio HiFi is used to create the initial graph Verkko's first theoretical task is to create an assembly graph from HiFi data, but it has to prepare the HiFi reads first. HiFi data is less accurate in homopolymer repeats and microsatellites, so before creating an assembly graph, the reads are \"compressed\" in these regions: The reads are then error corrected. You don't have to worry about how this works (just know that it is very computationally intensive). Once that is done, a graph is created from the HiFi reads: If you aren't familiar with what an assembly graph is, that is also ok! The annoying thing is that there are different ways to make assembly graphs, but they all have the common feature of linking together reads by their overlaps. In this graph the boxes (also called nodes) represent sequences and the lines (also called edges) represent the relationship between those overlaps. Note that in the middle section the orange and blue lines represent the ONT reads aligned to the graph. Oxford Nanopore Data (which is long) helps simplify the graph Using these alignments, nodes that are linked (or phased) by a read are combined. This \"simplifies\" the graph -- in other words, you get nice long nodes where you previously had shorter nodes. (Long nodes are good, they mean you have longer sequences that are assembled.) The graph can now be phased In the case of trio Illumina data, Verkko looks at the nodes and counts the number of maternal-specific or paternal-specific sequences of DNA (from meryl hapmer DBs). If it finds that a node has, for instance, a bunch of maternal specific sequences/ k -mers and almost no paternal specific sequences/ k -mers then the assembler will assign that node to be maternal. Nodes that are the same haplotype but are separated by a homozygous region are then merged. Finally the assembly graph can be converted into two contigs which represent maternal and paternal haplotypes. Maternal and paternal contigs for the entire assembly are then put into one diploid FASTA as well as two haploid FASTAs. How does Hi-C phasing work? Like using trio data, Hi-C phasing aims to find nodes that are near to each other and come from the same haplotype. To achieve this, Hi-C data is aligned to the graph and reads that are linked across nodes can be used to phase the graph as shown in this figure modified from (Garg, Shilpa): One key difference between trio phasing and Hi-C is that Hi-C data cannot say that a set of nodes all come from the sample's mother or father, only that they come from the same haplotype. Exploring With Test Data \u00b6 Running assemblers is very computationally intensive and the output files can be big. Let's not jump straight into assembling human genomes. Instead we can use the test data that both assemblers provide as a way to both ensure that we know how to run the tool (which is easy) and we can start to get a feel for the process and outputs in real life. Run Hifiasm With Test Data \u00b6 Create a directory code cd ~/obss_2023/genome_assembly/ mkdir -p assembly/hifiasm_test cd assembly/hifiasm_test Now download Hifiasm's test data code wget https://github.com/chhylp123/hifiasm/releases/download/v0.7/chr11-2M.fa.gz This is HiFi data from about 2 million bases of chromosome 11. HiFi data is the only required data type for Hifiasm and Verkko. You can create assemblies from only HiFi data and you can add ONT and phasing later. Also notice that this data is in FASTA format! Presumably this is to make the file smaller since this is test data. Now let's load the Hifiasm module code module purge module load hifiasm And actually run the test data code hifiasm \\ -o test \\ -t4 \\ -f0 \\ chr11-2M.fa.gz \\ 2 > test.log & code head -n 60 test.log Now check the Hifiasm log interpretation section of the documentation to give that some context. What does the histogram represent, and how many peaks do you expect? The histogram represents the k -mer count in the HiFi reads. For humans we expect to see a large peak somewhere around our expected sequencing coverage: this represents homozygous k -mers. The smaller peak represents heterozygous k -mers. Now ls the directory to see what outputs are present. What do you see? No FASTA files, but there are a lot of files that end in gfa . If you haven't seen these before, then we get to introduce you to another file format! Introduction To GFA Files \u00b6 GFA stands for Graphical Fragment Alignment and Hifiasm outputs assemblies in GFA files. GFAs aren't like bed or sam files which have one entry per line (or FASTA/Q that have 2/4 lines per entry). But this is bioinformatics, so you can rest assured that it is just a text file with a new file extension. It's easiest to just look at an example of a GFA file from the spec: H VN:Z:1.0 S 11 ACCTT S 12 TCAAGG S 13 CTTGATT L 11 + 12 - 4M L 12 - 13 + 5M L 11 + 13 + 3M P 14 11 +,12-,13+ 4M,5M Here we see the following line types H (Header): File header. You get the idea. The example here the header is just saying that the file follows GFA 1.0 spec. Notice that this line follows a TAG:TYPE:VALUE convention. Type in this case is Z which corresponds to printable string. S (Segment): A sequence of DNA This is what we care about for the moment! L (Link): Overlap between two segments We can read the first Link line as saying that the end of Segment 11 (+) connects to the beginning of Segment 12 (-) and the overlap is 4 matching bases. In this case it would look like this: ACCTT ( Segment 11 ) |||| GGAACT ( Segment 12 -- reversed ) P (Path): Ordered list of segments (connected by links) So how do we get a FASTA from a GFA? To get a FASTA we just pull the S lines from a GFA and print them to a file: code awk '/^S/{print \">\"$2;print $3}' \\ test.bp.p_ctg.gfa \\ > test.p_ctg.fa You can read this awk command as: Give me all input lines that start with S Then print the second column of those lines (which is the sequence ID) Also print another line with the actual sequence Why does Hifiasm output GFAs and not FASTAs? Hifiasm (and many other assemblers) use GFAs while they are actually assembling. The GFA represents/stores the assembly graph. Hifiasm probably doesn't output FASTAs just because everything in the FASTA is contained in the GFA, so why store it twice? View Hifiasm Test Assembly GFA in Bandage \u00b6 We are going to take a look at the assembly GFA file in a browser called Bandage. Bandage provides a way to visualize something called unitig graphs. Start Bandage Open Jupyter Virtual Desktop according to these instructions In the Virtual Desktop, click on the terminal emulator icon (in your toolbar at the bottom of your screen) Load the Bandage module with module load Bandage Type Bandage & to start Bandage Load a unitig GFA Click the File dropdown then Load Graph Navigate to our current folder ( cd ~/obss_2023/genome_assembly/assembly/hifiasm_test ) Select the test.bp.r_utg.noseq.gfa file and press the Open icon Under Graph Drawing on the left-hand side click Draw Graph Ok, so what are we looking at? The thick lines are nodes\u2014which in this case represent sequences. Since we loaded the unitig graph the sequences are unitigs. A unitig is a high confidence contig. It is a place where the assembler says \"I know exactly what is going on here\". The ends of unitigs are where it gets messy. At the ends, an assembler has choices to make about which unitig(s) to connect to next. Now load a contig GFA Open the test.bp.p_ctg.noseq.gfa file to see how boring it is. In general, when using Bandage people look at the unitig GFAs (not contig GFAs). An assembly is a hypothesis, and the contigs output by the assembler are its best guess at the correct haplotype sequence. The contigs don't show much information about the decisions being made, however. They are the output. We view unitig GFAs so we can see the data structure at the point that the assembler was making tough decisions. Here are some things you can do with Bandage Let's say you mapped a sample's ONT reads back onto that sample's de novo assembly and have identified a misjoin. You can open up bandage and find that unitigs that went into the contig to see if it can be easily manually broken. If you have a phased diploid assembly with a large sequence that is missing, you can look at the unitig gfa, color the nodes by haplotype, and see which sequences are omitted. Those sequences can then be analyzed and manually added into the final assembly. You can label nodes with (HiFi) coverage and inspect regions with low quality too see if they have low coverage as well. If so, you might want to throw them out. (This does happen, in particular for small contigs that assemblers tend to output.) Run Verkko With Test Data \u00b6 Create a directory code cd ~/obss_2023/genome_assembly mkdir -p assembly/verkko_test cd assembly/verkko_test Now download Verkko's test data code curl -L https://obj.umiacs.umd.edu/sergek/shared/ecoli_hifi_subset24x.fastq.gz -o hifi.fastq.gz curl -L https://obj.umiacs.umd.edu/sergek/shared/ecoli_ont_subset50x.fastq.gz -o ont.fastq.gz You can see that this dataset is for E. coli and there is both HiFi and ONT data included. Create Slurm script for test Verkko run code Start your favourite text editor nano verkko_test.sl And then paste in the following #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --partition milan #SBATCH --job-name test_verkko #SBATCH --cpus-per-task 8 #SBATCH --time 00:15:00 #SBATCH --mem 12G #SBATCH --output slurmlogs/%x.%j.log #SBATCH --error slurmlogs/%x.%j.err ## load modules module purge module load verkko/1.3.1-Miniconda3 ## run verkko verkko \\ -d assembly \\ --hifi ./hifi.fastq.gz \\ --nano ./ont.fastq.gz Run verkko test sbatch verkko_test.sl This should only take a few minutes to complete. You can keep track of the run with the squeue command. code squeue --me How does Verkko run? It turns out that if you run Verkko more than once or twice you will have to know a bit about how it is constructed. Verkko is a program that reads in the parameters you gave it and figures out a few things about your verkko installation and then creates a configuration file ( verkko.yml ) and a shell script ( snakemake.sh ). The shell script is then automatically executed. Take a look at the shell script that was created for your run code cat assembly/snakemake.sh It is just a call to Snakemake!!! You can think of Verkko as a tool, but also as a pipeline because it is. This has some advantages. One is that if you know what Verkko is doing (which is somewhat achievable given that the Snakemake rules guide you through Verkko's logic), you can add to it, or even swap out how Verkko performs a given step for how you'd like to do it. It also means that you can restart a run at any given step (if you made a mistake or if the run failed). Lastly, and maybe most importantly, Snakemake supports Slurm as a backend. So if you have access to an HPC you could (and probably should) run Verkko and allow it to launch Slurm jobs for you. (This is in contrast to what we just did which was to run a Slurm job and just allow all jobs to run on the allocated resources that we requested for the entire run.) Now take a look at the jobs that were run You can view the stderr from the run in your Slurm logs, or in Snakemake's logs. Let's take a look at the top of the log: code head -n 35 assembly/.snakemake/log/*.log This shows a list of Snakemake jobs that will get executed for this dataset. There are a few things to note. The first is that for larger datasets some jobs will get executed many times (hence the count column). This dataset is small, so most jobs have count=1 . The second thing to note is that these jobs are sorted alphabetically, so we can get a feel for scale, but it's a bit hard to figure out what Verkko is really doing. Open the logs and scroll through them code less assembly/.snakemake/log/*.log You can see all of the Snakemake jobs, in order, that were run. Even for this tiny dataset there are many. Since there are a lot of jobs, there are a lot of outputs, and these are organized (roughly) by Snakemake rule. Take a look at the output folder in order to familiarize yourself with the layout. code ls -lh assembly Take a look at the initial HiFi graph Open the assembly/1-buildGraph/hifi-resolved.gfa file in Bandage. You will see that it is already pretty good. There are only three nodes. Now take a look at the ONT resolved graph Open the assembly/5-untip/unitig-normal-connected-tip.gfa file in Bandage. Now our three nodes have been resolved into one. Do we need to phase? We just worked through test data that was HiFi only (Hifiasm) then HiFi + ONT UL (Verkko) so you'd be forgiven for thinking that for these assemblers you don't need phasing data. In truth, if the HiFi and ONT data could produce contigs that are chromosome-level in scale, then trios/Hi-C wouldn't be needed, but assemblers aren't there yet. Let's look at some example data from Hifiasm which was produced with only HiFi for the Eastern Narrow Mouth Toad ( Gastrophryne carolinensis ). Below is a Merqury kmer count plot for the primary haplotype from this assembly: Hifiasm (primary) assembly without Purge Dups This is the primary assembly of a diploid, so we'd like to see the red section have two peaks -- one for heterozygous positions and one for homozygous positions. This is because the primary would ideally represent all of the homozygous regions as well as one set of alleles for heterozygous regions. We'd also like to see almost no blue section (since the blue section represents regions that are represented twice in the assembly), as these mean that a region is being represented twice. Unfortunately there are a lot of kmers seen twice in the assembly. When purge dups (a tool that looks for false duplications based on coverage information) is run, it gets better: Hifiasm assembly with Purge Dups But you can see that even after purging duplications, there is a large blue area. Now look what happens when Hi-C phasing is used: Hifiasm assembly (one haplotype) with HiC phasing The blue section reduces in size drastically! So what is happening here? Without phasing data such as trio or Hi-C, Hifiasm will create an assembly graph as you'd expect. Then it will try and figure out what path to walk in order to create a primary and an alternate assembly. It does this by just picking one side of the bubbles it encounters when walking the graph and assigning those to the primary assembly. The other sides gets put in the alternate assembly. In very heterozygous genomes, or very heterozygous regions of a genome, the primary assembly can still retain haplotigs from the alternate allele because the regions may look very different (and therefore don't create a nice bubble in the graph), even though they are still just representing alternate alleles at the same locus. This can result in the same genomic region incorrectly being represented twice in the primary assembly, which we call a false duplication. Verkko for its part, is more conservative. Once Verkko gets to a bubble that it doesn't know how to phase it just stops and you get a break in your assembly. Running on Full Data Sets \u00b6 We won't be running the full datasets during the workshop, but you are encouraged to do so yourself. To help facilitate that we've downloaded the full datasets to NeSi for you. Raw Data /nesi/nobackup/nesi02659/LRA/resources/ \u251c\u2500\u2500 deepconsensus/ # HiFi \u251c\u2500\u2500 ont_ul/ \u251c\u2500\u2500 ilmn/ # Maternal/Paternal/Child \u2514\u2500\u2500 hic/ And we've run these assemblies for you /nesi/nobackup/nesi02659/LRA/resources/assemblies/ \u251c\u2500\u2500 verkko/ \u251c\u2500\u2500 trio/ # full verkko folder available! \u2514\u2500\u2500 hic/ \u2514\u2500\u2500 hifiasm/ \u251c\u2500\u2500 trio/ \u2514\u2500\u2500 hic/ We will be taking advantage of these pre-baked assemblies in the remainder of the workshop. How would you run Verkko/Hifiasm on full datasets? Verkko with trio data verkko -d asm \\ --hifi hifi/*.fastq.gz \\ --nano ont/*.fastq.gz \\ --hap-kmers maternal_compress.k30.hapmer.meryl \\ paternal_compress.k30.hapmer.meryl \\ trio Verkko with Hi-C data verkko -d asm \\ --hifi hifi/*.fastq.gz \\ --nano ont/*.fastq.gz \\ --hic1 hic/*R1*fastq.gz \\ --hic2 hic/*R2*fastq.gz Hifiasm with trio data hifiasm \\ -o sample_name \\ -t32 \\ --ul ul.fq.gz \\ -1 pat.yak \\ -2 mat.yak \\ HiFi-reads.fq.gz Hifiasm with Hi-C data hifiasm \\ -o sample_name \\ -t32 \\ --h1 read1.fq.gz \\ --h2 read2.fq.gz \\ --ul ul.fq.gz \\ HiFi-reads.fq.gz Building Graph Sense \u00b6 We\u2019re going to open Bandage and look around. This is a demonstration, so you do not need to try to do this on your own machine. The goal is to develop an intution for what the graph visualization is telling you. Overall feel for graphs and bandage graphs: nodes, edges, and paths simple examples: cycle & hairpin bandage UI loading CSVs and launching from CLI How does a typical human assembly look? ~23 chunks zippers: het-hom-het-etc. non-zippers (rDNA tangles, sex chrs, breaks) Complex examples hom with short hets w/o markers long hom with long hets \u201cscaffold\u201d from graph structure complete fail Comparison of Computational Cost \u00b6 Hifiasm \u00b6 Hifiasm is compiled into a single binary file, and, when executed, it manages all tasks and parallelism under one parent process. You can run it the same on a VM in the cloud or in an HPC. For a human sample with around 40X HiFi and 30X UL and either HiC or trio phasing Hifiasm can assemble with: 64 cores 240GB of memory (most samples will use less) Around 24 hours of total runtime So Hifiasm takes about 1500 CPU hours to assemble this sample. On a cluster you can just execute the run command. If you are on a cloud and would like to take advantage of pre-emptible instances, you can break the run command into three parts (each take around 8 hours). Verkko \u00b6 Verkko is written as a shell wrapper around a Snakemake pipeline. This has the advantages of easily restarting after failures and increased potential for parallelism in an HPC environment with multiple nodes available, but it is hard to profile all the individual tasks. If the cluster is not too busy a human assembly can finish in around a day. Most of the compute is done in the overlap and graph aligner jobs. So we can break the runtimes into steps that revolve around the big jobs. That looks something like this: Step CPUs Shards Time/Shard (est) Total CPU Hours pre overlap 24 1 3 72 overlap 8 600 1 4800 create graph 80 1 13 1040 graph aligner 12 100 2 2400 complete asm 64 1 12 768 This gives an estimate of around 9000 CPU hours for the same data as above. This is almost certainly an overestimate, but not by more than a factor of 2. Note that the runtime estimates for Hifiasm and Verkko don't consider the preparatory work of counting parental k -mers with yak or Meryl, which are necessary steps before running either in trio mode. Comparison of Outputs \u00b6 Verkko and Hifiasm are both excellent assemblers. If you have a human sample with over 40X HiFi and over 15X ONT data over 100kb then the high level metrics that you will learn about tomorrow should be pretty comparable across the two assemblers. If you have a set of data that you spent a bunch of money on, and you are hoping to make a high-quality assembly, your best bet is to assemble with both and see which assembly is better. You could even stitch the good parts of each assembly together\u2014though the people who have to do the actual work tend to flinch when they hear that. Verrko's Approach \u00b6 As we saw previously, Verkko uses HiFi data to create a graph (in the case of Verkko it is a DeBruijn graph). ONT reads are aligned to the graph and the graph is then simplified. One thing that Verkko does is it outputs scaffolds\u2014where Hifiasm only outputs contigs. With 40X+ HiFi, Verkko's scaffolding tends to add about 12 extra T2T chromosomes to a diploid human assembly. The way it is scaffolded comes from the graph (as shown below). One the left we see one haplotype and there is a tangle in the middle of the sequence. Verkko doesn't necessarily know how to walk through this tangle and it doesn't want to output incorrect sequence. So it just estimates the size of the nodes in the tangle and puts the corresponding number of N's into the final assembly. Similarly, on the right we have a gap in one haplotype. Verkko will infer the size of the missing sequence from the other haplotype and put that many N's into to top sequence. This has led some people (well at least one person) to call this approach grapholding. Hifiasm's Approach \u00b6 Hifiasm creates string graphs from HiFi and ONT data separately (kind of) and then combines them. The argument here is that by creating a standalone ONT graph you don't risk losing information that may be missing in the HiFi-only graph. At the time moment (July 2023) Hifiasm does not include a scaffolding step. Though that will likely change in the coming months. How Should I Choose? \u00b6 It's not an easy choice, but here are some guidelines If you can, use both If you have Hifi coverage under 40X: use Hifiasm Verkko tends to perform less well at lower HiFi coverages If you have to pay for compute time: use Hifiasm (see the previous section) Verkko is more expensive to run. If you are on an HPC that may be ok. If you are paying Amazon for your compute then Verkko assemblies can cost upwards of $300 (USD). If you want to assemble then fiddle with it to perfect the assembly: use both, then fix things with Verkko Verkko allows you to see its inner workings. You can also make manual changes and then restart from that point in the assembly process. If you do things right, Verkko will take care of the rest. This was done, for instance, by the Verkko team on their version of the HG002 assembly: they manually resolved tangles in the graph. How Much Input Data Do I Need? \u00b6 Let's do some back of the envelope math to see how much is an ideal amount of data that would go into an assembly... PacBio HiFi Computing overlaps isn't so different from calling variants. For each haplotype we probably want around 10X coverage in order to calculate good overlaps. So that would give about 20X total. But HiFi coverage is variable, and there are some well know regions (such as GA repeats) that drop out of HiFi data. In order to get as many regions as possible above the threshold for assembly we increase the value to 40X. ONT UL The answer to this depends on who you ask. In the case of Verkko you are often just looking for one or a handful of reads to span a tricky region in the graph. The Verkko team has shown that coverages over 15X of 100kb+ reads don't add much in terms of contiguity. (Though for advanced applications such as using ONT to fill-in missing parts of the graph the math may be different.) Trio and Hi-C In general the better your graph, the easier it is to phase. If you have only a few big bubbles in the graph, it is a lot easier to find Illumina data that will map to them in a haplotype-specific way. This hasn't been tested rigorously, but people tend to talk about 30X for these datasets.","title":"4. Assembly"},{"location":"pages/3_assembly/#4-assembly","text":"Here is a rundown of what we will do: Learn about how Verkko creates assemblies Run Hifiasm with test data (HiFi only) Run Verkko with test data (HiFi + ONT) See how to run both with full datasets Compare the two assemblers Talk about how much data we need At the end of the day you will hopefully have a feel for how to actually run each assembler, what data to give them, and when to choose one over the other.","title":"4. Assembly"},{"location":"pages/3_assembly/#theoretical-walkthrough-of-the-assembly-process","text":"","title":"Theoretical Walkthrough Of The Assembly Process"},{"location":"pages/3_assembly/#verkko","text":"In this section we will go over the rough outline of Verkko's approach to assembly. Hopefully this will help put the data types from yesterday in context. Knowing how each data type is used also helps you to make better decisions when planning your sequencing runs. Both Verkko and Hifiasm can use a variety of data sources: PacBio HiFi: >10kbp, around 99.9% accuracy Oxford Nanopore Ultralong: >100kb, around 97% accuracy Phasing data from Hi-C or trio Illumina data PacBio HiFi data is required for both assemblers. Other data types are optional\u2014but they lead to much better assemblies. So let's jump ahead a bit and take a peek at how Verkko creates assemblies using figure 1 from the recent Verkko paper (Rautiainen, Mikko, et al.). It's ok if this is a bit confusing, you don't need to know the inner workings of Verkko in order to make great assemblies. PacBio HiFi is used to create the initial graph Verkko's first theoretical task is to create an assembly graph from HiFi data, but it has to prepare the HiFi reads first. HiFi data is less accurate in homopolymer repeats and microsatellites, so before creating an assembly graph, the reads are \"compressed\" in these regions: The reads are then error corrected. You don't have to worry about how this works (just know that it is very computationally intensive). Once that is done, a graph is created from the HiFi reads: If you aren't familiar with what an assembly graph is, that is also ok! The annoying thing is that there are different ways to make assembly graphs, but they all have the common feature of linking together reads by their overlaps. In this graph the boxes (also called nodes) represent sequences and the lines (also called edges) represent the relationship between those overlaps. Note that in the middle section the orange and blue lines represent the ONT reads aligned to the graph. Oxford Nanopore Data (which is long) helps simplify the graph Using these alignments, nodes that are linked (or phased) by a read are combined. This \"simplifies\" the graph -- in other words, you get nice long nodes where you previously had shorter nodes. (Long nodes are good, they mean you have longer sequences that are assembled.) The graph can now be phased In the case of trio Illumina data, Verkko looks at the nodes and counts the number of maternal-specific or paternal-specific sequences of DNA (from meryl hapmer DBs). If it finds that a node has, for instance, a bunch of maternal specific sequences/ k -mers and almost no paternal specific sequences/ k -mers then the assembler will assign that node to be maternal. Nodes that are the same haplotype but are separated by a homozygous region are then merged. Finally the assembly graph can be converted into two contigs which represent maternal and paternal haplotypes. Maternal and paternal contigs for the entire assembly are then put into one diploid FASTA as well as two haploid FASTAs. How does Hi-C phasing work? Like using trio data, Hi-C phasing aims to find nodes that are near to each other and come from the same haplotype. To achieve this, Hi-C data is aligned to the graph and reads that are linked across nodes can be used to phase the graph as shown in this figure modified from (Garg, Shilpa): One key difference between trio phasing and Hi-C is that Hi-C data cannot say that a set of nodes all come from the sample's mother or father, only that they come from the same haplotype.","title":"Verkko"},{"location":"pages/3_assembly/#exploring-with-test-data","text":"Running assemblers is very computationally intensive and the output files can be big. Let's not jump straight into assembling human genomes. Instead we can use the test data that both assemblers provide as a way to both ensure that we know how to run the tool (which is easy) and we can start to get a feel for the process and outputs in real life.","title":"Exploring With Test Data"},{"location":"pages/3_assembly/#run-hifiasm-with-test-data","text":"Create a directory code cd ~/obss_2023/genome_assembly/ mkdir -p assembly/hifiasm_test cd assembly/hifiasm_test Now download Hifiasm's test data code wget https://github.com/chhylp123/hifiasm/releases/download/v0.7/chr11-2M.fa.gz This is HiFi data from about 2 million bases of chromosome 11. HiFi data is the only required data type for Hifiasm and Verkko. You can create assemblies from only HiFi data and you can add ONT and phasing later. Also notice that this data is in FASTA format! Presumably this is to make the file smaller since this is test data. Now let's load the Hifiasm module code module purge module load hifiasm And actually run the test data code hifiasm \\ -o test \\ -t4 \\ -f0 \\ chr11-2M.fa.gz \\ 2 > test.log & code head -n 60 test.log Now check the Hifiasm log interpretation section of the documentation to give that some context. What does the histogram represent, and how many peaks do you expect? The histogram represents the k -mer count in the HiFi reads. For humans we expect to see a large peak somewhere around our expected sequencing coverage: this represents homozygous k -mers. The smaller peak represents heterozygous k -mers. Now ls the directory to see what outputs are present. What do you see? No FASTA files, but there are a lot of files that end in gfa . If you haven't seen these before, then we get to introduce you to another file format!","title":"Run Hifiasm With Test Data"},{"location":"pages/3_assembly/#introduction-to-gfa-files","text":"GFA stands for Graphical Fragment Alignment and Hifiasm outputs assemblies in GFA files. GFAs aren't like bed or sam files which have one entry per line (or FASTA/Q that have 2/4 lines per entry). But this is bioinformatics, so you can rest assured that it is just a text file with a new file extension. It's easiest to just look at an example of a GFA file from the spec: H VN:Z:1.0 S 11 ACCTT S 12 TCAAGG S 13 CTTGATT L 11 + 12 - 4M L 12 - 13 + 5M L 11 + 13 + 3M P 14 11 +,12-,13+ 4M,5M Here we see the following line types H (Header): File header. You get the idea. The example here the header is just saying that the file follows GFA 1.0 spec. Notice that this line follows a TAG:TYPE:VALUE convention. Type in this case is Z which corresponds to printable string. S (Segment): A sequence of DNA This is what we care about for the moment! L (Link): Overlap between two segments We can read the first Link line as saying that the end of Segment 11 (+) connects to the beginning of Segment 12 (-) and the overlap is 4 matching bases. In this case it would look like this: ACCTT ( Segment 11 ) |||| GGAACT ( Segment 12 -- reversed ) P (Path): Ordered list of segments (connected by links) So how do we get a FASTA from a GFA? To get a FASTA we just pull the S lines from a GFA and print them to a file: code awk '/^S/{print \">\"$2;print $3}' \\ test.bp.p_ctg.gfa \\ > test.p_ctg.fa You can read this awk command as: Give me all input lines that start with S Then print the second column of those lines (which is the sequence ID) Also print another line with the actual sequence Why does Hifiasm output GFAs and not FASTAs? Hifiasm (and many other assemblers) use GFAs while they are actually assembling. The GFA represents/stores the assembly graph. Hifiasm probably doesn't output FASTAs just because everything in the FASTA is contained in the GFA, so why store it twice?","title":"Introduction To GFA Files"},{"location":"pages/3_assembly/#view-hifiasm-test-assembly-gfa-in-bandage","text":"We are going to take a look at the assembly GFA file in a browser called Bandage. Bandage provides a way to visualize something called unitig graphs. Start Bandage Open Jupyter Virtual Desktop according to these instructions In the Virtual Desktop, click on the terminal emulator icon (in your toolbar at the bottom of your screen) Load the Bandage module with module load Bandage Type Bandage & to start Bandage Load a unitig GFA Click the File dropdown then Load Graph Navigate to our current folder ( cd ~/obss_2023/genome_assembly/assembly/hifiasm_test ) Select the test.bp.r_utg.noseq.gfa file and press the Open icon Under Graph Drawing on the left-hand side click Draw Graph Ok, so what are we looking at? The thick lines are nodes\u2014which in this case represent sequences. Since we loaded the unitig graph the sequences are unitigs. A unitig is a high confidence contig. It is a place where the assembler says \"I know exactly what is going on here\". The ends of unitigs are where it gets messy. At the ends, an assembler has choices to make about which unitig(s) to connect to next. Now load a contig GFA Open the test.bp.p_ctg.noseq.gfa file to see how boring it is. In general, when using Bandage people look at the unitig GFAs (not contig GFAs). An assembly is a hypothesis, and the contigs output by the assembler are its best guess at the correct haplotype sequence. The contigs don't show much information about the decisions being made, however. They are the output. We view unitig GFAs so we can see the data structure at the point that the assembler was making tough decisions. Here are some things you can do with Bandage Let's say you mapped a sample's ONT reads back onto that sample's de novo assembly and have identified a misjoin. You can open up bandage and find that unitigs that went into the contig to see if it can be easily manually broken. If you have a phased diploid assembly with a large sequence that is missing, you can look at the unitig gfa, color the nodes by haplotype, and see which sequences are omitted. Those sequences can then be analyzed and manually added into the final assembly. You can label nodes with (HiFi) coverage and inspect regions with low quality too see if they have low coverage as well. If so, you might want to throw them out. (This does happen, in particular for small contigs that assemblers tend to output.)","title":"View Hifiasm Test Assembly GFA in Bandage"},{"location":"pages/3_assembly/#run-verkko-with-test-data","text":"Create a directory code cd ~/obss_2023/genome_assembly mkdir -p assembly/verkko_test cd assembly/verkko_test Now download Verkko's test data code curl -L https://obj.umiacs.umd.edu/sergek/shared/ecoli_hifi_subset24x.fastq.gz -o hifi.fastq.gz curl -L https://obj.umiacs.umd.edu/sergek/shared/ecoli_ont_subset50x.fastq.gz -o ont.fastq.gz You can see that this dataset is for E. coli and there is both HiFi and ONT data included. Create Slurm script for test Verkko run code Start your favourite text editor nano verkko_test.sl And then paste in the following #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --partition milan #SBATCH --job-name test_verkko #SBATCH --cpus-per-task 8 #SBATCH --time 00:15:00 #SBATCH --mem 12G #SBATCH --output slurmlogs/%x.%j.log #SBATCH --error slurmlogs/%x.%j.err ## load modules module purge module load verkko/1.3.1-Miniconda3 ## run verkko verkko \\ -d assembly \\ --hifi ./hifi.fastq.gz \\ --nano ./ont.fastq.gz Run verkko test sbatch verkko_test.sl This should only take a few minutes to complete. You can keep track of the run with the squeue command. code squeue --me How does Verkko run? It turns out that if you run Verkko more than once or twice you will have to know a bit about how it is constructed. Verkko is a program that reads in the parameters you gave it and figures out a few things about your verkko installation and then creates a configuration file ( verkko.yml ) and a shell script ( snakemake.sh ). The shell script is then automatically executed. Take a look at the shell script that was created for your run code cat assembly/snakemake.sh It is just a call to Snakemake!!! You can think of Verkko as a tool, but also as a pipeline because it is. This has some advantages. One is that if you know what Verkko is doing (which is somewhat achievable given that the Snakemake rules guide you through Verkko's logic), you can add to it, or even swap out how Verkko performs a given step for how you'd like to do it. It also means that you can restart a run at any given step (if you made a mistake or if the run failed). Lastly, and maybe most importantly, Snakemake supports Slurm as a backend. So if you have access to an HPC you could (and probably should) run Verkko and allow it to launch Slurm jobs for you. (This is in contrast to what we just did which was to run a Slurm job and just allow all jobs to run on the allocated resources that we requested for the entire run.) Now take a look at the jobs that were run You can view the stderr from the run in your Slurm logs, or in Snakemake's logs. Let's take a look at the top of the log: code head -n 35 assembly/.snakemake/log/*.log This shows a list of Snakemake jobs that will get executed for this dataset. There are a few things to note. The first is that for larger datasets some jobs will get executed many times (hence the count column). This dataset is small, so most jobs have count=1 . The second thing to note is that these jobs are sorted alphabetically, so we can get a feel for scale, but it's a bit hard to figure out what Verkko is really doing. Open the logs and scroll through them code less assembly/.snakemake/log/*.log You can see all of the Snakemake jobs, in order, that were run. Even for this tiny dataset there are many. Since there are a lot of jobs, there are a lot of outputs, and these are organized (roughly) by Snakemake rule. Take a look at the output folder in order to familiarize yourself with the layout. code ls -lh assembly Take a look at the initial HiFi graph Open the assembly/1-buildGraph/hifi-resolved.gfa file in Bandage. You will see that it is already pretty good. There are only three nodes. Now take a look at the ONT resolved graph Open the assembly/5-untip/unitig-normal-connected-tip.gfa file in Bandage. Now our three nodes have been resolved into one. Do we need to phase? We just worked through test data that was HiFi only (Hifiasm) then HiFi + ONT UL (Verkko) so you'd be forgiven for thinking that for these assemblers you don't need phasing data. In truth, if the HiFi and ONT data could produce contigs that are chromosome-level in scale, then trios/Hi-C wouldn't be needed, but assemblers aren't there yet. Let's look at some example data from Hifiasm which was produced with only HiFi for the Eastern Narrow Mouth Toad ( Gastrophryne carolinensis ). Below is a Merqury kmer count plot for the primary haplotype from this assembly: Hifiasm (primary) assembly without Purge Dups This is the primary assembly of a diploid, so we'd like to see the red section have two peaks -- one for heterozygous positions and one for homozygous positions. This is because the primary would ideally represent all of the homozygous regions as well as one set of alleles for heterozygous regions. We'd also like to see almost no blue section (since the blue section represents regions that are represented twice in the assembly), as these mean that a region is being represented twice. Unfortunately there are a lot of kmers seen twice in the assembly. When purge dups (a tool that looks for false duplications based on coverage information) is run, it gets better: Hifiasm assembly with Purge Dups But you can see that even after purging duplications, there is a large blue area. Now look what happens when Hi-C phasing is used: Hifiasm assembly (one haplotype) with HiC phasing The blue section reduces in size drastically! So what is happening here? Without phasing data such as trio or Hi-C, Hifiasm will create an assembly graph as you'd expect. Then it will try and figure out what path to walk in order to create a primary and an alternate assembly. It does this by just picking one side of the bubbles it encounters when walking the graph and assigning those to the primary assembly. The other sides gets put in the alternate assembly. In very heterozygous genomes, or very heterozygous regions of a genome, the primary assembly can still retain haplotigs from the alternate allele because the regions may look very different (and therefore don't create a nice bubble in the graph), even though they are still just representing alternate alleles at the same locus. This can result in the same genomic region incorrectly being represented twice in the primary assembly, which we call a false duplication. Verkko for its part, is more conservative. Once Verkko gets to a bubble that it doesn't know how to phase it just stops and you get a break in your assembly.","title":"Run Verkko With Test Data"},{"location":"pages/3_assembly/#running-on-full-data-sets","text":"We won't be running the full datasets during the workshop, but you are encouraged to do so yourself. To help facilitate that we've downloaded the full datasets to NeSi for you. Raw Data /nesi/nobackup/nesi02659/LRA/resources/ \u251c\u2500\u2500 deepconsensus/ # HiFi \u251c\u2500\u2500 ont_ul/ \u251c\u2500\u2500 ilmn/ # Maternal/Paternal/Child \u2514\u2500\u2500 hic/ And we've run these assemblies for you /nesi/nobackup/nesi02659/LRA/resources/assemblies/ \u251c\u2500\u2500 verkko/ \u251c\u2500\u2500 trio/ # full verkko folder available! \u2514\u2500\u2500 hic/ \u2514\u2500\u2500 hifiasm/ \u251c\u2500\u2500 trio/ \u2514\u2500\u2500 hic/ We will be taking advantage of these pre-baked assemblies in the remainder of the workshop. How would you run Verkko/Hifiasm on full datasets? Verkko with trio data verkko -d asm \\ --hifi hifi/*.fastq.gz \\ --nano ont/*.fastq.gz \\ --hap-kmers maternal_compress.k30.hapmer.meryl \\ paternal_compress.k30.hapmer.meryl \\ trio Verkko with Hi-C data verkko -d asm \\ --hifi hifi/*.fastq.gz \\ --nano ont/*.fastq.gz \\ --hic1 hic/*R1*fastq.gz \\ --hic2 hic/*R2*fastq.gz Hifiasm with trio data hifiasm \\ -o sample_name \\ -t32 \\ --ul ul.fq.gz \\ -1 pat.yak \\ -2 mat.yak \\ HiFi-reads.fq.gz Hifiasm with Hi-C data hifiasm \\ -o sample_name \\ -t32 \\ --h1 read1.fq.gz \\ --h2 read2.fq.gz \\ --ul ul.fq.gz \\ HiFi-reads.fq.gz","title":"Running on Full Data Sets"},{"location":"pages/3_assembly/#building-graph-sense","text":"We\u2019re going to open Bandage and look around. This is a demonstration, so you do not need to try to do this on your own machine. The goal is to develop an intution for what the graph visualization is telling you. Overall feel for graphs and bandage graphs: nodes, edges, and paths simple examples: cycle & hairpin bandage UI loading CSVs and launching from CLI How does a typical human assembly look? ~23 chunks zippers: het-hom-het-etc. non-zippers (rDNA tangles, sex chrs, breaks) Complex examples hom with short hets w/o markers long hom with long hets \u201cscaffold\u201d from graph structure complete fail","title":"Building Graph Sense"},{"location":"pages/3_assembly/#comparison-of-computational-cost","text":"","title":"Comparison of Computational Cost"},{"location":"pages/3_assembly/#hifiasm","text":"Hifiasm is compiled into a single binary file, and, when executed, it manages all tasks and parallelism under one parent process. You can run it the same on a VM in the cloud or in an HPC. For a human sample with around 40X HiFi and 30X UL and either HiC or trio phasing Hifiasm can assemble with: 64 cores 240GB of memory (most samples will use less) Around 24 hours of total runtime So Hifiasm takes about 1500 CPU hours to assemble this sample. On a cluster you can just execute the run command. If you are on a cloud and would like to take advantage of pre-emptible instances, you can break the run command into three parts (each take around 8 hours).","title":"Hifiasm"},{"location":"pages/3_assembly/#verkko_1","text":"Verkko is written as a shell wrapper around a Snakemake pipeline. This has the advantages of easily restarting after failures and increased potential for parallelism in an HPC environment with multiple nodes available, but it is hard to profile all the individual tasks. If the cluster is not too busy a human assembly can finish in around a day. Most of the compute is done in the overlap and graph aligner jobs. So we can break the runtimes into steps that revolve around the big jobs. That looks something like this: Step CPUs Shards Time/Shard (est) Total CPU Hours pre overlap 24 1 3 72 overlap 8 600 1 4800 create graph 80 1 13 1040 graph aligner 12 100 2 2400 complete asm 64 1 12 768 This gives an estimate of around 9000 CPU hours for the same data as above. This is almost certainly an overestimate, but not by more than a factor of 2. Note that the runtime estimates for Hifiasm and Verkko don't consider the preparatory work of counting parental k -mers with yak or Meryl, which are necessary steps before running either in trio mode.","title":"Verkko"},{"location":"pages/3_assembly/#comparison-of-outputs","text":"Verkko and Hifiasm are both excellent assemblers. If you have a human sample with over 40X HiFi and over 15X ONT data over 100kb then the high level metrics that you will learn about tomorrow should be pretty comparable across the two assemblers. If you have a set of data that you spent a bunch of money on, and you are hoping to make a high-quality assembly, your best bet is to assemble with both and see which assembly is better. You could even stitch the good parts of each assembly together\u2014though the people who have to do the actual work tend to flinch when they hear that.","title":"Comparison of Outputs"},{"location":"pages/3_assembly/#verrkos-approach","text":"As we saw previously, Verkko uses HiFi data to create a graph (in the case of Verkko it is a DeBruijn graph). ONT reads are aligned to the graph and the graph is then simplified. One thing that Verkko does is it outputs scaffolds\u2014where Hifiasm only outputs contigs. With 40X+ HiFi, Verkko's scaffolding tends to add about 12 extra T2T chromosomes to a diploid human assembly. The way it is scaffolded comes from the graph (as shown below). One the left we see one haplotype and there is a tangle in the middle of the sequence. Verkko doesn't necessarily know how to walk through this tangle and it doesn't want to output incorrect sequence. So it just estimates the size of the nodes in the tangle and puts the corresponding number of N's into the final assembly. Similarly, on the right we have a gap in one haplotype. Verkko will infer the size of the missing sequence from the other haplotype and put that many N's into to top sequence. This has led some people (well at least one person) to call this approach grapholding.","title":"Verrko's Approach"},{"location":"pages/3_assembly/#hifiasms-approach","text":"Hifiasm creates string graphs from HiFi and ONT data separately (kind of) and then combines them. The argument here is that by creating a standalone ONT graph you don't risk losing information that may be missing in the HiFi-only graph. At the time moment (July 2023) Hifiasm does not include a scaffolding step. Though that will likely change in the coming months.","title":"Hifiasm's Approach"},{"location":"pages/3_assembly/#how-should-i-choose","text":"It's not an easy choice, but here are some guidelines If you can, use both If you have Hifi coverage under 40X: use Hifiasm Verkko tends to perform less well at lower HiFi coverages If you have to pay for compute time: use Hifiasm (see the previous section) Verkko is more expensive to run. If you are on an HPC that may be ok. If you are paying Amazon for your compute then Verkko assemblies can cost upwards of $300 (USD). If you want to assemble then fiddle with it to perfect the assembly: use both, then fix things with Verkko Verkko allows you to see its inner workings. You can also make manual changes and then restart from that point in the assembly process. If you do things right, Verkko will take care of the rest. This was done, for instance, by the Verkko team on their version of the HG002 assembly: they manually resolved tangles in the graph.","title":"How Should I Choose?"},{"location":"pages/3_assembly/#how-much-input-data-do-i-need","text":"Let's do some back of the envelope math to see how much is an ideal amount of data that would go into an assembly... PacBio HiFi Computing overlaps isn't so different from calling variants. For each haplotype we probably want around 10X coverage in order to calculate good overlaps. So that would give about 20X total. But HiFi coverage is variable, and there are some well know regions (such as GA repeats) that drop out of HiFi data. In order to get as many regions as possible above the threshold for assembly we increase the value to 40X. ONT UL The answer to this depends on who you ask. In the case of Verkko you are often just looking for one or a handful of reads to span a tricky region in the graph. The Verkko team has shown that coverages over 15X of 100kb+ reads don't add much in terms of contiguity. (Though for advanced applications such as using ONT to fill-in missing parts of the graph the math may be different.) Trio and Hi-C In general the better your graph, the easier it is to phase. If you have only a few big bubbles in the graph, it is a lot easier to find Illumina data that will map to them in a haplotype-specific way. This hasn't been tested rigorously, but people tend to talk about 30X for these datasets.","title":"How Much Input Data Do I Need?"},{"location":"pages/4_assembly_qc/","text":"5. Assembly QC \u00b6 Now that we have understood our data types (day 1) and put them through an assembly algorithm (day 2), we have this file of A's, T's, C's, and G's that's supposed to be our assembly. This file is meant to represent a biological reality, so let's try to assess its quality through several lens, some biological and some more technical. One way to remember the ways we evaluate assemblies is by thinking about the \"3C's\": contiguity , correctness , and completeness . Food for thought What do you think a 'good' de novo assembly looks like? What are some qualities of an assembly that you might be interested in measuring? Contiguity (assembly statistics using gfastats) \u00b6 Recall that the sequences in our assembly are referred to as contigs . Normally, when we receive a hodgepodge of things with different values of the same variable, such as our contigs of varying lengths, we are inclined to use descriptive statistics such as average or median to try to get a grasp on how our data looks. However, it can be hard to compare average contig length between assemblies\u2014if they have the same total size and same number of contigs, it's still the same average, even if it's five contigs of 100bp, or one 460 bp contig and four 10bp ones! This matters for assembly because ideally we want fewer contigs that are larger . Median comes closer to reaching what we're trying to measure, but it can be skewed by having many very small contigs, so instead a popular metric for assessing assemblies is N50 . The N50 is similar to the median in that one must first sort the numbers, but then instead of taking the middle value, the N50 value is the length of the first contig that is equal to or greater than half of the assembly sum . But that can be hard to understand verbally, so let's look at it visually: Image adapted from Elin Videvall at The Molecular Ecologist . The N50 can be interpreted as such: given an N50 value, 50% of the sequence in that assembly is contained in contigs of that length or longer. Thus, N50 has been traditionally used as the assembly statistic of choice for comparing assemblies, as it's more intuitive (compared to average contig length) to see that an assembly with an N50 value of 100Mbp is more contiguous than one with an N50 value of 50MBp, since it seems like there are more larger contigs in the former assembly. Another statistic that is often reported with N50 is the L50 , which is the rank of the contig that gives the N50 value. For instance, in the above image, the L50 would be 3, because it would be the third largest contig that gives the N50 value. L50 is useful for contextualizing the N50, because it gives an idea of how many contigs make up that half of your assembly. N50 or NG50? Another measurement you might see is NG50. This is the N50 value, just calculated using a given genome size instead of the sum of the contigs. Given how the N50 value can be so affected by addition or removal of small contigs, another metric has come into use: the area under the (N50) curve , or the auN value. Though N50 is measured at the 50% mark, we could make similar values for any value of x, for instance N30 would be the value where 30% of the sequence in that assembly is of that length or longer. These metrics are thus called Nx statistics, and one could plot them against contig length to get an Nx curve , which gives a more nuanced view of the actual contig size distribution of your assembly. Image adapted from Heng Li's blog, which in turn adapts it from a NIBS workshop auN tries to capture the nature of this curve, instead of a value from an arbitrary point on it. On the above example, each step on the curve represents a contig (length on the y-axis), so the black curve is the most contiguous as it has one contig that covers over 40% of the assembly. Despite that, this assembly would have the same N50 value (on the x-axis) as multiple other assemblies that are more fragmented in the same area. Run gfastats on a FASTA \u00b6 Let's get some basic statistics for an assembly using a tool called gfastats , which will output metrics such as N50, auN, total size, etc. We can try it out on a Verkko trio assembly of HG002 that's already been downloaded onto NeSI. code cd ~/obss_2023/genome_assembly cd assembly_qc/gfastats ls -la # you should see a bunch of \"files\" that are actually symlinks pointing to their actual content Now we're ready to roll! module purge module load gfastats gfastats assembly.haplotype1.fasta It should take about 20 seconds Output +++Assembly summary+++: # scaffolds: 81 Total scaffold length: 3025610697 Average scaffold length: 37353218 .48 Scaffold N50: 112270693 Scaffold auN: 120344050 .34 Scaffold L50: 10 Largest scaffold: 242261106 Smallest scaffold: 7980 # contigs: 95 Total contig length: 3024505387 Average contig length: 31836898 .81 Contig N50: 101137168 Contig auN: 98109753 .40 Contig L50: 12 Largest contig: 201096255 Smallest contig: 7980 # gaps in scaffolds: 14 Total gap length in scaffolds: 1105310 Average gap length in scaffolds: 78950 .71 Gap N50 in scaffolds: 222184 Gap auN in scaffolds: 267123 .76 Gap L50 in scaffolds: 2 Largest gap in scaffolds: 425849 Smallest gap in scaffolds: 1000 Base composition ( A:C:G:T ) : 893171444 :618413603:615928136:896992204 GC content %: 40 .81 # soft-masked bases: 0 # segments: 95 Total segment length: 3024505387 Average segment length: 31836898 .81 # gaps: 14 # paths: 81 Run gfastats on a GFA \u00b6 Remember that the file we initially got was an assembly graph \u2014what if we wanted to know some graph-specific stats about our assembly, such as number of nodes or disconnected components? We can also assess that using gfastats. code gfastats --discover-paths unitig-normal-connected-tip.gfa Output +++Assembly summary+++: # scaffolds: 2312 Total scaffold length: 4179275425 Average scaffold length: 1807645 .08 Scaffold N50: 10829035 Scaffold auN: 11407399 .64 Scaffold L50: 134 Largest scaffold: 38686347 Smallest scaffold: 1987 # contigs: 2312 Total contig length: 4179275425 Average contig length: 1807645 .08 Contig N50: 10829035 Contig auN: 11407399 .64 Contig L50: 134 Largest contig: 38686347 Smallest contig: 1987 # gaps in scaffolds: 0 Total gap length in scaffolds: 0 Average gap length in scaffolds: 0 .00 Gap N50 in scaffolds: 0 Gap auN in scaffolds: 0 .00 Gap L50 in scaffolds: 0 Largest gap in scaffolds: 0 Smallest gap in scaffolds: 0 Base composition ( A:C:G:T ) : 1180405407 :909914907:908088683:1180866428 GC content %: 43 .50 # soft-masked bases: 0 # segments: 2312 Total segment length: 4179275425 Average segment length: 1807645 .08 # gaps: 0 # paths: 2312 # edges: 5790 Average degree: 2 .50 # connected components: 47 Largest connected component length: 530557693 # dead ends: 562 # disconnected components: 184 Total length disconnected components: 60717682 # separated components: 231 # bubbles: 8 # circular segments: 11 What's the --discover-paths flag for? gfastats tries to clearly distinguish contigs from segments, so it will not pick up on contigs in a GFA without paths defined. To get the contig stats as well as graph stats from these GFAs, you'll need to add the --discover-paths flag. Check out the graph-specific statistics at the end of the output. Compare two graphs' stats \u00b6 Now that we know how to get the statistics for one assembly, let's get them for two so we can actually compare them. We already compared a Verkko HiFi-only and HiFi+ONT graph visually, so let's do it with assembly stats this time. We're going to use a one-liner to put the assembly stats side-by-side, because it can be kind of cumbersome to scroll up and down between two separate command line runs and their outputs. code paste \\ < ( gfastats -t --discover-paths hifi-resolved.gfa ) \\ < ( gfastats -t --discover-paths unitig-normal-connected-tip.gfa \\ | cut -f 2 ) paste is a command that pastes two files side by side The <(COMMAND) syntax is called process substitution, and it passes the output of the command(s) inside the parentheses to another command (here it is passing the gfastats output to paste ), and can be useful when using a pipe ( | ) might not be possible The -t flag in gfastats specifies that the output should be tab-delimited, which makes it more computer-parsable The cut command in the substitution is just getting the actual statistics column from the gfastats output, because the first column is the name of the statistic Your output should look something like this: Output # contigs 68252 2312 Total contig length 3858601893 4179275425 Average contig length 56534 .63 1807645 .08 Contig N50 223090 10829035 Contig auN 638121 .24 11407399 .64 Contig L50 4232 134 Largest contig 23667669 38686347 Smallest contig 1608 1987 # gaps in scaffolds 0 0 Total gap length in scaffolds 0 0 Average gap length in scaffolds 0 .00 0 .00 Gap N50 in scaffolds 0 0 Gap auN in scaffolds 0 .00 0 .00 Gap L50 in scaffolds 0 0 Largest gap in scaffolds 0 0 Smallest gap in scaffolds 0 0 Base composition ( A:C:G:T ) 1089358958 :840831652:838233402:1090177881 1180405407 :909914907:908088683:1180866428 GC content % 43 .51 43 .50 # soft-masked bases 0 0 # segments 68252 2312 Total segment length 3858601893 4179275425 Average segment length 56534 .63 1807645 .08 # gaps 0 0 # paths 68252 2312 # edges 181274 5790 Average degree 2 .66 2 .50 # connected components 45 47 Largest connected component length 496518810 530557693 # dead ends 684 562 # disconnected components 18 184 Total length disconnected components 7267912 60717682 # separated components 63 231 # bubbles 4312 8 # circular segments 31 11 Output explained ... where the first column is the stats from the HiFi-only assembly graph, and the second column is the stats from the HiFi+ONT assembly graph. Notice how the HiFi-only graph has way more nodes than the HiFi+ONT one, like we'd seen in Bandage. Stats-wise, this results in the HiFi-only graph having a N50 value of 223 Kbp while the HiFi+ONT one is 10.8 Mbp, a whole order of magnitude larger. For the HiFi-only graph, though, there's a bigger difference between its N50 value and its auN value: 223 Kbp vs. 638 Kbp, while the HiFi+ONT stats have a smaller difference of 10.8 Mbp vs. 11.4 Mbp. This might be due to the HiFi-only graph having on average shorter segments and more of the shorter ones, since it doesn't have the ONT data to resolve the segments into larger ones. Correctness (QV using Merqury) \u00b6 Correctness refers to the base pair accuracy, and can be measured by comparing one's assembly to a gold standard reference genome. This approach is limited by 1) an assumption about the quality of the reference itself and the closeness between it and the assembly being compared, and 2) the need for a reference genome at all, which many species do not have (yet). To avoid this, we can use Merqury : a reference-free suite of tools for assessing assembly quality (particularly w.r.t. error rate) using k -mers and the read set that generated that assembly. If an assembly is made up from the same sequences that were in the sequencing reads, then we would not expect any sequences ( k -mers) in the assembly that aren't present in the read set\u2014but we do find those sometimes, and those are what Merqury flags as error k -mers. It uses the following formula to calculate QV value, which typically results in QVs of 50-60 : \\[ \\Large E=1-P=1-\\left (1-\\frac{K_{\\textrm{asm}}}{K_{\\textrm{total}}} \\right ) ^{\\frac{1}{k}} \\] OK, but what does 'QV' mean, anyway? The QV that Merqury is interpreted similarly to the commonly used Phred quality scale, which might be familiar to those who have done short-read sequencing or are otherwise acquainted with FASTQ files. Phred quality scores are logarithmically related to error-probability, such that: - Phred score of 30 represents a 1 in 1,000 error probability ( i.e. , 99.9% accuracy) - Phred score of 40 represents a 1 in 10,000 error probability ( i.e. , 99.99% accuracy) - Phred score of 50 represents a 1 in 100,000 error probability ( i.e. , 99.999% accuracy) - Phred score of 60 represents a 1 in 1,000,000 error probability ( i.e. , 99.9999% accuracy) Merqury operates using k -mer databases like the ones we generated using meryl, so that's what we'll do now. Running Meryl and GenomeScope on the E. coli Verkko assembly \u00b6 Let's try this out on the E. coli Verkko assembly. First we need a Meryl database, so let's generate that. code cd ~/obss_2023/genome_assembly/assembly_qc mkdir merqury cd merqury We just made a directory for our runs, now let's sym link the fasta and reads here so we can refer to them more easily ln -s ~/obss_2023/genome_assembly/assembly/verkko_test/assembly/assembly.fasta . ln -s ~/obss_2023/genome_assembly//assembly/verkko_test/hifi.fastq.gz . Now we can run Mercury! meryl count k=30 memory=4 threads=2 hifi.fastq.gz output read-db.meryl --wrap ??? Previously, we used the sbatch command to submit a slurm script to the cluster and the slurm job handler. The sbatch command can actually take a lot of parameters like the ones we included in the beginning of our script, and one of those parameters is --wrap which kind of wraps whatever command you give it in a Slurm wrapper so that the cluster can schedule it as if it was a Slurm script. Take note that running a process in this manner is not reproducible. Unless you have access to the history logs, other researchers are not able to know what parameters you have used. Therefore, it is advisable to write it out in a Slurm script as below: code #!/bin/bash -e #SBATCH --account=nesi02659 #SBATCH --job-name=meryl #SBATCH --time=00:15:00 #SBATCH --cpus-per-task=2 #SBATCH --mem=4G #SBATCH --partition=milan # Modules module purge module load Merqury/1.3-Miniconda3 # Run meryl count k = 30 memory = 4 threads = 2 \\ hifi.fastq.gz \\ output read-db.meryl That shouldn't take too long to run. Now we have a Meryl DB for our HiFi reads. If we're curious about the distribution of our k -mers, we can use Meryl generate a histogram of the counts to show us how often a k -mer occurs only once in the reads, twice, etc. How would you go about trying to do this with meryl? When you want to use a tool to do something (and you are decently confident that the tool can actually do it), then a good point to start is just querying the tool's manual or help dialogue. Try out meryl --help and see if there's a function that looks like it could generate the histogram we want. spoiler alert: it's meryl histogram read-db.meryl If you tried to run that command with the output straight to standard out ( i.e. , your terminal screen), you'll see it's rather overwhelming and puts you all the way at the high, high coverage k -mer counts, which are only one occurrence. Let's look at just the first 100 lines instead. code meryl histogram read-db.meryl > read-db.hist head -n 100 read-db.hist This is more manageable, and you can even kind of see the histogram forming from the count values. There's a lot of k -mers that are present at only one copy (or otherwise very low copy) in the read set: these are usually sequencing errors, because there's a lot of these k -mers present at low copy. Because the sequence isn't actually real ( i.e. , it isn't actually in the genome and isn't actually serving as sequencing template), these k -mers stay at low copy. After these error k -mers, there's a dip in the histogram until about the 24\u201328 copy range. This peak is the coverage of the actual k -mers coming from the genome that you sequenced, thus it corresponds to having coverage of ~26X in this read set. We only have one peak here because this is a haploid dataset, but if your dataset is diploid then expect two peaks with the first peak varying in height depending on heterozygosity of your sample. \"What if I want a pretty graph instead of imagining it?\" Good news\u2014there's an app a program for that. GenomeScope is a straightforward program with an online web page where you can just drop in your meryl histogram file and it will draw the histogram for you as well as use the GenomeScope model to predict some genome characteristics of your data, given the expected ploidy. Let's try it out! Download the read-db.hist file and throw it into the GenomeScope website: http://qb.cshl.edu/genomescope/genomescope2.0/ and adjust the parameters accordingly. Can I use GenomeScope to QC my raw data before assembly? As you can see here, GenomeScope can be useful for getting an idea of what your raw dataset looks like, as well as feeling out the genome that should be represented by those sequencing reads. This can be useful as a QC step before even running the assembly, to make sure that your dataset is good enough to use. Here's an example of a good diploid dataset of HiFi reads: How does the data look? What does the coverage look to be? How many peaks are there in the data and what do they represent? What are some characteristics of the genome as inferred by GenomeScope? This data looks good, and you know that 1) because this tutorial text already called it good previously, and 2) there's a good amount of coverage, around 40X diploid coverage in fact. Additionally, the peaks are all very clear and distinct from each other and from the error k -mer slope on the left. Recall that the first peak represents haploid coverage (i.e., coverage of heterozygous loci) and the second peak is diploid coverage. GenomeScope is predicting the total size of the genome to be about 2.2 Gbp with 1.23% heterozygosity. This is data for Microtus pennsylvaticus , the eastern meadow vole. Here's an example of another HiFi dataset: Compare this to the previous examples. Does this look like a good dataset? Ignoring how GenomeScope itself is getting confused and can't get its model to fit properly to the k -mer spectra, let's look at the actual observed k -mer spectra. It does look like there's potentially two peaks whose distributions are overlapping, one peak around 10X and the other just under 20X. These are presumably our haploid and diploid peaks, but there's not enough coverage to resolve them properly here. This is an example of a GenomeScope QC that would tell me we don't have enough HiFi data to continue onto assembly, let's try to generate some more data. Wow, more data! This result is from after adding one more (notably more successful) SMRT cell of HiFi data and re-running GenomeScope. We can see the resolution of the peaks much more cleanly, and the GenomeScope model fits the data much better now, so we can trust the genome characteristic estimates here more than before. If you noted the comparatively small genome size and wondered what this was, it's Anadara tuberculosa , the piangua. Now you might be wondering: what happens if I try to assemble data without enough coverage? Answer: a headache. The assembly that results from the dataset that made the first GenomeScope plot resulted in two haplotypes of over 3,000 contigs each, which is very fragmented for a genome this small, recapitulated by their auN values being ~0.5 Mbp. In comparison, an assembly with the dataset pictured in the second GenomeScope plot resulted in two haplotypes of 500-800 contigs with auN values of 3.6-3.9 Mbp! The improvement in contiguity can also be visualized in the Bandage plots: The above is the Hifiasm unitig graph for the assembly done without good HiFi coverage. The above is the Hifiasm unitig graph for the assembly done with good (~56X) HiFi coverage. Using Merqury in solo mode \u00b6 Let's use Merqury on that database we just made to get the QV and some plots. Use your text editor of choice to make a Slurm script ( run_merqury.sl ) to run the actual Merqury program with the following contents: code #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name merqury1 #SBATCH --cpus-per-task 8 #SBATCH --time 00:15:00 #SBATCH --mem 1G #SBATCH --output slurmlogs/%x.%j.log #SBATCH --error slurmlogs/%x.%j.err #SBATCH --partition milan ## load modules module purge module load Merqury export MERQURY = /opt/nesi/CS400_centos7_bdw/Merqury/1.3-Miniconda3/merqury ## create solo merqury dir and use it mkdir -p merqury_solo cd merqury_solo ## run merqury merqury.sh \\ ../read-db.meryl \\ ../assembly.fasta \\ output What's that export command doing there? Merqury as a package ships with a lot of scripts, especially for plotting. The merqury.sh command that we're using is calling those scripts, but we need to tell it where we installed Merqury. To find out the QV, we want the file named output.qv . Take a look at it and try to interpret the QV value you find (third column). If we recall the Phred scale system, this would mean that this QV value is great! Which is not surprising, considering we used HiFi data. It's worth noting, though, that we are using HiFi k -mers to evaluate sequences derived from those same HiFi reads. This does a good job of showing whether the assembly worked with that data well, but what if the HiFi data itself is missing parts of the genome, such as due to bias ( e.g. , GA dropout)? That's why it's important to use orthogonal datasets made using different sequencing technology, when possible. For instance, we can use an Illumina-based Meryl database to evaluate a HiFi assembly. For non-human vertebrates, this often results in the QV dropping from 50-60 to 35-45, depending on the genome in question. Using Merqury in trio mode \u00b6 We just ran Merqury on our E. coli assembly, and evaluated it using the HiFi reads that we used for that assembly. Merqury can also utilize trio data (using those hapmer DBs we talked about before) to evaluate the phasing of an assembly, so let's try that with our HG002 trio data. We can create run_merqury_trio.sl to do so. code #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name merqury2 #SBATCH --cpus-per-task 8 #SBATCH --time 02:00:00 #SBATCH --mem 40G #SBATCH --output slurmlogs/%x.%j.log #SBATCH --error slurmlogs/%x.%j.err #SBATCH --partition milan ## load modules module purge module load Merqury export MERQURY = /opt/nesi/CS400_centos7_bdw/Merqury/1.3-Miniconda3/merqury ## create trio merqury dir and use it cd ~/obss_2023/genome_assembly/assembly_qc mkdir -p merqury_trio cd merqury_trio # Go get the necessary files OBSS_RESOURCES = /nesi/project/nesi02659/obss_2023/resources/genome_assembly ln -s $OBSS_RESOURCES /assembly.*.fasta . ln -s $OBSS_RESOURCES /maternal.k30.hapmer.meryl . ln -s $OBSS_RESOURCES /paternal.k30.hapmer.meryl . ## let's run the program in a results directory to make things a little neater mkdir -p results cd results ## run merqury merqury.sh \\ ../read-db.meryl \\ ../paternal.k30.hapmer.meryl \\ ../maternal.k30.hapmer.meryl \\ ../assembly.haplotype1.fasta \\ ../assembly.haplotype2.fasta \\ output You can also look in this folder for the merqury outputs if there isn't enough time to run the actual program: /nesi/nobackup/nesi02659/LRA/resources/merqury There will be lots of outputs, but let's look at two now to get an idea for how the sequences have been partitioned between our assemblies, and whether that's consistent with information we know from trio sequencing. First let's look at the spectra-asm plot: Merqury's spectra plots take the kmer spectrum (like you'd seen in GenomeScope) and color the kmers according to where the kmer is found: the reads only , one of the assemblies, or both of the assemblies. A useful output from Merqury for evaluating phasing is the blob plot: In this plot, each blob is a contig, and its x,y position represents parental hapmer content, while color represents assembly-of-origin. What do different phasing approaches look like in Merqury? \u00b6 Now we know what a nice trio-phased assembly looks like in Merqury, but what do the other options (Hi-C phasing or no phasing at all) look like? Let's look at an example from the zebra finch ( Taeniopygia guttata ), where the Vertebrate Genomes Project (VGP) has used hifiasm on the same HiFi dataset with different phasing approaches and evaluated the resulting assemblies with trio data in order to benchmark the different methods. On the left, we have the blob plot for the trio assembly, which looks nicely phased as we expect -- all the contigs exhibit hapmers from only one parent (which you can tell because all blobs are along the x- or y- axis), and on top of that all the contigs from one assembly show only one parent's hapmers ( i.e. , hap1's red blobs all show only bTaeGut3 hapmers). In the middle, we have a blob plot for a primary/alternate set of assemblies generated without any phasing data. You'll notice that a lot of the primary assembly blobs are not on the axes, meaning they have hapmers from both parents. Why are the alternate contigs all in phase? It starts to make sense if you think about what the alternae assembly is meant to represent: the haplotigs. These are the alternate alleles for the heterozygous loci, so it would track that only one parent's hapmers are represented -- the alternate assembly is a collection of the \"other side of the bubble\"s when looking at the assembly graph. The right plot is a blob plot from the Hi-C-phased assembly. Notably, most of the contigs are able to be properly phased, since they are almost all on the x- or -y axis. Switch and Hamming errors using yak \u00b6 Two more types of errors we use to assess assemblies are switch errors and Hamming errors. Hamming errors represent the percentage of SNPs wrongly phased (compared to ground truth), while switch errors represent the percentage of adjacent SNP pairs wrongly phased. See the following graphic: Let's first create a director within assembly_qc for it. code cd ~/obss_2023/genome_assembly/assembly_qc mkdir -p yak cd yak As the image illustrates, switch errors occur when an assembly switches between haplotypes. These errors are more prevalent in pseudohaplotype ( e.g. , primary/alternate) assemblies that did not use any phasing data, as the assembler has no way of properly linking haplotype blocks, which can result in mixed hapmer content contigs that are a chimera of parental sequences. code #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name yaktrioeval #SBATCH --cpus-per-task 32 #SBATCH --time 01:00:00 #SBATCH --mem 256G #SBATCH --output slurmlogs/%x.%j.log #SBATCH --error slurmlogs/%x.%j.err #SBATCH --partition milan ## change to qc dir, link the necessary files cd ~/obss_2023/genome_assembly/assembly_qc/yak OBSS_RESOURCES = /nesi/project/nesi02659/obss_2023/resources/genome_assembly ln -s $OBSS_RESOURCES /yak . ln -s $OBSS_RESOURCES /hic . ln -s $OBSS_RESOURCES /trio . mkdir -p qc_yak cd qc_yak ## load modules module purge module load yak ## run yak yak trioeval -t 32 \\ ../yak/pat.HG003.yak ../yak/mat.HG004.yak \\ ../hic/HG002.hap1.fa.gz \\ > hifiasm.hic.hap1.trioeval yak trioeval -t 32 \\ ../yak/pat.HG003.yak ../yak/mat.HG004.yak \\ ../trio/HG002.mat.fa.gz \\ > hifiasm.trio.mat.trioeval Completeness (asmgene) \u00b6 Another way to assess an assembly is via completeness , particularly with regard to expected gene content. If you have a reference genome that's been annotated with coding sequences, then you can use the tool asmgene to align multi-copy genes to your assembly and see if they remain multi-copy, or if the assembler has created a misassembly. asmgene works by aligning annotated transcripts to the reference genome, and record hits if the transcript is mapped at or over 99% identity over 99% or greater of the transcript length. If the transcript only has one hit, then it is single-copy (SC), otherwise it's multi-copy (MC). The same is then done for your assembly, and the fraction of missing multi-copy (%MMC) gene content is computed. A perfect assembly would have %MMC be zero, while a higher fraction indicates the assembly has collapsed some of these multi-copy genes. Additionally, you can look at the presence (or absence!) of expected single-copy genes in order to check gene completeness of the assembly. The output will be a tab-delimed list of metrics and the value of that metric for the reference and for your given assembly. The line full_sgl gives the number of single-copy genes present in the reference and your assembly\u2014if these numbers are off-balanced, then you might have false duplications, which are also pointed out on the full_dup line. For the multi-copy genes, you can look at dup_cnt to see the number of multi-copy genes in the reference and see how many of those genes are still multi-copy in your assembly. You can then use these values to calculate %MMC via the formula 1 - (dup_cnt asm / dup_cnt ref) . Let's try running asmgene on haplotype1 and haplotype2 from the pre-baked Verkko trio assemblies. code ## asmgene cd ~/obss_2023/genome_assembly/ mkdir -p ~/obss_2023/genome_assembly/assembly_qc/asmgene cd assembly_qc/asmgene # let's symlink some of the necessary files OBSS_RESOURCES = /nesi/project/nesi02659/obss_2023/resources/genome_assembly ln -s $OBSS_RESOURCES /chm13v2.0.fa . ln -s $OBSS_RESOURCES /CHM13-T2T.cds.fasta . ln -s $OBSS_RESOURCES /assembly.haplotype1.fasta . ln -s $OBSS_RESOURCES /assembly.haplotype2.fasta . Now that we have our files, we're ready to go. Make a script with the following content and run it in the directory with the appropriate files: code #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name asmgene #SBATCH --cpus-per-task 32 #SBATCH --time 05:00:00 #SBATCH --mem 256G #SBATCH --output slurmlogs/%x.%j.log #SBATCH --error slurmlogs/%x.%j.err #SBATCH --partition milan ## load modules module purge module load minimap2/2.24-GCC-11.3.0 ## run minimap2 on ref, hap1, and hap2 minimap2 -cxsplice:hq -t32 \\ chm13v2.0.fa CHM13-T2T.cds.fasta \\ > ref.cdna.paf minimap2 -cxsplice:hq -t32 \\ assembly.haplotype1.fasta CHM13-T2T.cds.fasta \\ > asm.hap1.cdna.paf minimap2 -cxsplice:hq -t32 \\ assembly.haplotype2.fasta CHM13-T2T.cds.fasta \\ > asm.hap2.cdna.paf ## run asmgene k8 /opt/nesi/CS400_centos7_bdw/minimap2/2.24-GCC-11.3.0/bin/paftools.js asmgene -a ref.cdna.paf asm.hap1.cdna.paf > verkko.haplotype1.asmgene.tsv k8 /opt/nesi/CS400_centos7_bdw/minimap2/2.24-GCC-11.3.0/bin/paftools.js asmgene -a ref.cdna.paf asm.hap2.cdna.paf > verkko.haplotype2.asmgene.tsv Tip Another popular tool for checking genome completeness using gene content is the software Benchmarking Universal Single-Copy Orthologs (BUSCO). This approach uses a set of evolutionarily conserved genes that are expected to be present at single copy for a given taxa, so one could check their genome to see if, for instance, it has all the genes predicted to be necessary for Aves or Vertebrata . This approach is useful if your de novo genome assembly is for a species that does not have a reference genome yet. And it's even faster now with the recently developed tool minibusco !","title":"5. Assembly QC"},{"location":"pages/4_assembly_qc/#5-assembly-qc","text":"Now that we have understood our data types (day 1) and put them through an assembly algorithm (day 2), we have this file of A's, T's, C's, and G's that's supposed to be our assembly. This file is meant to represent a biological reality, so let's try to assess its quality through several lens, some biological and some more technical. One way to remember the ways we evaluate assemblies is by thinking about the \"3C's\": contiguity , correctness , and completeness . Food for thought What do you think a 'good' de novo assembly looks like? What are some qualities of an assembly that you might be interested in measuring?","title":"5. Assembly QC"},{"location":"pages/4_assembly_qc/#contiguity-assembly-statistics-using-gfastats","text":"Recall that the sequences in our assembly are referred to as contigs . Normally, when we receive a hodgepodge of things with different values of the same variable, such as our contigs of varying lengths, we are inclined to use descriptive statistics such as average or median to try to get a grasp on how our data looks. However, it can be hard to compare average contig length between assemblies\u2014if they have the same total size and same number of contigs, it's still the same average, even if it's five contigs of 100bp, or one 460 bp contig and four 10bp ones! This matters for assembly because ideally we want fewer contigs that are larger . Median comes closer to reaching what we're trying to measure, but it can be skewed by having many very small contigs, so instead a popular metric for assessing assemblies is N50 . The N50 is similar to the median in that one must first sort the numbers, but then instead of taking the middle value, the N50 value is the length of the first contig that is equal to or greater than half of the assembly sum . But that can be hard to understand verbally, so let's look at it visually: Image adapted from Elin Videvall at The Molecular Ecologist . The N50 can be interpreted as such: given an N50 value, 50% of the sequence in that assembly is contained in contigs of that length or longer. Thus, N50 has been traditionally used as the assembly statistic of choice for comparing assemblies, as it's more intuitive (compared to average contig length) to see that an assembly with an N50 value of 100Mbp is more contiguous than one with an N50 value of 50MBp, since it seems like there are more larger contigs in the former assembly. Another statistic that is often reported with N50 is the L50 , which is the rank of the contig that gives the N50 value. For instance, in the above image, the L50 would be 3, because it would be the third largest contig that gives the N50 value. L50 is useful for contextualizing the N50, because it gives an idea of how many contigs make up that half of your assembly. N50 or NG50? Another measurement you might see is NG50. This is the N50 value, just calculated using a given genome size instead of the sum of the contigs. Given how the N50 value can be so affected by addition or removal of small contigs, another metric has come into use: the area under the (N50) curve , or the auN value. Though N50 is measured at the 50% mark, we could make similar values for any value of x, for instance N30 would be the value where 30% of the sequence in that assembly is of that length or longer. These metrics are thus called Nx statistics, and one could plot them against contig length to get an Nx curve , which gives a more nuanced view of the actual contig size distribution of your assembly. Image adapted from Heng Li's blog, which in turn adapts it from a NIBS workshop auN tries to capture the nature of this curve, instead of a value from an arbitrary point on it. On the above example, each step on the curve represents a contig (length on the y-axis), so the black curve is the most contiguous as it has one contig that covers over 40% of the assembly. Despite that, this assembly would have the same N50 value (on the x-axis) as multiple other assemblies that are more fragmented in the same area.","title":"Contiguity (assembly statistics using gfastats)"},{"location":"pages/4_assembly_qc/#run-gfastats-on-a-fasta","text":"Let's get some basic statistics for an assembly using a tool called gfastats , which will output metrics such as N50, auN, total size, etc. We can try it out on a Verkko trio assembly of HG002 that's already been downloaded onto NeSI. code cd ~/obss_2023/genome_assembly cd assembly_qc/gfastats ls -la # you should see a bunch of \"files\" that are actually symlinks pointing to their actual content Now we're ready to roll! module purge module load gfastats gfastats assembly.haplotype1.fasta It should take about 20 seconds Output +++Assembly summary+++: # scaffolds: 81 Total scaffold length: 3025610697 Average scaffold length: 37353218 .48 Scaffold N50: 112270693 Scaffold auN: 120344050 .34 Scaffold L50: 10 Largest scaffold: 242261106 Smallest scaffold: 7980 # contigs: 95 Total contig length: 3024505387 Average contig length: 31836898 .81 Contig N50: 101137168 Contig auN: 98109753 .40 Contig L50: 12 Largest contig: 201096255 Smallest contig: 7980 # gaps in scaffolds: 14 Total gap length in scaffolds: 1105310 Average gap length in scaffolds: 78950 .71 Gap N50 in scaffolds: 222184 Gap auN in scaffolds: 267123 .76 Gap L50 in scaffolds: 2 Largest gap in scaffolds: 425849 Smallest gap in scaffolds: 1000 Base composition ( A:C:G:T ) : 893171444 :618413603:615928136:896992204 GC content %: 40 .81 # soft-masked bases: 0 # segments: 95 Total segment length: 3024505387 Average segment length: 31836898 .81 # gaps: 14 # paths: 81","title":"Run gfastats on a FASTA"},{"location":"pages/4_assembly_qc/#run-gfastats-on-a-gfa","text":"Remember that the file we initially got was an assembly graph \u2014what if we wanted to know some graph-specific stats about our assembly, such as number of nodes or disconnected components? We can also assess that using gfastats. code gfastats --discover-paths unitig-normal-connected-tip.gfa Output +++Assembly summary+++: # scaffolds: 2312 Total scaffold length: 4179275425 Average scaffold length: 1807645 .08 Scaffold N50: 10829035 Scaffold auN: 11407399 .64 Scaffold L50: 134 Largest scaffold: 38686347 Smallest scaffold: 1987 # contigs: 2312 Total contig length: 4179275425 Average contig length: 1807645 .08 Contig N50: 10829035 Contig auN: 11407399 .64 Contig L50: 134 Largest contig: 38686347 Smallest contig: 1987 # gaps in scaffolds: 0 Total gap length in scaffolds: 0 Average gap length in scaffolds: 0 .00 Gap N50 in scaffolds: 0 Gap auN in scaffolds: 0 .00 Gap L50 in scaffolds: 0 Largest gap in scaffolds: 0 Smallest gap in scaffolds: 0 Base composition ( A:C:G:T ) : 1180405407 :909914907:908088683:1180866428 GC content %: 43 .50 # soft-masked bases: 0 # segments: 2312 Total segment length: 4179275425 Average segment length: 1807645 .08 # gaps: 0 # paths: 2312 # edges: 5790 Average degree: 2 .50 # connected components: 47 Largest connected component length: 530557693 # dead ends: 562 # disconnected components: 184 Total length disconnected components: 60717682 # separated components: 231 # bubbles: 8 # circular segments: 11 What's the --discover-paths flag for? gfastats tries to clearly distinguish contigs from segments, so it will not pick up on contigs in a GFA without paths defined. To get the contig stats as well as graph stats from these GFAs, you'll need to add the --discover-paths flag. Check out the graph-specific statistics at the end of the output.","title":"Run gfastats on a GFA"},{"location":"pages/4_assembly_qc/#compare-two-graphs-stats","text":"Now that we know how to get the statistics for one assembly, let's get them for two so we can actually compare them. We already compared a Verkko HiFi-only and HiFi+ONT graph visually, so let's do it with assembly stats this time. We're going to use a one-liner to put the assembly stats side-by-side, because it can be kind of cumbersome to scroll up and down between two separate command line runs and their outputs. code paste \\ < ( gfastats -t --discover-paths hifi-resolved.gfa ) \\ < ( gfastats -t --discover-paths unitig-normal-connected-tip.gfa \\ | cut -f 2 ) paste is a command that pastes two files side by side The <(COMMAND) syntax is called process substitution, and it passes the output of the command(s) inside the parentheses to another command (here it is passing the gfastats output to paste ), and can be useful when using a pipe ( | ) might not be possible The -t flag in gfastats specifies that the output should be tab-delimited, which makes it more computer-parsable The cut command in the substitution is just getting the actual statistics column from the gfastats output, because the first column is the name of the statistic Your output should look something like this: Output # contigs 68252 2312 Total contig length 3858601893 4179275425 Average contig length 56534 .63 1807645 .08 Contig N50 223090 10829035 Contig auN 638121 .24 11407399 .64 Contig L50 4232 134 Largest contig 23667669 38686347 Smallest contig 1608 1987 # gaps in scaffolds 0 0 Total gap length in scaffolds 0 0 Average gap length in scaffolds 0 .00 0 .00 Gap N50 in scaffolds 0 0 Gap auN in scaffolds 0 .00 0 .00 Gap L50 in scaffolds 0 0 Largest gap in scaffolds 0 0 Smallest gap in scaffolds 0 0 Base composition ( A:C:G:T ) 1089358958 :840831652:838233402:1090177881 1180405407 :909914907:908088683:1180866428 GC content % 43 .51 43 .50 # soft-masked bases 0 0 # segments 68252 2312 Total segment length 3858601893 4179275425 Average segment length 56534 .63 1807645 .08 # gaps 0 0 # paths 68252 2312 # edges 181274 5790 Average degree 2 .66 2 .50 # connected components 45 47 Largest connected component length 496518810 530557693 # dead ends 684 562 # disconnected components 18 184 Total length disconnected components 7267912 60717682 # separated components 63 231 # bubbles 4312 8 # circular segments 31 11 Output explained ... where the first column is the stats from the HiFi-only assembly graph, and the second column is the stats from the HiFi+ONT assembly graph. Notice how the HiFi-only graph has way more nodes than the HiFi+ONT one, like we'd seen in Bandage. Stats-wise, this results in the HiFi-only graph having a N50 value of 223 Kbp while the HiFi+ONT one is 10.8 Mbp, a whole order of magnitude larger. For the HiFi-only graph, though, there's a bigger difference between its N50 value and its auN value: 223 Kbp vs. 638 Kbp, while the HiFi+ONT stats have a smaller difference of 10.8 Mbp vs. 11.4 Mbp. This might be due to the HiFi-only graph having on average shorter segments and more of the shorter ones, since it doesn't have the ONT data to resolve the segments into larger ones.","title":"Compare two graphs' stats"},{"location":"pages/4_assembly_qc/#correctness-qv-using-merqury","text":"Correctness refers to the base pair accuracy, and can be measured by comparing one's assembly to a gold standard reference genome. This approach is limited by 1) an assumption about the quality of the reference itself and the closeness between it and the assembly being compared, and 2) the need for a reference genome at all, which many species do not have (yet). To avoid this, we can use Merqury : a reference-free suite of tools for assessing assembly quality (particularly w.r.t. error rate) using k -mers and the read set that generated that assembly. If an assembly is made up from the same sequences that were in the sequencing reads, then we would not expect any sequences ( k -mers) in the assembly that aren't present in the read set\u2014but we do find those sometimes, and those are what Merqury flags as error k -mers. It uses the following formula to calculate QV value, which typically results in QVs of 50-60 : \\[ \\Large E=1-P=1-\\left (1-\\frac{K_{\\textrm{asm}}}{K_{\\textrm{total}}} \\right ) ^{\\frac{1}{k}} \\] OK, but what does 'QV' mean, anyway? The QV that Merqury is interpreted similarly to the commonly used Phred quality scale, which might be familiar to those who have done short-read sequencing or are otherwise acquainted with FASTQ files. Phred quality scores are logarithmically related to error-probability, such that: - Phred score of 30 represents a 1 in 1,000 error probability ( i.e. , 99.9% accuracy) - Phred score of 40 represents a 1 in 10,000 error probability ( i.e. , 99.99% accuracy) - Phred score of 50 represents a 1 in 100,000 error probability ( i.e. , 99.999% accuracy) - Phred score of 60 represents a 1 in 1,000,000 error probability ( i.e. , 99.9999% accuracy) Merqury operates using k -mer databases like the ones we generated using meryl, so that's what we'll do now.","title":"Correctness (QV using Merqury)"},{"location":"pages/4_assembly_qc/#running-meryl-and-genomescope-on-the-e-coli-verkko-assembly","text":"Let's try this out on the E. coli Verkko assembly. First we need a Meryl database, so let's generate that. code cd ~/obss_2023/genome_assembly/assembly_qc mkdir merqury cd merqury We just made a directory for our runs, now let's sym link the fasta and reads here so we can refer to them more easily ln -s ~/obss_2023/genome_assembly/assembly/verkko_test/assembly/assembly.fasta . ln -s ~/obss_2023/genome_assembly//assembly/verkko_test/hifi.fastq.gz . Now we can run Mercury! meryl count k=30 memory=4 threads=2 hifi.fastq.gz output read-db.meryl --wrap ??? Previously, we used the sbatch command to submit a slurm script to the cluster and the slurm job handler. The sbatch command can actually take a lot of parameters like the ones we included in the beginning of our script, and one of those parameters is --wrap which kind of wraps whatever command you give it in a Slurm wrapper so that the cluster can schedule it as if it was a Slurm script. Take note that running a process in this manner is not reproducible. Unless you have access to the history logs, other researchers are not able to know what parameters you have used. Therefore, it is advisable to write it out in a Slurm script as below: code #!/bin/bash -e #SBATCH --account=nesi02659 #SBATCH --job-name=meryl #SBATCH --time=00:15:00 #SBATCH --cpus-per-task=2 #SBATCH --mem=4G #SBATCH --partition=milan # Modules module purge module load Merqury/1.3-Miniconda3 # Run meryl count k = 30 memory = 4 threads = 2 \\ hifi.fastq.gz \\ output read-db.meryl That shouldn't take too long to run. Now we have a Meryl DB for our HiFi reads. If we're curious about the distribution of our k -mers, we can use Meryl generate a histogram of the counts to show us how often a k -mer occurs only once in the reads, twice, etc. How would you go about trying to do this with meryl? When you want to use a tool to do something (and you are decently confident that the tool can actually do it), then a good point to start is just querying the tool's manual or help dialogue. Try out meryl --help and see if there's a function that looks like it could generate the histogram we want. spoiler alert: it's meryl histogram read-db.meryl If you tried to run that command with the output straight to standard out ( i.e. , your terminal screen), you'll see it's rather overwhelming and puts you all the way at the high, high coverage k -mer counts, which are only one occurrence. Let's look at just the first 100 lines instead. code meryl histogram read-db.meryl > read-db.hist head -n 100 read-db.hist This is more manageable, and you can even kind of see the histogram forming from the count values. There's a lot of k -mers that are present at only one copy (or otherwise very low copy) in the read set: these are usually sequencing errors, because there's a lot of these k -mers present at low copy. Because the sequence isn't actually real ( i.e. , it isn't actually in the genome and isn't actually serving as sequencing template), these k -mers stay at low copy. After these error k -mers, there's a dip in the histogram until about the 24\u201328 copy range. This peak is the coverage of the actual k -mers coming from the genome that you sequenced, thus it corresponds to having coverage of ~26X in this read set. We only have one peak here because this is a haploid dataset, but if your dataset is diploid then expect two peaks with the first peak varying in height depending on heterozygosity of your sample. \"What if I want a pretty graph instead of imagining it?\" Good news\u2014there's an app a program for that. GenomeScope is a straightforward program with an online web page where you can just drop in your meryl histogram file and it will draw the histogram for you as well as use the GenomeScope model to predict some genome characteristics of your data, given the expected ploidy. Let's try it out! Download the read-db.hist file and throw it into the GenomeScope website: http://qb.cshl.edu/genomescope/genomescope2.0/ and adjust the parameters accordingly. Can I use GenomeScope to QC my raw data before assembly? As you can see here, GenomeScope can be useful for getting an idea of what your raw dataset looks like, as well as feeling out the genome that should be represented by those sequencing reads. This can be useful as a QC step before even running the assembly, to make sure that your dataset is good enough to use. Here's an example of a good diploid dataset of HiFi reads: How does the data look? What does the coverage look to be? How many peaks are there in the data and what do they represent? What are some characteristics of the genome as inferred by GenomeScope? This data looks good, and you know that 1) because this tutorial text already called it good previously, and 2) there's a good amount of coverage, around 40X diploid coverage in fact. Additionally, the peaks are all very clear and distinct from each other and from the error k -mer slope on the left. Recall that the first peak represents haploid coverage (i.e., coverage of heterozygous loci) and the second peak is diploid coverage. GenomeScope is predicting the total size of the genome to be about 2.2 Gbp with 1.23% heterozygosity. This is data for Microtus pennsylvaticus , the eastern meadow vole. Here's an example of another HiFi dataset: Compare this to the previous examples. Does this look like a good dataset? Ignoring how GenomeScope itself is getting confused and can't get its model to fit properly to the k -mer spectra, let's look at the actual observed k -mer spectra. It does look like there's potentially two peaks whose distributions are overlapping, one peak around 10X and the other just under 20X. These are presumably our haploid and diploid peaks, but there's not enough coverage to resolve them properly here. This is an example of a GenomeScope QC that would tell me we don't have enough HiFi data to continue onto assembly, let's try to generate some more data. Wow, more data! This result is from after adding one more (notably more successful) SMRT cell of HiFi data and re-running GenomeScope. We can see the resolution of the peaks much more cleanly, and the GenomeScope model fits the data much better now, so we can trust the genome characteristic estimates here more than before. If you noted the comparatively small genome size and wondered what this was, it's Anadara tuberculosa , the piangua. Now you might be wondering: what happens if I try to assemble data without enough coverage? Answer: a headache. The assembly that results from the dataset that made the first GenomeScope plot resulted in two haplotypes of over 3,000 contigs each, which is very fragmented for a genome this small, recapitulated by their auN values being ~0.5 Mbp. In comparison, an assembly with the dataset pictured in the second GenomeScope plot resulted in two haplotypes of 500-800 contigs with auN values of 3.6-3.9 Mbp! The improvement in contiguity can also be visualized in the Bandage plots: The above is the Hifiasm unitig graph for the assembly done without good HiFi coverage. The above is the Hifiasm unitig graph for the assembly done with good (~56X) HiFi coverage.","title":"Running Meryl and GenomeScope on the E. coli Verkko assembly"},{"location":"pages/4_assembly_qc/#using-merqury-in-solo-mode","text":"Let's use Merqury on that database we just made to get the QV and some plots. Use your text editor of choice to make a Slurm script ( run_merqury.sl ) to run the actual Merqury program with the following contents: code #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name merqury1 #SBATCH --cpus-per-task 8 #SBATCH --time 00:15:00 #SBATCH --mem 1G #SBATCH --output slurmlogs/%x.%j.log #SBATCH --error slurmlogs/%x.%j.err #SBATCH --partition milan ## load modules module purge module load Merqury export MERQURY = /opt/nesi/CS400_centos7_bdw/Merqury/1.3-Miniconda3/merqury ## create solo merqury dir and use it mkdir -p merqury_solo cd merqury_solo ## run merqury merqury.sh \\ ../read-db.meryl \\ ../assembly.fasta \\ output What's that export command doing there? Merqury as a package ships with a lot of scripts, especially for plotting. The merqury.sh command that we're using is calling those scripts, but we need to tell it where we installed Merqury. To find out the QV, we want the file named output.qv . Take a look at it and try to interpret the QV value you find (third column). If we recall the Phred scale system, this would mean that this QV value is great! Which is not surprising, considering we used HiFi data. It's worth noting, though, that we are using HiFi k -mers to evaluate sequences derived from those same HiFi reads. This does a good job of showing whether the assembly worked with that data well, but what if the HiFi data itself is missing parts of the genome, such as due to bias ( e.g. , GA dropout)? That's why it's important to use orthogonal datasets made using different sequencing technology, when possible. For instance, we can use an Illumina-based Meryl database to evaluate a HiFi assembly. For non-human vertebrates, this often results in the QV dropping from 50-60 to 35-45, depending on the genome in question.","title":"Using Merqury in solo mode"},{"location":"pages/4_assembly_qc/#using-merqury-in-trio-mode","text":"We just ran Merqury on our E. coli assembly, and evaluated it using the HiFi reads that we used for that assembly. Merqury can also utilize trio data (using those hapmer DBs we talked about before) to evaluate the phasing of an assembly, so let's try that with our HG002 trio data. We can create run_merqury_trio.sl to do so. code #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name merqury2 #SBATCH --cpus-per-task 8 #SBATCH --time 02:00:00 #SBATCH --mem 40G #SBATCH --output slurmlogs/%x.%j.log #SBATCH --error slurmlogs/%x.%j.err #SBATCH --partition milan ## load modules module purge module load Merqury export MERQURY = /opt/nesi/CS400_centos7_bdw/Merqury/1.3-Miniconda3/merqury ## create trio merqury dir and use it cd ~/obss_2023/genome_assembly/assembly_qc mkdir -p merqury_trio cd merqury_trio # Go get the necessary files OBSS_RESOURCES = /nesi/project/nesi02659/obss_2023/resources/genome_assembly ln -s $OBSS_RESOURCES /assembly.*.fasta . ln -s $OBSS_RESOURCES /maternal.k30.hapmer.meryl . ln -s $OBSS_RESOURCES /paternal.k30.hapmer.meryl . ## let's run the program in a results directory to make things a little neater mkdir -p results cd results ## run merqury merqury.sh \\ ../read-db.meryl \\ ../paternal.k30.hapmer.meryl \\ ../maternal.k30.hapmer.meryl \\ ../assembly.haplotype1.fasta \\ ../assembly.haplotype2.fasta \\ output You can also look in this folder for the merqury outputs if there isn't enough time to run the actual program: /nesi/nobackup/nesi02659/LRA/resources/merqury There will be lots of outputs, but let's look at two now to get an idea for how the sequences have been partitioned between our assemblies, and whether that's consistent with information we know from trio sequencing. First let's look at the spectra-asm plot: Merqury's spectra plots take the kmer spectrum (like you'd seen in GenomeScope) and color the kmers according to where the kmer is found: the reads only , one of the assemblies, or both of the assemblies. A useful output from Merqury for evaluating phasing is the blob plot: In this plot, each blob is a contig, and its x,y position represents parental hapmer content, while color represents assembly-of-origin.","title":"Using Merqury in trio mode"},{"location":"pages/4_assembly_qc/#what-do-different-phasing-approaches-look-like-in-merqury","text":"Now we know what a nice trio-phased assembly looks like in Merqury, but what do the other options (Hi-C phasing or no phasing at all) look like? Let's look at an example from the zebra finch ( Taeniopygia guttata ), where the Vertebrate Genomes Project (VGP) has used hifiasm on the same HiFi dataset with different phasing approaches and evaluated the resulting assemblies with trio data in order to benchmark the different methods. On the left, we have the blob plot for the trio assembly, which looks nicely phased as we expect -- all the contigs exhibit hapmers from only one parent (which you can tell because all blobs are along the x- or y- axis), and on top of that all the contigs from one assembly show only one parent's hapmers ( i.e. , hap1's red blobs all show only bTaeGut3 hapmers). In the middle, we have a blob plot for a primary/alternate set of assemblies generated without any phasing data. You'll notice that a lot of the primary assembly blobs are not on the axes, meaning they have hapmers from both parents. Why are the alternate contigs all in phase? It starts to make sense if you think about what the alternae assembly is meant to represent: the haplotigs. These are the alternate alleles for the heterozygous loci, so it would track that only one parent's hapmers are represented -- the alternate assembly is a collection of the \"other side of the bubble\"s when looking at the assembly graph. The right plot is a blob plot from the Hi-C-phased assembly. Notably, most of the contigs are able to be properly phased, since they are almost all on the x- or -y axis.","title":"What do different phasing approaches look like in Merqury?"},{"location":"pages/4_assembly_qc/#switch-and-hamming-errors-using-yak","text":"Two more types of errors we use to assess assemblies are switch errors and Hamming errors. Hamming errors represent the percentage of SNPs wrongly phased (compared to ground truth), while switch errors represent the percentage of adjacent SNP pairs wrongly phased. See the following graphic: Let's first create a director within assembly_qc for it. code cd ~/obss_2023/genome_assembly/assembly_qc mkdir -p yak cd yak As the image illustrates, switch errors occur when an assembly switches between haplotypes. These errors are more prevalent in pseudohaplotype ( e.g. , primary/alternate) assemblies that did not use any phasing data, as the assembler has no way of properly linking haplotype blocks, which can result in mixed hapmer content contigs that are a chimera of parental sequences. code #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name yaktrioeval #SBATCH --cpus-per-task 32 #SBATCH --time 01:00:00 #SBATCH --mem 256G #SBATCH --output slurmlogs/%x.%j.log #SBATCH --error slurmlogs/%x.%j.err #SBATCH --partition milan ## change to qc dir, link the necessary files cd ~/obss_2023/genome_assembly/assembly_qc/yak OBSS_RESOURCES = /nesi/project/nesi02659/obss_2023/resources/genome_assembly ln -s $OBSS_RESOURCES /yak . ln -s $OBSS_RESOURCES /hic . ln -s $OBSS_RESOURCES /trio . mkdir -p qc_yak cd qc_yak ## load modules module purge module load yak ## run yak yak trioeval -t 32 \\ ../yak/pat.HG003.yak ../yak/mat.HG004.yak \\ ../hic/HG002.hap1.fa.gz \\ > hifiasm.hic.hap1.trioeval yak trioeval -t 32 \\ ../yak/pat.HG003.yak ../yak/mat.HG004.yak \\ ../trio/HG002.mat.fa.gz \\ > hifiasm.trio.mat.trioeval","title":"Switch and Hamming errors using yak"},{"location":"pages/4_assembly_qc/#completeness-asmgene","text":"Another way to assess an assembly is via completeness , particularly with regard to expected gene content. If you have a reference genome that's been annotated with coding sequences, then you can use the tool asmgene to align multi-copy genes to your assembly and see if they remain multi-copy, or if the assembler has created a misassembly. asmgene works by aligning annotated transcripts to the reference genome, and record hits if the transcript is mapped at or over 99% identity over 99% or greater of the transcript length. If the transcript only has one hit, then it is single-copy (SC), otherwise it's multi-copy (MC). The same is then done for your assembly, and the fraction of missing multi-copy (%MMC) gene content is computed. A perfect assembly would have %MMC be zero, while a higher fraction indicates the assembly has collapsed some of these multi-copy genes. Additionally, you can look at the presence (or absence!) of expected single-copy genes in order to check gene completeness of the assembly. The output will be a tab-delimed list of metrics and the value of that metric for the reference and for your given assembly. The line full_sgl gives the number of single-copy genes present in the reference and your assembly\u2014if these numbers are off-balanced, then you might have false duplications, which are also pointed out on the full_dup line. For the multi-copy genes, you can look at dup_cnt to see the number of multi-copy genes in the reference and see how many of those genes are still multi-copy in your assembly. You can then use these values to calculate %MMC via the formula 1 - (dup_cnt asm / dup_cnt ref) . Let's try running asmgene on haplotype1 and haplotype2 from the pre-baked Verkko trio assemblies. code ## asmgene cd ~/obss_2023/genome_assembly/ mkdir -p ~/obss_2023/genome_assembly/assembly_qc/asmgene cd assembly_qc/asmgene # let's symlink some of the necessary files OBSS_RESOURCES = /nesi/project/nesi02659/obss_2023/resources/genome_assembly ln -s $OBSS_RESOURCES /chm13v2.0.fa . ln -s $OBSS_RESOURCES /CHM13-T2T.cds.fasta . ln -s $OBSS_RESOURCES /assembly.haplotype1.fasta . ln -s $OBSS_RESOURCES /assembly.haplotype2.fasta . Now that we have our files, we're ready to go. Make a script with the following content and run it in the directory with the appropriate files: code #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name asmgene #SBATCH --cpus-per-task 32 #SBATCH --time 05:00:00 #SBATCH --mem 256G #SBATCH --output slurmlogs/%x.%j.log #SBATCH --error slurmlogs/%x.%j.err #SBATCH --partition milan ## load modules module purge module load minimap2/2.24-GCC-11.3.0 ## run minimap2 on ref, hap1, and hap2 minimap2 -cxsplice:hq -t32 \\ chm13v2.0.fa CHM13-T2T.cds.fasta \\ > ref.cdna.paf minimap2 -cxsplice:hq -t32 \\ assembly.haplotype1.fasta CHM13-T2T.cds.fasta \\ > asm.hap1.cdna.paf minimap2 -cxsplice:hq -t32 \\ assembly.haplotype2.fasta CHM13-T2T.cds.fasta \\ > asm.hap2.cdna.paf ## run asmgene k8 /opt/nesi/CS400_centos7_bdw/minimap2/2.24-GCC-11.3.0/bin/paftools.js asmgene -a ref.cdna.paf asm.hap1.cdna.paf > verkko.haplotype1.asmgene.tsv k8 /opt/nesi/CS400_centos7_bdw/minimap2/2.24-GCC-11.3.0/bin/paftools.js asmgene -a ref.cdna.paf asm.hap2.cdna.paf > verkko.haplotype2.asmgene.tsv Tip Another popular tool for checking genome completeness using gene content is the software Benchmarking Universal Single-Copy Orthologs (BUSCO). This approach uses a set of evolutionarily conserved genes that are expected to be present at single copy for a given taxa, so one could check their genome to see if, for instance, it has all the genes predicted to be necessary for Aves or Vertebrata . This approach is useful if your de novo genome assembly is for a species that does not have a reference genome yet. And it's even faster now with the recently developed tool minibusco !","title":"Completeness (asmgene)"},{"location":"pages/5_assembly_cleanup_annotation/","text":"6. Assembly Cleanup & Genome Annotation \u00b6 Assembly Cleanup \u00b6 Once you have a genome, and you think you want to hold onto it for a while and start doing actual analysis it is time to check it for contamination. In particular, before using and sharing your assembly it is best to make sure that you don't have: Contamination from other species Lots of contigs for Epstein Barr Virus (EBV) or mitochondrial sequence Adapters embedded in your contigs When you upload your assembly to Genbank, the sequence is automatically screen for contaminants and if anything is found you have to fix it and upload the fixed assembly. It's much better to take a look on your end. Luckily NCBI has released a toolkit that can be run locally. There are tools to look find/fix adapter sequences in your assembly (fcs-adapter) and for finding/fixing foreign genomic contamination (fcs-gx). We will be running fcs-gx, so let's look at the steps that fcs-gx runs. Repeat and low-complexity sequence masking Alignment to reference database using GX aligner Alignment refinement with high-scoring taxa matches Classifying sequences to assign taxonomic divisions Generating contaminant cleaning actions With that understanding, we are ready to test it out for ourselves... Make a directory code cd ~/obss_2023/genome_assembly/ mkdir -p cleanup/fcs/ cd cleanup/fcs/ Download the Foreign Contamination Screen (FCS) tool from NCBI code curl -LO https://github.com/ncbi/fcs/raw/main/dist/fcs.py curl -LO https://github.com/ncbi/fcs/raw/main/examples/fcsgx_test.fa.gz This tool is a python script that calls a Docker/Singularity container. This was done because contamination screens notoriously require a ton of dependencies. So having a Docker container makes things easier on the user. The docker container requires that a database of contaminants are downloaded. We have already downloaded the test database here: test-only`. The container has already been downloaded as well, we just need to load the singularity module and let FCS know where the container is: code module purge module load Python/3.8.2-gimkl-2020a module load Singularity/3.11.3 export FCS_DEFAULT_IMAGE = /opt/nesi/containers/fcs/fcs-gx-0.4.0.sif Now we can run the test data Note that the test data is not human (this matters for the --tax-id parameter). code python3 ./fcs.py \\ screen genome \\ --fasta ./fcsgx_test.fa.gz \\ --out-dir ./gx_out \\ --gx-db test-only \\ --tax-id 6973 Here is what is printed to the screen tax-id : 6973 fasta : /sample-volume/fcsgx_test.fa.gz size : 8 .55 MiB split-fa : True BLAST-div : roaches gx-div : anml:insects w/same-tax: True bin-dir : /app/bin gx-db : /app/db/gxdb/test-only/test-only.gxi gx-ver : Mar 10 2023 15 :34:33 ; git:v0.4.0-3-g8096f62 output : /output-volume//fcsgx_test.fa.6973.taxonomy.rpt -------------------------------------------------------------------- GX requires the database to be entirely in RAM to avoid thrashing. Consider placing the database files in a non-swappable tmpfs or ramfs. See https://github.com/ncbi/fcs/wiki/FCS-GX for details. Will prefetch ( vmtouch ) the database pages to have the OS cache them in main memory. Prefetching /app/db/gxdb/test-only/test-only.gxs 99 %... Prefetched /app/db/gxdb/test-only/test-only.gxs in 0 .243985s ; 0 .290255 GB/s. The file is 100 % in RAM. Prefetching /app/db/gxdb/test-only/test-only.gxi 99 %... Prefetched /app/db/gxdb/test-only/test-only.gxi in 7 .24798s ; 0 .62397 GB/s. The file is 100 % in RAM. Collecting masking statistics... Collected masking stats: 0 .0295689 Gbp ; 3 .21688s ; 9 .19177 Mbp/s. Baseline: 1 .0774 28 .2MiB 0 :00:20 [ 1 .34MiB/s ] [ 1 .34MiB/s ] [==========================================================================] 102 % Processed 714 queries, 29 .1754Mbp in 14 .3783s. ( 2 .02913Mbp/s ) ; num-jobs:294 Warning: asserted div 'anml:insects' is not represented in the output! -------------------------------------------------------------------------------------------------- Warning: Asserted tax-div 'anml:insects' is well-represented in db, but absent from inferred-primary-divs. This means that either asserted tax-div is incorrect, or the input is predominantly contamination. Will trust the asserted div and treat inferred-primary-divs as contaminants. -------------------------------------------------------------------------------------------------- Asserted div : anml:insects Inferred primary-divs : [ 'prok:CFB group bacteria' ] Corrected primary-divs : [ 'anml:insects' ] Putative contaminant divs : [ 'prok:CFB group bacteria' ] Aggregate coverage : 51 % Minimum contam. coverage : 30 % -------------------------------------------------------------------- fcs_gx_report.txt contamination summary: ---------------------------------------- seqs bases ----- ---------- TOTAL 243 27170378 ----- ----- ---------- prok:CFB group bacteria 243 27170378 -------------------------------------------------------------------- fcs_gx_report.txt action summary: --------------------------------- seqs bases ----- ---------- TOTAL 243 27170378 ----- ----- ---------- EXCLUDE 214 25795430 REVIEW 29 1374948 -------------------------------------------------------------------- There is a file created called something like *fcs_gx_report.txt . Open it in your terminal, or if it's easier you can view it on the FCS GitHub . Here's how you would run your actual data We will copy over our prebaked Hifiasm trio assembly (maternal haplotype) and create a slurm script... cp /nesi/project/nesi02659/obss_2023/resources/genome_assembly/trio/HG002.mat.fa.gz . nano fcs_full.sl And paste in the following #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name test_fcs #SBATCH --cpus-per-task 24 #SBATCH --time 03:00:00 #SBATCH --mem 460G #SBATCH --output slurmlogs/test.slurmoutput.%x.%j.log #SBATCH --error slurmlogs/test.slurmoutput.%x.%j.err ## load modules module purge module load Python/3.7.3-gimkl-2018b module load Singularity/3.11.3 export FCS_DEFAULT_IMAGE = /opt/nesi/containers/fcs/fcs-gx-0.4.0.sif python3 ./fcs.py \\ screen genome \\ --fasta ./HG002.mat.fa.gz \\ --out-dir ./asm_fcs_output \\ --gx-db /nesi/project/nesi02659/obss_2023/resources/genome_assembly/gxdb \\ --tax-id 9606 Then you would just run the slurm script. Don't do this now. The results are boring for this assembly and the run takes 500GB of memory! (This is required to load the contamination database into memory -- if you don't give fcs enough memory it will take much much longer.) We've run this for you, and you can find the results in the folder asm_fcs_output : Genome Annotation \u00b6 A genome assembly\u2014even a very good one\u2014is of limited utility without annotation to provide context about the raw bases. Broadly speaking, an annotation is any information about some region of the genome, e.g., the GC% in a given window or whether and in which way a particular region is repetitive. Accordingly, annotation techniques, computational resources, input requirements, and output formats can vary widely. However, the term \"annotation\" typically refers to the more specific case of locating the protein-coding genes, ideally specifying features such as exon-intron boundaries, untranslated regions (UTRs), splice variants, etc. Accordingly, we will focus on this type of annotation here. When annotating genes, many methods and tools are available, depending on your objective. When planning a project, you might ask yourself: Do I want structural annotation , functional annotation , or both? Do I want to locate genes using de novo prediction models, transcript- or RNA-seq-based evidence, or both? Do I have a reliable source of annotations from the assembly of a related genome? Do I have a collaborator willing and able to perform the annotation for me? A full discussion or tutorial covering the various scenarios is beyond the scope of this workshop, but pipelines like MAKER (Holt and Yandell, 2011; doi: 10.1186/1471-2105-12-491 ) can be configured to do many of these things, though installing all necessary dependencies can be challenging in some circumstances. Many tools also assume short reads (e.g., when using RNA-seq evidence), and incorporating information from long-read sources may require adjustments. Here we will get a taste for annotation with one popular tool: Liftoff, which makes use of another genome assembly with annotations you wish to \"copy\" onto your assembly. What is the easiest way to annotate my genome of interest ? The easiest way to get a genome annotated is to have someone else do it. If sharing data with NCBI is possible and your assembly is the best option to represent your species of interest, they may be willing to annotate your genome for you using their pipeline . Otherwise, finding a collaborator with expertise is a good option. What are the most common formats for sharing annotation data ? The most common formats are BED (Browser Extensible Data) , GFF (Generic Feature Format; v3) , GTF (General Transfer Format; a.k.a., deprecated GFF v2) , and custom TSV (tab-separated value). Wiggle format and its variants are also common for displaying information in a genome browser. Which tool(s) should I use for my project ? Annotation is a complex problem, and no single tool exists that can be universally recommended. A high-quality annotation plan often requires the use of may tools and/or complex pipelines, and the installation of many of these tools can be complicated, even for expert command-line users. Generally speaking, following the best-practice of those in your field or who work on the same taxa is a reasonable option. In some cases, tools specific to some set of organisms have been developed (e.g., Funannotate for fungi). Recently, the BRAKER team released version 3 of their pipeline for gene structure prediction (wrapping GeneMark-ES/ET & AUGUSTUS). If you have a trustworthy source of annotations from another assembly, you can consider Liftoff and CAT . InterProScan can give you functional annotations relatively quickly. If you are able to share your data with NCBI and your assembly is the best assembly (or if the community agrees it is otherwise preferred), the NCBI annotation team will annotate it for you using their automated pipeline. GFF3 Toolkit can be useful when working with GFF3 files, and gFACs can help with filtering, analysis, and conversion tasks. How can I learn more about annotation ? Please consider the following sources: - Review of eukaryotic genome annotation written for beginners (Yandell and Ence, 2012; doi: <a href=\"https://doi.org/10.1038/nrg3174\">10.1038/nrg3174</a>) - Review of assembly and annotation written for conservation geneticists and assuming limited understanding of bioinformatics and high-throughput sequencing (Ekblom and Wolf, 2014; doi: <a href=\"https://doi.org/10.1111/eva.12178\">10.1111/eva.12178</a>) - Review of structural and functional annotation, providing definitions and the limitations of annotation (Mudge and Harrow, 2016; doi: <a href=\"https://doi.org/10.1038/nrg.2016.119\">10.1038/nrg.2016.119</a>) - Protocol (from <a href=\"https://www.protocols.io\">protocols.io</a>) for <em>de novo</em> annotation using the <a href=\"https://yandell-lab.org/software/maker.html\">MAKER</a> pipeline. This is annotation \"in the wild\" describing actual steps taken if not the justification for them, but it is based on this <a href=\"https://weatherby.genetics.utah.edu/MAKER/wiki/index.php/MAKER_Tutorial_for_WGS_Assembly_and_Annotation_Winter_School_2018\">2018 tutorial</a> by the developers of MAKER. <a href=\"https://doi.org/10.17504/protocols.io.b3xvqpn6\">The protocol</a> was used to annotate a non-model fish genome (Pickett and Talma <em>et al.</em>, 2022; doi: <a href=\"https://doi.org/10.46471/gigabyte.44\">10.46471/gigabyte.44</a>). Annotation with Liftoff \u00b6 According to the Liftoff GitHub Repository : Liftoff is a tool that accurately maps annotations in GFF or GTF between assemblies of the same, or closely-related species. Unlike current coordinate lift-over tools which require a pre-generated \"chain\" file as input, Liftoff is a standalone tool that takes two genome assemblies and a reference annotation as input and outputs an annotation of the target genome. Liftoff uses Minimap2 (Li, 2018; doi: 10.1093/bioinformatics/bty191 ) to align the gene sequences from a reference genome to the target genome. Rather than aligning whole genomes, aligning only the gene sequences allows genes to be lifted over even if there are many structural differences between the two genomes. For each gene, Liftoff finds the alignments of the exons that maximize sequence identity while preserving the transcript and gene structure. If two genes incorrectly map to overlapping loci, Liftoff determines which gene is most-likely mis-mapped, and attempts to re-map it. Liftoff can also find additional gene copies present in the target assembly that are not annotated in the reference. We will use Liftoff (doi: 10.1093/bioinformatics/btaa1016 ) to lift over the annotations from the T2T-CHM13 reference assembly to our assembly of HG002. Create directory code cd ~/obss_2023/genome_assembly/ cd liftoff-annotation Gather the necessary files What's with the *.sqlite3 file? Liftoff stores features from the GFF file in a SQLite database. The first part of any liftoff run processes the GFF file and creates the database, which by default is written to the same file location and name with a .gz appended to the end. If you experience an error at any point (e.g., failing to allocate enough memory to your job), liftoff will automatically re-create the database, even if that part of the process completed successfully previously. This SQLite database is from a previous run we did, and we\u2019ll use it in place of the GFF file to shave some time (~10 mins for a full human genome) during our run. If you run liftoff -h , you\u2019ll see that -g (to provide the input GFF file) and -db (to provide the input SQLite database) are interchangeable. We\u2019ll be using the database to save time, but we could instead use the other option and supply the GFF file. If you haven\u2019t yet viewed a GFF file, now would be a good time to check one out: less -S chm13-annotations.gff.gz Run Liftoff First create a shell script liftoff.sh with the following content: code #!/bin/bash set -euo pipefail module purge module load Liftoff/1.6.3.2-gimkl-2022a-Python-3.11.3 liftoff \\ -p 8 \\ -db chm13-annotations.gff.liftoff.sqlite3 \\ -o asm.hap1.annotations.gff \\ asm.hap1.fa \\ chm13.fa What do each of these options do? -p specifies the number of threads to use. -db specifies the location of the SQLite database of features extracted by Liftoff from the GFF file with the input annotations for the reference. -o specifies the location of the GFF file with the output annotations for the target. The two positional parameters at the end are respectively the target assembly (our HG002 assembly) and the reference assembly (T2T-CHM13). Run the following command to see all the options described in more detail: liftoff -h This will probably take ~1 hour with 8 CPUs and 48 GB of memory. You could submit it as a job; however, we will pull pre-computed results for the next step. How to submit Liftoff with sbatch code sbatch -J liftoff -N1 -n1 -c8 --mem = 48G -t 0 -02:00 -A nesi02659 -o %x.%j.log liftoff.sh To skip the run-time, we have pre-computed the output 'asm.hap1.annotations.gff' somewhere on nesi. You can access by creating a lin (i.e. a shortcut): code ln -s /nesi/nobackup/nesi02659/LRA/resources/liftoff/asm.hap1.annotations.gff . Look at the output GFF3 file code less -S asm.hap1.annotations.gff You could also explore the files in Liftoff's intermediate directory ( /nesi/nobackup/nesi02659/LRA/resources/liftoff/intermediate_files ) or the unmapped features ( /nesi/nobackup/nesi02659/LRA/resources/liftoff/unmapped_features.txt ). Visualize the annotations in a genome browser Before we visualize the annotations in IGV, it is best if we sort and index the output GFF file: code module load IGV/2.16.1 igvtools sort asm.hap1.annotations.gff asm.hap1.annotations.sorted.gff igvtools index asm.hap1.annotations.sorted.gff These operations are small enough that you should not need to submit a job for them. To open IGV and view the annotations, do the following: Open IGV Open a Virtual Desktop according to these instructions Open the Terminal Emulator application Load the IGV module and launch IGV module load IGV/2.16.1 igv.sh Load the HG002 hap1 genome (instead of the default hg19): Genome > Load Genome from File... > asm.hap1.fa Load the GFF file: File > Load from File... > asm.hap1.annotations.sorted.gff Explore What do you observe? Do you have any questions? Long Read/Contig Mapping \u00b6 We are ready to map long reads and assembly contigs to genomes. For this we need a set of tools that are different from what you might be used to using for Illumina data. You might be asking \"what's the matter with bwa-mem for long reads?\" It is slow and probably inaccurate for this application. So let's jump in to long read mappers and see what we can do with them. Mashmap: Super Fast (Approximate) Mapping \u00b6 The first thing we would like to do is to find out how our assembled genome compares to the T2T genome CHM13. If we can map our assembly quickly onto CHM13, we can answer questions like: Which contigs correspond to chr1? How many of my contigs are T2T assembled? Are we seeing any large duplications or missing regions? MashMap is a super fast mapper that is commonly used for these kinds of questions. MashMap doesn't seed and extend, it takes sequences and plays tricks with kmers. Loosely speaking if you take a sequence and get all of its kmers of a certain size and then sort those kmers, you can do a lot with just the \"smallest\" kmer (this is called a minimizer). MashMap uses a set of those smallest kmers to say: this sequence here and my query sequence share a lot of smallest kmers. The output of MashMap is an approximation of read position and identity. Let's actually use it now to align our asembly to CHM13 and we will use the results to map our contigs onto chromosomes and to try and find some T2T chromosomes in our assemblies. Create A Directory code cd ~/obss_2023/genome_assembly/ cd annotation/mashmap Link the files we need code We are going to use CHM13 v2.0 (which includes a Y chromosome) as well as haplotype 1 from our Verkko trio assembly in the file `assembly.haplotype1.fasta. Run MashMap code module purge module load MashMap/3.0.4-Miniconda3 mashmap \\ -r chm13v2.0.fa \\ -q assembly.haplotype1.fasta \\ -f one-to-one \\ --pi 95 \\ -s 100000 \\ -t 4 \\ -o asm-to-chm13.mashmap.out Take a look at the parameters for MashMap. What are -f , --pi , and -s ? --pi is the percent identity. The default value is 85 which means that mappings with 85% or more identity should be reported. -s is the segment length. MasMap will not report segments under this value. -f is the filter mode. After MashMap identifies mappings, it can go through and filter to ensure only the best mapping is reported (if we pass \"map\") or in our case that all good mappings are reported (if we pass \"one-to-one\"). Why did we use the values that we used for --pi and -s ? Since we are mapping a human assembly against another human assembly we expect the sequences to be very similar. This is why we overwrote the default value of 85 for --pi . If we were to use the default value for -s (5000) then we would likely see a lot of hits from homologous regions. This may be interesting, but if we want to know how good our assembly is extra hits will just confuse things. Once the mapping is done, let's take a look at the output. code head asm-to-chm13.mashmap.out Note that the output is similar to the Paired Alignment Format, or PAF , and has the following columns: query name length 0-based start end strand target name length start end mapping nucleotide identity (estimate) The most recent versions of MashMap actually output PAF by defalt. But not the version we are using here. Now generate a dot plot of the alignment code generateDotPlot png medium asm-to-chm13.mashmap.out And take a look at it (you can click on it in your file explorer). Does anything stick out to you ? There is no sequence in the assembly mapping to chrY. This makes sense as this is the maternal haplotype's assembly. Exercise: Can We Find Any Contigs/Scaffolds That Are T2T? Look through the dotplot to try and identify a contig or two that are assembled T2T. Once you've done that we will have to bring in another tool to make sure that there are actually telomeric repeats on the 5' and 3' ends. (Some people care about the presence of telomeric repeats when defining what T2T means.) Run the telo tool from seqtk to identify (human) telomeric ends: module load seqtk seqtk telo assembly.haplotype1.fasta > telos.bed Now look through the bed file and find the contig you are interested in to see if there are telomeres on both ends. Did you find any T2T contigs or Scaffolds ? In this assembly chromosomes 3, 10, 11, and 16 are assembled T2T with contigs (no gaps) and chr5 is assembled T2T in a scaffold. As of the writing of this document (June 2023) for a diploid human assembly you can expect to get around 12 chromosomes assembled T2T in contigs and an additional 12 or so assembled T2T in scaffolds with Verkko. Hifiasm is very similar but without scaffolds. This means that for a diploid verkko assembly about half of the chromosomes are T2T out of the box for ~40X HiFi and >15X (or so) UL ONT with either HiC or trio phasing. Pretty amazing. This sample (HG002) is not very heterozygous so it's actually a tough sample to assemble. That is why we aren't seeing very many T2T contigs/scaffolds. Sometimes you have to judge how good an assembly is on a per-sample basis. Closing notes This could all be automated, of course. The T2T and HPRC consortiums have workflows that go through assemblies to determine T2T status at the chromosomal level, but those workflows are pretty similar to what we did above, actually. Not having base-level information is actually ok for what we just did. If you would like base level information at only a few fold of the run cost of mashmap (so still very fast), then you probably want to check out wfmash . It can be used for alignments of long reads (100kb+) or assemblies at very large scales. Minimap2: \u00b6 If you are mapping long read data and you'd like to have base-level alignment, minimap2 is probably your first stop. Today we are going to use minimap2 to align ONT reads that have 5mC information stored in Mm/Ml tags to our diploid assembly. Create A Directory code cd ~/obss_2023/genome_assembly/ cd annotation/minimap2 We are going to use the diploid version of our Verkko trio assembly. (This just means that the maternal and paternal haplotypes are both included in the fasta.) Create a minimap2 slurm script Open your favourite text editor code nano ont_mm2.sl And paste in the following: code #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name minimap2-ont #SBATCH --cpus-per-task 48 #SBATCH --time 05:00:00 #SBATCH --mem 128G #SBATCH --partition milan #SBATCH --output slurmlogs/%x.%j.log #SBATCH --error slurmlogs/%x.%j.err ## load modules module purge module load minimap2/2.24-GCC-11.3.0 module load SAMtools/1.16.1-GCC-11.3.0 ## Create minimap2 index of our diploid assembly minimap2 \\ -k 17 \\ -I 8G \\ -d verkko_trio_diploid.fa.mmi \\ verkko_trio_diploid.fa ## minimap parameters appropriate for nanopore in_args = \"-y -x map-ont --MD --eqx --cs -Y -L -p0.1 -a -k 17 -K 10g\" #do the mapping with methylation tags by dumping the Mm/Ml tags to the fastq headers samtools fastq \\ -TMm,Ml 03_08_22_R941_HG002_2_Guppy_6.1.2_5mc_cg_prom_sup.bam \\ | minimap2 -t 24 ${ in_args } verkko_trio_diploid.fa.mmi - \\ | samtools view -@ 24 -bh - \\ | samtools sort -@ 24 - > \\ verkko_trio_diploid.mm2.5mC.bam samtools index verkko_trio_diploid.mm2.5mC.bam And run the script Terminal sbatch ont_mm2.sl This should take only 3 hours or so, but we have some pre-baked results for you already. We will use these results in the next section. Why did we align to the diploid version of our assembly ? The traditional thing to do is to align your data to a haploid or pseudo-haploid assembly like CHM13 or GRCh38. We are diploid individuals, though. And for some use cases we want to align long reads to a diploid assembly in order to have the reads segregate by haplotype. When we are aligning a samples reads to its own assembly this is especially important. Visualize The Alignments In IGV \u00b6 Before switching over to your virtual desktop, link the prerun alignments to your working directory code Now view the alignments in IGV Open an IGV window as you did above. Don't forget to module load IGV/2.16.1 (we need v 2.14 or greater) We need to use our assembly as the genome file. Do that by clicking the Genomes dropdown at the top and then select Load Genome From File . Navigate to your folder and select the Verkko trio genome that we copied in. Load the 5mC bam by clicking on the File dropdown then selecting Load From File Show the methylation predictions by right clicking on the alignment track and selecting Color alignments by , then click base modifications (5mc) Explore the data a bit Zoom in on a random region that is a few hundred basepairs in size. You can see the methylation levels in the track histogram and in the reads themselves. Red is methylated and blue is unmethylated. If you are familiar with ONT data already you know that ONT and PacBio have the ability to detect base modifications without any additional steps like bisulfite conversion. What we are adding here is the ability to look at methylation in the context of a sample's own assembly. Maybe that wouldn't matter if you are looking at regions of the genome which are structurally consistent across samples\u2014like promoters for well known genes. Take a look around mat-0000038 (this is the biggest contig that maps to chrX). Why did we choose to show you an example in chrX ? In male samples chrX is haploid, so outside of the psuedoautosomal, or PAR, regions we don't have to worry about whether or not our reads map to the correct haplotype. What would have happened if we had just aligned this data to CHM13 ? Centromeres are highly repetitive and alpha satellite arrays can vary from individual to individual in size by more than a factor of two. So aligning centromeric reads from one individual to another individual's assembly is an inherently dodgy proposition.","title":"6. Assembly Cleanup & Genome Annotation"},{"location":"pages/5_assembly_cleanup_annotation/#6-assembly-cleanup-genome-annotation","text":"","title":"6. Assembly Cleanup &amp; Genome Annotation"},{"location":"pages/5_assembly_cleanup_annotation/#assembly-cleanup","text":"Once you have a genome, and you think you want to hold onto it for a while and start doing actual analysis it is time to check it for contamination. In particular, before using and sharing your assembly it is best to make sure that you don't have: Contamination from other species Lots of contigs for Epstein Barr Virus (EBV) or mitochondrial sequence Adapters embedded in your contigs When you upload your assembly to Genbank, the sequence is automatically screen for contaminants and if anything is found you have to fix it and upload the fixed assembly. It's much better to take a look on your end. Luckily NCBI has released a toolkit that can be run locally. There are tools to look find/fix adapter sequences in your assembly (fcs-adapter) and for finding/fixing foreign genomic contamination (fcs-gx). We will be running fcs-gx, so let's look at the steps that fcs-gx runs. Repeat and low-complexity sequence masking Alignment to reference database using GX aligner Alignment refinement with high-scoring taxa matches Classifying sequences to assign taxonomic divisions Generating contaminant cleaning actions With that understanding, we are ready to test it out for ourselves... Make a directory code cd ~/obss_2023/genome_assembly/ mkdir -p cleanup/fcs/ cd cleanup/fcs/ Download the Foreign Contamination Screen (FCS) tool from NCBI code curl -LO https://github.com/ncbi/fcs/raw/main/dist/fcs.py curl -LO https://github.com/ncbi/fcs/raw/main/examples/fcsgx_test.fa.gz This tool is a python script that calls a Docker/Singularity container. This was done because contamination screens notoriously require a ton of dependencies. So having a Docker container makes things easier on the user. The docker container requires that a database of contaminants are downloaded. We have already downloaded the test database here: test-only`. The container has already been downloaded as well, we just need to load the singularity module and let FCS know where the container is: code module purge module load Python/3.8.2-gimkl-2020a module load Singularity/3.11.3 export FCS_DEFAULT_IMAGE = /opt/nesi/containers/fcs/fcs-gx-0.4.0.sif Now we can run the test data Note that the test data is not human (this matters for the --tax-id parameter). code python3 ./fcs.py \\ screen genome \\ --fasta ./fcsgx_test.fa.gz \\ --out-dir ./gx_out \\ --gx-db test-only \\ --tax-id 6973 Here is what is printed to the screen tax-id : 6973 fasta : /sample-volume/fcsgx_test.fa.gz size : 8 .55 MiB split-fa : True BLAST-div : roaches gx-div : anml:insects w/same-tax: True bin-dir : /app/bin gx-db : /app/db/gxdb/test-only/test-only.gxi gx-ver : Mar 10 2023 15 :34:33 ; git:v0.4.0-3-g8096f62 output : /output-volume//fcsgx_test.fa.6973.taxonomy.rpt -------------------------------------------------------------------- GX requires the database to be entirely in RAM to avoid thrashing. Consider placing the database files in a non-swappable tmpfs or ramfs. See https://github.com/ncbi/fcs/wiki/FCS-GX for details. Will prefetch ( vmtouch ) the database pages to have the OS cache them in main memory. Prefetching /app/db/gxdb/test-only/test-only.gxs 99 %... Prefetched /app/db/gxdb/test-only/test-only.gxs in 0 .243985s ; 0 .290255 GB/s. The file is 100 % in RAM. Prefetching /app/db/gxdb/test-only/test-only.gxi 99 %... Prefetched /app/db/gxdb/test-only/test-only.gxi in 7 .24798s ; 0 .62397 GB/s. The file is 100 % in RAM. Collecting masking statistics... Collected masking stats: 0 .0295689 Gbp ; 3 .21688s ; 9 .19177 Mbp/s. Baseline: 1 .0774 28 .2MiB 0 :00:20 [ 1 .34MiB/s ] [ 1 .34MiB/s ] [==========================================================================] 102 % Processed 714 queries, 29 .1754Mbp in 14 .3783s. ( 2 .02913Mbp/s ) ; num-jobs:294 Warning: asserted div 'anml:insects' is not represented in the output! -------------------------------------------------------------------------------------------------- Warning: Asserted tax-div 'anml:insects' is well-represented in db, but absent from inferred-primary-divs. This means that either asserted tax-div is incorrect, or the input is predominantly contamination. Will trust the asserted div and treat inferred-primary-divs as contaminants. -------------------------------------------------------------------------------------------------- Asserted div : anml:insects Inferred primary-divs : [ 'prok:CFB group bacteria' ] Corrected primary-divs : [ 'anml:insects' ] Putative contaminant divs : [ 'prok:CFB group bacteria' ] Aggregate coverage : 51 % Minimum contam. coverage : 30 % -------------------------------------------------------------------- fcs_gx_report.txt contamination summary: ---------------------------------------- seqs bases ----- ---------- TOTAL 243 27170378 ----- ----- ---------- prok:CFB group bacteria 243 27170378 -------------------------------------------------------------------- fcs_gx_report.txt action summary: --------------------------------- seqs bases ----- ---------- TOTAL 243 27170378 ----- ----- ---------- EXCLUDE 214 25795430 REVIEW 29 1374948 -------------------------------------------------------------------- There is a file created called something like *fcs_gx_report.txt . Open it in your terminal, or if it's easier you can view it on the FCS GitHub . Here's how you would run your actual data We will copy over our prebaked Hifiasm trio assembly (maternal haplotype) and create a slurm script... cp /nesi/project/nesi02659/obss_2023/resources/genome_assembly/trio/HG002.mat.fa.gz . nano fcs_full.sl And paste in the following #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name test_fcs #SBATCH --cpus-per-task 24 #SBATCH --time 03:00:00 #SBATCH --mem 460G #SBATCH --output slurmlogs/test.slurmoutput.%x.%j.log #SBATCH --error slurmlogs/test.slurmoutput.%x.%j.err ## load modules module purge module load Python/3.7.3-gimkl-2018b module load Singularity/3.11.3 export FCS_DEFAULT_IMAGE = /opt/nesi/containers/fcs/fcs-gx-0.4.0.sif python3 ./fcs.py \\ screen genome \\ --fasta ./HG002.mat.fa.gz \\ --out-dir ./asm_fcs_output \\ --gx-db /nesi/project/nesi02659/obss_2023/resources/genome_assembly/gxdb \\ --tax-id 9606 Then you would just run the slurm script. Don't do this now. The results are boring for this assembly and the run takes 500GB of memory! (This is required to load the contamination database into memory -- if you don't give fcs enough memory it will take much much longer.) We've run this for you, and you can find the results in the folder asm_fcs_output :","title":"Assembly Cleanup"},{"location":"pages/5_assembly_cleanup_annotation/#genome-annotation","text":"A genome assembly\u2014even a very good one\u2014is of limited utility without annotation to provide context about the raw bases. Broadly speaking, an annotation is any information about some region of the genome, e.g., the GC% in a given window or whether and in which way a particular region is repetitive. Accordingly, annotation techniques, computational resources, input requirements, and output formats can vary widely. However, the term \"annotation\" typically refers to the more specific case of locating the protein-coding genes, ideally specifying features such as exon-intron boundaries, untranslated regions (UTRs), splice variants, etc. Accordingly, we will focus on this type of annotation here. When annotating genes, many methods and tools are available, depending on your objective. When planning a project, you might ask yourself: Do I want structural annotation , functional annotation , or both? Do I want to locate genes using de novo prediction models, transcript- or RNA-seq-based evidence, or both? Do I have a reliable source of annotations from the assembly of a related genome? Do I have a collaborator willing and able to perform the annotation for me? A full discussion or tutorial covering the various scenarios is beyond the scope of this workshop, but pipelines like MAKER (Holt and Yandell, 2011; doi: 10.1186/1471-2105-12-491 ) can be configured to do many of these things, though installing all necessary dependencies can be challenging in some circumstances. Many tools also assume short reads (e.g., when using RNA-seq evidence), and incorporating information from long-read sources may require adjustments. Here we will get a taste for annotation with one popular tool: Liftoff, which makes use of another genome assembly with annotations you wish to \"copy\" onto your assembly. What is the easiest way to annotate my genome of interest ? The easiest way to get a genome annotated is to have someone else do it. If sharing data with NCBI is possible and your assembly is the best option to represent your species of interest, they may be willing to annotate your genome for you using their pipeline . Otherwise, finding a collaborator with expertise is a good option. What are the most common formats for sharing annotation data ? The most common formats are BED (Browser Extensible Data) , GFF (Generic Feature Format; v3) , GTF (General Transfer Format; a.k.a., deprecated GFF v2) , and custom TSV (tab-separated value). Wiggle format and its variants are also common for displaying information in a genome browser. Which tool(s) should I use for my project ? Annotation is a complex problem, and no single tool exists that can be universally recommended. A high-quality annotation plan often requires the use of may tools and/or complex pipelines, and the installation of many of these tools can be complicated, even for expert command-line users. Generally speaking, following the best-practice of those in your field or who work on the same taxa is a reasonable option. In some cases, tools specific to some set of organisms have been developed (e.g., Funannotate for fungi). Recently, the BRAKER team released version 3 of their pipeline for gene structure prediction (wrapping GeneMark-ES/ET & AUGUSTUS). If you have a trustworthy source of annotations from another assembly, you can consider Liftoff and CAT . InterProScan can give you functional annotations relatively quickly. If you are able to share your data with NCBI and your assembly is the best assembly (or if the community agrees it is otherwise preferred), the NCBI annotation team will annotate it for you using their automated pipeline. GFF3 Toolkit can be useful when working with GFF3 files, and gFACs can help with filtering, analysis, and conversion tasks. How can I learn more about annotation ? Please consider the following sources: - Review of eukaryotic genome annotation written for beginners (Yandell and Ence, 2012; doi: <a href=\"https://doi.org/10.1038/nrg3174\">10.1038/nrg3174</a>) - Review of assembly and annotation written for conservation geneticists and assuming limited understanding of bioinformatics and high-throughput sequencing (Ekblom and Wolf, 2014; doi: <a href=\"https://doi.org/10.1111/eva.12178\">10.1111/eva.12178</a>) - Review of structural and functional annotation, providing definitions and the limitations of annotation (Mudge and Harrow, 2016; doi: <a href=\"https://doi.org/10.1038/nrg.2016.119\">10.1038/nrg.2016.119</a>) - Protocol (from <a href=\"https://www.protocols.io\">protocols.io</a>) for <em>de novo</em> annotation using the <a href=\"https://yandell-lab.org/software/maker.html\">MAKER</a> pipeline. This is annotation \"in the wild\" describing actual steps taken if not the justification for them, but it is based on this <a href=\"https://weatherby.genetics.utah.edu/MAKER/wiki/index.php/MAKER_Tutorial_for_WGS_Assembly_and_Annotation_Winter_School_2018\">2018 tutorial</a> by the developers of MAKER. <a href=\"https://doi.org/10.17504/protocols.io.b3xvqpn6\">The protocol</a> was used to annotate a non-model fish genome (Pickett and Talma <em>et al.</em>, 2022; doi: <a href=\"https://doi.org/10.46471/gigabyte.44\">10.46471/gigabyte.44</a>).","title":"Genome Annotation"},{"location":"pages/5_assembly_cleanup_annotation/#annotation-with-liftoff","text":"According to the Liftoff GitHub Repository : Liftoff is a tool that accurately maps annotations in GFF or GTF between assemblies of the same, or closely-related species. Unlike current coordinate lift-over tools which require a pre-generated \"chain\" file as input, Liftoff is a standalone tool that takes two genome assemblies and a reference annotation as input and outputs an annotation of the target genome. Liftoff uses Minimap2 (Li, 2018; doi: 10.1093/bioinformatics/bty191 ) to align the gene sequences from a reference genome to the target genome. Rather than aligning whole genomes, aligning only the gene sequences allows genes to be lifted over even if there are many structural differences between the two genomes. For each gene, Liftoff finds the alignments of the exons that maximize sequence identity while preserving the transcript and gene structure. If two genes incorrectly map to overlapping loci, Liftoff determines which gene is most-likely mis-mapped, and attempts to re-map it. Liftoff can also find additional gene copies present in the target assembly that are not annotated in the reference. We will use Liftoff (doi: 10.1093/bioinformatics/btaa1016 ) to lift over the annotations from the T2T-CHM13 reference assembly to our assembly of HG002. Create directory code cd ~/obss_2023/genome_assembly/ cd liftoff-annotation Gather the necessary files What's with the *.sqlite3 file? Liftoff stores features from the GFF file in a SQLite database. The first part of any liftoff run processes the GFF file and creates the database, which by default is written to the same file location and name with a .gz appended to the end. If you experience an error at any point (e.g., failing to allocate enough memory to your job), liftoff will automatically re-create the database, even if that part of the process completed successfully previously. This SQLite database is from a previous run we did, and we\u2019ll use it in place of the GFF file to shave some time (~10 mins for a full human genome) during our run. If you run liftoff -h , you\u2019ll see that -g (to provide the input GFF file) and -db (to provide the input SQLite database) are interchangeable. We\u2019ll be using the database to save time, but we could instead use the other option and supply the GFF file. If you haven\u2019t yet viewed a GFF file, now would be a good time to check one out: less -S chm13-annotations.gff.gz Run Liftoff First create a shell script liftoff.sh with the following content: code #!/bin/bash set -euo pipefail module purge module load Liftoff/1.6.3.2-gimkl-2022a-Python-3.11.3 liftoff \\ -p 8 \\ -db chm13-annotations.gff.liftoff.sqlite3 \\ -o asm.hap1.annotations.gff \\ asm.hap1.fa \\ chm13.fa What do each of these options do? -p specifies the number of threads to use. -db specifies the location of the SQLite database of features extracted by Liftoff from the GFF file with the input annotations for the reference. -o specifies the location of the GFF file with the output annotations for the target. The two positional parameters at the end are respectively the target assembly (our HG002 assembly) and the reference assembly (T2T-CHM13). Run the following command to see all the options described in more detail: liftoff -h This will probably take ~1 hour with 8 CPUs and 48 GB of memory. You could submit it as a job; however, we will pull pre-computed results for the next step. How to submit Liftoff with sbatch code sbatch -J liftoff -N1 -n1 -c8 --mem = 48G -t 0 -02:00 -A nesi02659 -o %x.%j.log liftoff.sh To skip the run-time, we have pre-computed the output 'asm.hap1.annotations.gff' somewhere on nesi. You can access by creating a lin (i.e. a shortcut): code ln -s /nesi/nobackup/nesi02659/LRA/resources/liftoff/asm.hap1.annotations.gff . Look at the output GFF3 file code less -S asm.hap1.annotations.gff You could also explore the files in Liftoff's intermediate directory ( /nesi/nobackup/nesi02659/LRA/resources/liftoff/intermediate_files ) or the unmapped features ( /nesi/nobackup/nesi02659/LRA/resources/liftoff/unmapped_features.txt ). Visualize the annotations in a genome browser Before we visualize the annotations in IGV, it is best if we sort and index the output GFF file: code module load IGV/2.16.1 igvtools sort asm.hap1.annotations.gff asm.hap1.annotations.sorted.gff igvtools index asm.hap1.annotations.sorted.gff These operations are small enough that you should not need to submit a job for them. To open IGV and view the annotations, do the following: Open IGV Open a Virtual Desktop according to these instructions Open the Terminal Emulator application Load the IGV module and launch IGV module load IGV/2.16.1 igv.sh Load the HG002 hap1 genome (instead of the default hg19): Genome > Load Genome from File... > asm.hap1.fa Load the GFF file: File > Load from File... > asm.hap1.annotations.sorted.gff Explore What do you observe? Do you have any questions?","title":"Annotation with Liftoff"},{"location":"pages/5_assembly_cleanup_annotation/#long-readcontig-mapping","text":"We are ready to map long reads and assembly contigs to genomes. For this we need a set of tools that are different from what you might be used to using for Illumina data. You might be asking \"what's the matter with bwa-mem for long reads?\" It is slow and probably inaccurate for this application. So let's jump in to long read mappers and see what we can do with them.","title":"Long Read/Contig Mapping"},{"location":"pages/5_assembly_cleanup_annotation/#mashmap-super-fast-approximate-mapping","text":"The first thing we would like to do is to find out how our assembled genome compares to the T2T genome CHM13. If we can map our assembly quickly onto CHM13, we can answer questions like: Which contigs correspond to chr1? How many of my contigs are T2T assembled? Are we seeing any large duplications or missing regions? MashMap is a super fast mapper that is commonly used for these kinds of questions. MashMap doesn't seed and extend, it takes sequences and plays tricks with kmers. Loosely speaking if you take a sequence and get all of its kmers of a certain size and then sort those kmers, you can do a lot with just the \"smallest\" kmer (this is called a minimizer). MashMap uses a set of those smallest kmers to say: this sequence here and my query sequence share a lot of smallest kmers. The output of MashMap is an approximation of read position and identity. Let's actually use it now to align our asembly to CHM13 and we will use the results to map our contigs onto chromosomes and to try and find some T2T chromosomes in our assemblies. Create A Directory code cd ~/obss_2023/genome_assembly/ cd annotation/mashmap Link the files we need code We are going to use CHM13 v2.0 (which includes a Y chromosome) as well as haplotype 1 from our Verkko trio assembly in the file `assembly.haplotype1.fasta. Run MashMap code module purge module load MashMap/3.0.4-Miniconda3 mashmap \\ -r chm13v2.0.fa \\ -q assembly.haplotype1.fasta \\ -f one-to-one \\ --pi 95 \\ -s 100000 \\ -t 4 \\ -o asm-to-chm13.mashmap.out Take a look at the parameters for MashMap. What are -f , --pi , and -s ? --pi is the percent identity. The default value is 85 which means that mappings with 85% or more identity should be reported. -s is the segment length. MasMap will not report segments under this value. -f is the filter mode. After MashMap identifies mappings, it can go through and filter to ensure only the best mapping is reported (if we pass \"map\") or in our case that all good mappings are reported (if we pass \"one-to-one\"). Why did we use the values that we used for --pi and -s ? Since we are mapping a human assembly against another human assembly we expect the sequences to be very similar. This is why we overwrote the default value of 85 for --pi . If we were to use the default value for -s (5000) then we would likely see a lot of hits from homologous regions. This may be interesting, but if we want to know how good our assembly is extra hits will just confuse things. Once the mapping is done, let's take a look at the output. code head asm-to-chm13.mashmap.out Note that the output is similar to the Paired Alignment Format, or PAF , and has the following columns: query name length 0-based start end strand target name length start end mapping nucleotide identity (estimate) The most recent versions of MashMap actually output PAF by defalt. But not the version we are using here. Now generate a dot plot of the alignment code generateDotPlot png medium asm-to-chm13.mashmap.out And take a look at it (you can click on it in your file explorer). Does anything stick out to you ? There is no sequence in the assembly mapping to chrY. This makes sense as this is the maternal haplotype's assembly. Exercise: Can We Find Any Contigs/Scaffolds That Are T2T? Look through the dotplot to try and identify a contig or two that are assembled T2T. Once you've done that we will have to bring in another tool to make sure that there are actually telomeric repeats on the 5' and 3' ends. (Some people care about the presence of telomeric repeats when defining what T2T means.) Run the telo tool from seqtk to identify (human) telomeric ends: module load seqtk seqtk telo assembly.haplotype1.fasta > telos.bed Now look through the bed file and find the contig you are interested in to see if there are telomeres on both ends. Did you find any T2T contigs or Scaffolds ? In this assembly chromosomes 3, 10, 11, and 16 are assembled T2T with contigs (no gaps) and chr5 is assembled T2T in a scaffold. As of the writing of this document (June 2023) for a diploid human assembly you can expect to get around 12 chromosomes assembled T2T in contigs and an additional 12 or so assembled T2T in scaffolds with Verkko. Hifiasm is very similar but without scaffolds. This means that for a diploid verkko assembly about half of the chromosomes are T2T out of the box for ~40X HiFi and >15X (or so) UL ONT with either HiC or trio phasing. Pretty amazing. This sample (HG002) is not very heterozygous so it's actually a tough sample to assemble. That is why we aren't seeing very many T2T contigs/scaffolds. Sometimes you have to judge how good an assembly is on a per-sample basis. Closing notes This could all be automated, of course. The T2T and HPRC consortiums have workflows that go through assemblies to determine T2T status at the chromosomal level, but those workflows are pretty similar to what we did above, actually. Not having base-level information is actually ok for what we just did. If you would like base level information at only a few fold of the run cost of mashmap (so still very fast), then you probably want to check out wfmash . It can be used for alignments of long reads (100kb+) or assemblies at very large scales.","title":"Mashmap: Super Fast (Approximate) Mapping"},{"location":"pages/5_assembly_cleanup_annotation/#minimap2","text":"If you are mapping long read data and you'd like to have base-level alignment, minimap2 is probably your first stop. Today we are going to use minimap2 to align ONT reads that have 5mC information stored in Mm/Ml tags to our diploid assembly. Create A Directory code cd ~/obss_2023/genome_assembly/ cd annotation/minimap2 We are going to use the diploid version of our Verkko trio assembly. (This just means that the maternal and paternal haplotypes are both included in the fasta.) Create a minimap2 slurm script Open your favourite text editor code nano ont_mm2.sl And paste in the following: code #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name minimap2-ont #SBATCH --cpus-per-task 48 #SBATCH --time 05:00:00 #SBATCH --mem 128G #SBATCH --partition milan #SBATCH --output slurmlogs/%x.%j.log #SBATCH --error slurmlogs/%x.%j.err ## load modules module purge module load minimap2/2.24-GCC-11.3.0 module load SAMtools/1.16.1-GCC-11.3.0 ## Create minimap2 index of our diploid assembly minimap2 \\ -k 17 \\ -I 8G \\ -d verkko_trio_diploid.fa.mmi \\ verkko_trio_diploid.fa ## minimap parameters appropriate for nanopore in_args = \"-y -x map-ont --MD --eqx --cs -Y -L -p0.1 -a -k 17 -K 10g\" #do the mapping with methylation tags by dumping the Mm/Ml tags to the fastq headers samtools fastq \\ -TMm,Ml 03_08_22_R941_HG002_2_Guppy_6.1.2_5mc_cg_prom_sup.bam \\ | minimap2 -t 24 ${ in_args } verkko_trio_diploid.fa.mmi - \\ | samtools view -@ 24 -bh - \\ | samtools sort -@ 24 - > \\ verkko_trio_diploid.mm2.5mC.bam samtools index verkko_trio_diploid.mm2.5mC.bam And run the script Terminal sbatch ont_mm2.sl This should take only 3 hours or so, but we have some pre-baked results for you already. We will use these results in the next section. Why did we align to the diploid version of our assembly ? The traditional thing to do is to align your data to a haploid or pseudo-haploid assembly like CHM13 or GRCh38. We are diploid individuals, though. And for some use cases we want to align long reads to a diploid assembly in order to have the reads segregate by haplotype. When we are aligning a samples reads to its own assembly this is especially important.","title":"Minimap2:"},{"location":"pages/5_assembly_cleanup_annotation/#visualize-the-alignments-in-igv","text":"Before switching over to your virtual desktop, link the prerun alignments to your working directory code Now view the alignments in IGV Open an IGV window as you did above. Don't forget to module load IGV/2.16.1 (we need v 2.14 or greater) We need to use our assembly as the genome file. Do that by clicking the Genomes dropdown at the top and then select Load Genome From File . Navigate to your folder and select the Verkko trio genome that we copied in. Load the 5mC bam by clicking on the File dropdown then selecting Load From File Show the methylation predictions by right clicking on the alignment track and selecting Color alignments by , then click base modifications (5mc) Explore the data a bit Zoom in on a random region that is a few hundred basepairs in size. You can see the methylation levels in the track histogram and in the reads themselves. Red is methylated and blue is unmethylated. If you are familiar with ONT data already you know that ONT and PacBio have the ability to detect base modifications without any additional steps like bisulfite conversion. What we are adding here is the ability to look at methylation in the context of a sample's own assembly. Maybe that wouldn't matter if you are looking at regions of the genome which are structurally consistent across samples\u2014like promoters for well known genes. Take a look around mat-0000038 (this is the biggest contig that maps to chrX). Why did we choose to show you an example in chrX ? In male samples chrX is haploid, so outside of the psuedoautosomal, or PAR, regions we don't have to worry about whether or not our reads map to the correct haplotype. What would have happened if we had just aligned this data to CHM13 ? Centromeres are highly repetitive and alpha satellite arrays can vary from individual to individual in size by more than a factor of two. So aligning centromeric reads from one individual to another individual's assembly is an inherently dodgy proposition.","title":"Visualize The Alignments In IGV"},{"location":"pages/6_phased_assemblies_in_action/","text":"7. Phased Assemblies in Action \u00b6 What can you do with a phased assembly that you cannot do with a squashed assembly? Why does having a highly-continuous and highly-accurate assembly matter? In same cases, you don\u2019t need an assembly like that, e.g., if doing targeted sequencing of a region of interest that fits inside the length of a read. Conversely, some applications require a good assembly, e.g., when studying repetitive DNA, especially long and/or interspersed repeats. Phasing is also helpful when the haplotypes differ, especially when those differences are structural in nature (e.g., copy number variants or large insertions, deletions, duplications, or inversions). Let\u2019s take a look at the repeats in chromosome Y of our HG002 assembly. Visualizing Repeats using ModDotPlot \u00b6 ModDotPlot is a tool for visualizing tandem repeats using dot plots, similar to StainedGlass ( Vollger et al. 2022 ), but using sketching methods to radically reduce the computational requirements. Visualizing tandem repeats like this requires an assembly that spans all the repeat arrays and assembles them accurately, making this a potent example of the benefits of combining highly-accurate, long reads (like PacBio HiFi) with ultralong reads from ONT in the assembly process with hifiasm ( Cheng et al. 2022 ) or Verkko ( Rautiainen et al. 2023 ). Take a moment, and consider that. You can\u2019t do this with reads alone (even long reads). Mapping to a good reference (e.g., CHM13-T2T) for your species of interest (if one exists) won\u2019t work either because the alignment software can\u2019t distinguish between repeat copies. What are sketching methods ? Sketching is a technique to create reduced-representations of a sequence. The most widely-known option for sketching is probably minimizers, made particularly popular with tools like minimap2 ( Li 2018 ), which applies minimizers to the alignment problem. We discussed minimizers earlier when running MashMap. The minimizer for any given window is the lexicographically smallest canonical k-mer. Several variants to minimizers exist, e.g., syncmers, minmers, and modimizers, the latter of which is used in ModDotPlot. Each sketching method has different properties depending on how they select the subsequence used to represent a larger area, whether they allow overlaps, whether a certain density of representative sequences is enforced in any given window, whether the neighboring windows are dependent on eachother, etc. In general, the representative sequences are found by sliding along the sequence and selecting a representative subsequence in the given window. Many other tools use sketching in some way, here are a few examples: Mash ( Ondov et al. 2016 ) MashMap ( Kille et al. 2023 ) MBG ( Rautiainen & Marschall 2021 ) <-- Used in Verkko ( Rautiainen et al. 2023 )! hifiasm ( Cheng et al. 2022 ) We're going to run ModDotPlot on part of the Y chromosome from our earlier assembly. First we\u2019ll need to identify the appropriate chunk, which can be done a few different ways depending on the sequence you\u2019re looking for. ChrY somtimes appears tied on only one end to chrX in the Bandage plot, making it easy to identify the Y node (which is shorter than the longer X node). This method may not get all of the Y chromosome because it may be in separate pieces (i.e., it isn\u2019t T2T), and we often see chrX and chrY untangled from eachother in one or two pieces each, so topology of the graph isn\u2019t always a reliable option (though, we recommend labelling the graph with the chromosome names to get a feel for how your assembly looks). The ideal way is to map some known sequence against your assembly or to map your assembly against a known reference. Since we\u2019re using data from HG002 (a human), we can map against the human CHM13-T2T reference. Initial setup \u00b6 Make a directory to work in code cd ~/obss_2023/genome_assembly/ mkdir moddotplot cd moddotplot Get the files code ln -s /nesi/project/nesi02659/obss_2023/resources/genome_assembly/chm13v2.0.fa chm13.fa ln -s /nesi/project/nesi02659/obss_2023/resources/genome_assembly/assembly.haplotype2.fasta hg002.hap2.fa ln -s /nesi/project/nesi02659/obss_2023/resources/genome_assembly/assembly.haplotype2.fasta.fai hg002.hap2.fa.fai Note that we\u2019re cheating a bit. We could map against the entire diploid assembly, but we\u2019ve already determined that chrY is in haplotype2, so we\u2019ll save some time and work with only that haplotype. Find chrY contigs with MashMap \u00b6 Do the alignments We\u2019ll do the alignments with MashMap ( Kille et al. 2023 ). This should take ~25 minutes with 4 CPUs and should use <4 GB RAM. With 16 CPUs, it should take ~4 minutes and will use <8 GB RAM. This command is the same as the one you ran earlier, except that this time it is on haplotype2 and the percent identity threshold is lower, which should help us recruit alignments in the satellites on chrY. We\u2019ll use pre-computed results to avoid needing to wait for the job to complete, but this is what the command would look like: view, but do not run this code module purge module load MashMap/3.0.4-Miniconda3 mashmap -f \"one-to-one\" \\ -k 16 --pi 90 \\ -s 100000 -t 4 \\ -r chm13.fa \\ -q hg002.hap2.fa \\ -o hg2hap2-x-chm13.ssv How would I submit this as a job? Submit it as a job with sbatch . First copy the command into a script named mashmap.sh : #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name mashmap #SBATCH --cpus-per-task 16 #SBATCH --time 00:20:00 #SBATCH --mem 8G #SBATCH --partition milan #SBATCH --output slurmlogs/%x.%j.out #SBATCH --error slurmlogs/%x.%j.err module purge module load MashMap/3.0.4-Miniconda3 mashmap -f \"one-to-one\" \\ -k 16 --pi 90 \\ -s 100000 -t 16 \\ -r chm13.fa \\ -q hg002.hap2.fa \\ -o hg2hap2-x-chm13.ssv Then submit the job with the following command: code sbatch mashmap.sh Get the pre-computed results code cp /nesi/nobackup/nesi02659/LRA/resources/hap2-mashmap/hg2hap2-x-chm13.ssv ./ cp /nesi/nobackup/nesi02659/LRA/resources/hap2-mashmap/mashmap.log ./ View the output file code less -S hg2hap2-x-chm13.ssv There is more information present than we need, and we can simplify things by looking at long alignments only. Optionally, view the MashMap log file code less -S mashmap.log Subset the alignments code awk 'BEGIN{FS=\" \"; OFS=FS}{if($6 == \"chrY\" && ($4-$3) >= 1000000){print $0}}' \\ < hg2hap2-x-chm13.ssv \\ > hg2hap2-x-chm13.gt1m-chrY.ssv What is the awk command doing ? This awk command is keeping only alignments (remember, one alignment is on each line of the file) that map to chrY; the sixth column is the \"reference\" or \"target\" sequence name. The third and fourth columns are respectively the start and stop positions of the aligned region on the \"query\" sequence (i.e., a contig from our assembly); thus, we\u2019re keeping only alignments that are 1 Mbp or longer. This also has the consequence of ignoring contigs that are shorter than 1 Mbp. Wait, what are each of the columns again? According to the MashMap README The output is space-delimited with each line consisting of query name, length, 0-based start, end, strand, target name, length, start, end, and mapping nucleotide identity. Re-written as a numbered list: MashMap output columns query name length 0-based start end strand target name length start end mapping nucleotide identity View the output file Now that we\u2019ve culled the alignments, viewing them should be much easier: code less -S hg2hap2-x-chm13.gt1m-chrY.ssv Which contigs belong to chrY? Answer pat-0000724 , pat-0000725 , and pat-0000727 are probably chrY. Others may be as well, but it is difficult to tell without a more refined investigation, and this is sufficient for our purposes. How can we tell? Anything with a long, high-identity alignment is a pretty good candidate, especially if the contig was determined to be paternal using trio markers (as these were). One thing that may help is seeing what percentage of the contig is covered by each alignment: code awk 'BEGIN{FS=\" \"; OFS=\"\\t\"; print \"Contig\", \"Length\", \"Percent Identity\", \"Percent Aligned\"}{print $1, $2, $10 \"%\", ($4-$3)/$2*100 \"%\"}' \\ hg2hap2-x-chm13.gt1m-chrY.ssv \\ | column -ts $'\\t' \\ > hg2hap2-x-chm13.gt1m-chrY.annotated.txt less -S hg2hap2-x-chm13.gt1m-chrY.annotated.txt To prevent you from having to do the math in your head, here are the percentages of each contig aligned to chrY (sum of the final column in the hg2hap2-x-chm13.gt1m-chrY.annotated.txt file): HG002 Contig % aligned to chrY pat-0000724 84.332% pat-0000725 99.9999% pat-0000727 98.342% More short contigs may also belong to chrY, but we would need to investigate more carefully to find them. We would want to confirm whether telomeres are present on the end of the terminal contigs. Also, we do not expect perfect or complete alignments here; not only are we using a sketching-based alignment method, but any aligner would likely struggle when aligning repetitve DNA. So, how much of chrY did we capture with these three contigs? You can see that these three contigs cover the majority, if not all, of chrY: Create self dot plots for each contig \u00b6 ModDotPlot is still in development, and it cannot currently support doing multiple self comparisons at one time. We will need to create separate fasta files for our contigs of interest. Create and index subset fastas code module load SAMtools/1.16.1-GCC-11.3.0 for CTG in pat-000072 { 4 ,5,7 } do samtools faidx hg002.hap2.fa ${ CTG } > hg002.hap2. ${ CTG } .fa samtools faidx hg002.hap2. ${ CTG } .fa done Run ModDotPlot On sequences of this size, ModDotPlot is relatively quick. It has a reasonable memory footprint for sequences <10 Mbp, but memory usage can exceed 20 GB for large sequences (>100 Mbp). Let's create a script to submit with sbatch . Paste the following into moddotplot.sh code #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name moddotplot #SBATCH --cpus-per-task 4 #SBATCH --time 00:10:00 #SBATCH --mem 8G #SBATCH --partition milan #SBATCH --output slurmlogs/%x.%j.log module purge module load ModDotPlot/2023-06-gimkl-2022a-Python-3.11.3 for CTG in pat-000072 { 4 ,5,7 } do moddotplot \\ -k 21 -id 85 \\ -i hg002.hap2. ${ CTG } .fa \\ -o mdp_hg002- ${ CTG } done Then submit with sbatch : code sbatch moddotplot.sh What do these parameters do ? You can run moddotplot -h to find out (and enjoy some excellent ASCII art). Here are the options we used: Required input: -i INPUT [INPUT ...], --input INPUT [INPUT ...] Path to input fasta file(s) Mod.Plot distance matrix commands: -k KMER, --kmer KMER k-mer length. Must be < 32 (default: 21) -id IDENTITY, --identity IDENTITY Identity cutoff threshold. (default: 80) -o OUTPUT, --output OUTPUT Name for bed file and plots. Will be set to input fasta file name if not provided. (default: None) Inspect the output files First, take a look at the log file: code less -S slurmlogs/moddotplot.*.log Then note that for every run, we created _HIST.png and _TRI.png files. The HIST files show the distribution of the Percent Identity of the alignments. The TRI files show everything above (or below, depending on how you look at it) the diagonal of the dotplot, rotated such that the diagonal is along the X-axis of the plot. Go ahead a view these files now. What do you observe?","title":"7. Phased Assemblies in Action"},{"location":"pages/6_phased_assemblies_in_action/#7-phased-assemblies-in-action","text":"What can you do with a phased assembly that you cannot do with a squashed assembly? Why does having a highly-continuous and highly-accurate assembly matter? In same cases, you don\u2019t need an assembly like that, e.g., if doing targeted sequencing of a region of interest that fits inside the length of a read. Conversely, some applications require a good assembly, e.g., when studying repetitive DNA, especially long and/or interspersed repeats. Phasing is also helpful when the haplotypes differ, especially when those differences are structural in nature (e.g., copy number variants or large insertions, deletions, duplications, or inversions). Let\u2019s take a look at the repeats in chromosome Y of our HG002 assembly.","title":"7. Phased Assemblies in Action"},{"location":"pages/6_phased_assemblies_in_action/#visualizing-repeats-using-moddotplot","text":"ModDotPlot is a tool for visualizing tandem repeats using dot plots, similar to StainedGlass ( Vollger et al. 2022 ), but using sketching methods to radically reduce the computational requirements. Visualizing tandem repeats like this requires an assembly that spans all the repeat arrays and assembles them accurately, making this a potent example of the benefits of combining highly-accurate, long reads (like PacBio HiFi) with ultralong reads from ONT in the assembly process with hifiasm ( Cheng et al. 2022 ) or Verkko ( Rautiainen et al. 2023 ). Take a moment, and consider that. You can\u2019t do this with reads alone (even long reads). Mapping to a good reference (e.g., CHM13-T2T) for your species of interest (if one exists) won\u2019t work either because the alignment software can\u2019t distinguish between repeat copies. What are sketching methods ? Sketching is a technique to create reduced-representations of a sequence. The most widely-known option for sketching is probably minimizers, made particularly popular with tools like minimap2 ( Li 2018 ), which applies minimizers to the alignment problem. We discussed minimizers earlier when running MashMap. The minimizer for any given window is the lexicographically smallest canonical k-mer. Several variants to minimizers exist, e.g., syncmers, minmers, and modimizers, the latter of which is used in ModDotPlot. Each sketching method has different properties depending on how they select the subsequence used to represent a larger area, whether they allow overlaps, whether a certain density of representative sequences is enforced in any given window, whether the neighboring windows are dependent on eachother, etc. In general, the representative sequences are found by sliding along the sequence and selecting a representative subsequence in the given window. Many other tools use sketching in some way, here are a few examples: Mash ( Ondov et al. 2016 ) MashMap ( Kille et al. 2023 ) MBG ( Rautiainen & Marschall 2021 ) <-- Used in Verkko ( Rautiainen et al. 2023 )! hifiasm ( Cheng et al. 2022 ) We're going to run ModDotPlot on part of the Y chromosome from our earlier assembly. First we\u2019ll need to identify the appropriate chunk, which can be done a few different ways depending on the sequence you\u2019re looking for. ChrY somtimes appears tied on only one end to chrX in the Bandage plot, making it easy to identify the Y node (which is shorter than the longer X node). This method may not get all of the Y chromosome because it may be in separate pieces (i.e., it isn\u2019t T2T), and we often see chrX and chrY untangled from eachother in one or two pieces each, so topology of the graph isn\u2019t always a reliable option (though, we recommend labelling the graph with the chromosome names to get a feel for how your assembly looks). The ideal way is to map some known sequence against your assembly or to map your assembly against a known reference. Since we\u2019re using data from HG002 (a human), we can map against the human CHM13-T2T reference.","title":"Visualizing Repeats using ModDotPlot"},{"location":"pages/6_phased_assemblies_in_action/#initial-setup","text":"Make a directory to work in code cd ~/obss_2023/genome_assembly/ mkdir moddotplot cd moddotplot Get the files code ln -s /nesi/project/nesi02659/obss_2023/resources/genome_assembly/chm13v2.0.fa chm13.fa ln -s /nesi/project/nesi02659/obss_2023/resources/genome_assembly/assembly.haplotype2.fasta hg002.hap2.fa ln -s /nesi/project/nesi02659/obss_2023/resources/genome_assembly/assembly.haplotype2.fasta.fai hg002.hap2.fa.fai Note that we\u2019re cheating a bit. We could map against the entire diploid assembly, but we\u2019ve already determined that chrY is in haplotype2, so we\u2019ll save some time and work with only that haplotype.","title":"Initial setup"},{"location":"pages/6_phased_assemblies_in_action/#find-chry-contigs-with-mashmap","text":"Do the alignments We\u2019ll do the alignments with MashMap ( Kille et al. 2023 ). This should take ~25 minutes with 4 CPUs and should use <4 GB RAM. With 16 CPUs, it should take ~4 minutes and will use <8 GB RAM. This command is the same as the one you ran earlier, except that this time it is on haplotype2 and the percent identity threshold is lower, which should help us recruit alignments in the satellites on chrY. We\u2019ll use pre-computed results to avoid needing to wait for the job to complete, but this is what the command would look like: view, but do not run this code module purge module load MashMap/3.0.4-Miniconda3 mashmap -f \"one-to-one\" \\ -k 16 --pi 90 \\ -s 100000 -t 4 \\ -r chm13.fa \\ -q hg002.hap2.fa \\ -o hg2hap2-x-chm13.ssv How would I submit this as a job? Submit it as a job with sbatch . First copy the command into a script named mashmap.sh : #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name mashmap #SBATCH --cpus-per-task 16 #SBATCH --time 00:20:00 #SBATCH --mem 8G #SBATCH --partition milan #SBATCH --output slurmlogs/%x.%j.out #SBATCH --error slurmlogs/%x.%j.err module purge module load MashMap/3.0.4-Miniconda3 mashmap -f \"one-to-one\" \\ -k 16 --pi 90 \\ -s 100000 -t 16 \\ -r chm13.fa \\ -q hg002.hap2.fa \\ -o hg2hap2-x-chm13.ssv Then submit the job with the following command: code sbatch mashmap.sh Get the pre-computed results code cp /nesi/nobackup/nesi02659/LRA/resources/hap2-mashmap/hg2hap2-x-chm13.ssv ./ cp /nesi/nobackup/nesi02659/LRA/resources/hap2-mashmap/mashmap.log ./ View the output file code less -S hg2hap2-x-chm13.ssv There is more information present than we need, and we can simplify things by looking at long alignments only. Optionally, view the MashMap log file code less -S mashmap.log Subset the alignments code awk 'BEGIN{FS=\" \"; OFS=FS}{if($6 == \"chrY\" && ($4-$3) >= 1000000){print $0}}' \\ < hg2hap2-x-chm13.ssv \\ > hg2hap2-x-chm13.gt1m-chrY.ssv What is the awk command doing ? This awk command is keeping only alignments (remember, one alignment is on each line of the file) that map to chrY; the sixth column is the \"reference\" or \"target\" sequence name. The third and fourth columns are respectively the start and stop positions of the aligned region on the \"query\" sequence (i.e., a contig from our assembly); thus, we\u2019re keeping only alignments that are 1 Mbp or longer. This also has the consequence of ignoring contigs that are shorter than 1 Mbp. Wait, what are each of the columns again? According to the MashMap README The output is space-delimited with each line consisting of query name, length, 0-based start, end, strand, target name, length, start, end, and mapping nucleotide identity. Re-written as a numbered list: MashMap output columns query name length 0-based start end strand target name length start end mapping nucleotide identity View the output file Now that we\u2019ve culled the alignments, viewing them should be much easier: code less -S hg2hap2-x-chm13.gt1m-chrY.ssv Which contigs belong to chrY? Answer pat-0000724 , pat-0000725 , and pat-0000727 are probably chrY. Others may be as well, but it is difficult to tell without a more refined investigation, and this is sufficient for our purposes. How can we tell? Anything with a long, high-identity alignment is a pretty good candidate, especially if the contig was determined to be paternal using trio markers (as these were). One thing that may help is seeing what percentage of the contig is covered by each alignment: code awk 'BEGIN{FS=\" \"; OFS=\"\\t\"; print \"Contig\", \"Length\", \"Percent Identity\", \"Percent Aligned\"}{print $1, $2, $10 \"%\", ($4-$3)/$2*100 \"%\"}' \\ hg2hap2-x-chm13.gt1m-chrY.ssv \\ | column -ts $'\\t' \\ > hg2hap2-x-chm13.gt1m-chrY.annotated.txt less -S hg2hap2-x-chm13.gt1m-chrY.annotated.txt To prevent you from having to do the math in your head, here are the percentages of each contig aligned to chrY (sum of the final column in the hg2hap2-x-chm13.gt1m-chrY.annotated.txt file): HG002 Contig % aligned to chrY pat-0000724 84.332% pat-0000725 99.9999% pat-0000727 98.342% More short contigs may also belong to chrY, but we would need to investigate more carefully to find them. We would want to confirm whether telomeres are present on the end of the terminal contigs. Also, we do not expect perfect or complete alignments here; not only are we using a sketching-based alignment method, but any aligner would likely struggle when aligning repetitve DNA. So, how much of chrY did we capture with these three contigs? You can see that these three contigs cover the majority, if not all, of chrY:","title":"Find chrY contigs with MashMap"},{"location":"pages/6_phased_assemblies_in_action/#create-self-dot-plots-for-each-contig","text":"ModDotPlot is still in development, and it cannot currently support doing multiple self comparisons at one time. We will need to create separate fasta files for our contigs of interest. Create and index subset fastas code module load SAMtools/1.16.1-GCC-11.3.0 for CTG in pat-000072 { 4 ,5,7 } do samtools faidx hg002.hap2.fa ${ CTG } > hg002.hap2. ${ CTG } .fa samtools faidx hg002.hap2. ${ CTG } .fa done Run ModDotPlot On sequences of this size, ModDotPlot is relatively quick. It has a reasonable memory footprint for sequences <10 Mbp, but memory usage can exceed 20 GB for large sequences (>100 Mbp). Let's create a script to submit with sbatch . Paste the following into moddotplot.sh code #!/bin/bash -e #SBATCH --account nesi02659 #SBATCH --job-name moddotplot #SBATCH --cpus-per-task 4 #SBATCH --time 00:10:00 #SBATCH --mem 8G #SBATCH --partition milan #SBATCH --output slurmlogs/%x.%j.log module purge module load ModDotPlot/2023-06-gimkl-2022a-Python-3.11.3 for CTG in pat-000072 { 4 ,5,7 } do moddotplot \\ -k 21 -id 85 \\ -i hg002.hap2. ${ CTG } .fa \\ -o mdp_hg002- ${ CTG } done Then submit with sbatch : code sbatch moddotplot.sh What do these parameters do ? You can run moddotplot -h to find out (and enjoy some excellent ASCII art). Here are the options we used: Required input: -i INPUT [INPUT ...], --input INPUT [INPUT ...] Path to input fasta file(s) Mod.Plot distance matrix commands: -k KMER, --kmer KMER k-mer length. Must be < 32 (default: 21) -id IDENTITY, --identity IDENTITY Identity cutoff threshold. (default: 80) -o OUTPUT, --output OUTPUT Name for bed file and plots. Will be set to input fasta file name if not provided. (default: None) Inspect the output files First, take a look at the log file: code less -S slurmlogs/moddotplot.*.log Then note that for every run, we created _HIST.png and _TRI.png files. The HIST files show the distribution of the Percent Identity of the alignments. The TRI files show everything above (or below, depending on how you look at it) the diagonal of the dotplot, rotated such that the diagonal is along the X-axis of the plot. Go ahead a view these files now. What do you observe?","title":"Create self dot plots for each contig"},{"location":"supplementary/supplementary_1/","text":"NeSI HPC Auth.Factors Setup & Jupyter Login \u00b6 1. Set HPC Password Done via https://my.nesi.org.nz/login 2. Set HPC Second Factor Done via https://my.nesi.org.nz/login 3. Jupyter Login Details Follow https://jupyter.nesi.org.nz/hub/login Enter NeSI username, HPC password and 6 digit second factor token Choose server options as below >>make sure to choose the correct project code nesi02659 , number of CPUs CPUs=4 , memory 8 GB prior to pressing button. 4. Reset HPC Password Done via https://my.nesi.org.nz/login 5. ReSet HPC Second Factor Done via https://my.nesi.org.nz/login","title":"NeSI HPC Auth.Factors Setup & Jupyter Login"},{"location":"supplementary/supplementary_1/#nesi-hpc-authfactors-setup-jupyter-login","text":"1. Set HPC Password Done via https://my.nesi.org.nz/login 2. Set HPC Second Factor Done via https://my.nesi.org.nz/login 3. Jupyter Login Details Follow https://jupyter.nesi.org.nz/hub/login Enter NeSI username, HPC password and 6 digit second factor token Choose server options as below >>make sure to choose the correct project code nesi02659 , number of CPUs CPUs=4 , memory 8 GB prior to pressing button. 4. Reset HPC Password Done via https://my.nesi.org.nz/login 5. ReSet HPC Second Factor Done via https://my.nesi.org.nz/login","title":"NeSI HPC Auth.Factors Setup &amp; Jupyter Login"},{"location":"supplementary/supplementary_2/","text":"NeSI File system, Working directory and Symlinks \u00b6 The part of the operating system responsible for managing files and directories is called the file system . It organizes our data into files, which hold information, and directories (also called \u2018folders\u2019), which hold files or other directories. Directories are like places \u2014 at any time while we are using the shell, we are in exactly one place called our current working directory. Commands mostly read and write files in the current working directory, i.e. \u2018here\u2019, so knowing where you are before running a command is important. NeSI Filesystem (For Researchers) All HPC platforms have custom File Systems for the purpose of general use and admin . NeSI Filesystem looks like above This may look a bit obscure but thing of them as different labels for some familiar names such as Desktop, Downloads, Documents /home/username is for user-specific files such as configuration files, environment setup, source code, etc. This will be the default landing file system during a login /nesi/project/projectcode is for persistent project-related data, project-related software, etc /nesi/nobackup/projectode is a 'scratch space', for data you don't need to keep long term. Old data is periodically deleted from nobackup projectode for this event is nesi02659 . If you are to open a NeSI project for your own research, it will have a unique project code with a prefix to represent your affiliated institute and a five digit number (randomly generated). Therefore, full path to persistent and nobackup/scratch file systems will be in the format of /nesi/project/nesi02659 /nesi/nobackup/nesi02659 Symlinks (shortcuts ?) \u00b6 All of the Long-read assembly material will be hosted on either /nesi/nobackup/nesi02659 or /nesi/project/nesi02659 file systems. Also, each one of the attendee has an individual working space in /nesi/project/nesi02659/lra/users/ . Although this is great in everyway, having to type the full path to access this particular path (or having to remember it) from the default login site ( /home filesystem) can be a tedious task. Therefore, we recommend creating a Symbolic link to your individual working directory from /home Think of it as a shortcut from your Desktop \ud83d\ude42 This is already done for you \ud83d\ude0a - Creating a symlink from /home/$USER to project(persistent) - Log into the NeSI Jupyter service as per S.1.1 : NeSI Mahuika Jupyter login in NeSI Setup Supplementary material and open a terminal session Let's call the symlink (shortcut) lra Following command will create the lra symlink from your /home directory to individual working directory in /nesi/project/nesi02659/lra/users/ ln -s /nesi/project/nesi02659/lra/users/ $USER ~/lra Now, you can access your working directory with cd ~/lra Run pwd to check the current working directory pwd commands on symlinks will print the \"relative path\" (location from where we are, rather than from the root of the file system) with respect to \"absolute path\" (entire path from the root directory) Run the command realpath /path/you/want/to/know to show the absolute path of the symlink. i..e realpath ~/lra OR if you are already at the symlinked path cd ~/lra realpath . Summary Jupyter File explorer \u00b6 Guide File Explorer to correct working directory Jupyter terminal and file explorer (on left) operate independently of each other. Therefore, changing the directory via terminal to your individual directory will not change the default working directory in explorer. Changing it to your individual directories can be done will couple of click","title":"NeSI File system, Working directory and Symlinks"},{"location":"supplementary/supplementary_2/#nesi-file-system-working-directory-and-symlinks","text":"The part of the operating system responsible for managing files and directories is called the file system . It organizes our data into files, which hold information, and directories (also called \u2018folders\u2019), which hold files or other directories. Directories are like places \u2014 at any time while we are using the shell, we are in exactly one place called our current working directory. Commands mostly read and write files in the current working directory, i.e. \u2018here\u2019, so knowing where you are before running a command is important. NeSI Filesystem (For Researchers) All HPC platforms have custom File Systems for the purpose of general use and admin . NeSI Filesystem looks like above This may look a bit obscure but thing of them as different labels for some familiar names such as Desktop, Downloads, Documents /home/username is for user-specific files such as configuration files, environment setup, source code, etc. This will be the default landing file system during a login /nesi/project/projectcode is for persistent project-related data, project-related software, etc /nesi/nobackup/projectode is a 'scratch space', for data you don't need to keep long term. Old data is periodically deleted from nobackup projectode for this event is nesi02659 . If you are to open a NeSI project for your own research, it will have a unique project code with a prefix to represent your affiliated institute and a five digit number (randomly generated). Therefore, full path to persistent and nobackup/scratch file systems will be in the format of /nesi/project/nesi02659 /nesi/nobackup/nesi02659","title":"NeSI File system, Working directory and Symlinks"},{"location":"supplementary/supplementary_2/#symlinks-shortcuts","text":"All of the Long-read assembly material will be hosted on either /nesi/nobackup/nesi02659 or /nesi/project/nesi02659 file systems. Also, each one of the attendee has an individual working space in /nesi/project/nesi02659/lra/users/ . Although this is great in everyway, having to type the full path to access this particular path (or having to remember it) from the default login site ( /home filesystem) can be a tedious task. Therefore, we recommend creating a Symbolic link to your individual working directory from /home Think of it as a shortcut from your Desktop \ud83d\ude42 This is already done for you \ud83d\ude0a - Creating a symlink from /home/$USER to project(persistent) - Log into the NeSI Jupyter service as per S.1.1 : NeSI Mahuika Jupyter login in NeSI Setup Supplementary material and open a terminal session Let's call the symlink (shortcut) lra Following command will create the lra symlink from your /home directory to individual working directory in /nesi/project/nesi02659/lra/users/ ln -s /nesi/project/nesi02659/lra/users/ $USER ~/lra Now, you can access your working directory with cd ~/lra Run pwd to check the current working directory pwd commands on symlinks will print the \"relative path\" (location from where we are, rather than from the root of the file system) with respect to \"absolute path\" (entire path from the root directory) Run the command realpath /path/you/want/to/know to show the absolute path of the symlink. i..e realpath ~/lra OR if you are already at the symlinked path cd ~/lra realpath . Summary","title":"Symlinks (shortcuts ?)"},{"location":"supplementary/supplementary_2/#jupyter-file-explorer","text":"Guide File Explorer to correct working directory Jupyter terminal and file explorer (on left) operate independently of each other. Therefore, changing the directory via terminal to your individual directory will not change the default working directory in explorer. Changing it to your individual directories can be done will couple of click","title":"Jupyter File explorer"},{"location":"supplementary/supplementary_3/","text":"Jupyter Virtual Desktop \u00b6 Open a Virtual Desktop from JupyterHub \u00b6 How to open a Virtual Desktop Click blue + button on top left corner and then the Virtual Desktop icon Troubleshooting \u00b6 In an instance where Virtual Desktop failed to connect If you are to encounter following during Virtual Desktop launch Restart Jupyter as below and try again please","title":"Jupyter Virtual Desktop"},{"location":"supplementary/supplementary_3/#jupyter-virtual-desktop","text":"","title":"Jupyter Virtual Desktop"},{"location":"supplementary/supplementary_3/#open-a-virtual-desktop-from-jupyterhub","text":"How to open a Virtual Desktop Click blue + button on top left corner and then the Virtual Desktop icon","title":"Open a Virtual Desktop from JupyterHub"},{"location":"supplementary/supplementary_3/#troubleshooting","text":"In an instance where Virtual Desktop failed to connect If you are to encounter following during Virtual Desktop launch Restart Jupyter as below and try again please","title":"Troubleshooting"},{"location":"supplementary/supplementary_4/","text":"Additional materials \u00b6 Please find below additional information about approaches to phasing and ONT methods . Phasing and Approaches to Phasing \u00b6 Methods for Oxford Nanopore Sequencing \u00b6","title":"Additional materials"},{"location":"supplementary/supplementary_4/#additional-materials","text":"Please find below additional information about approaches to phasing and ONT methods .","title":"Additional materials"},{"location":"supplementary/supplementary_4/#phasing-and-approaches-to-phasing","text":"","title":"Phasing and Approaches to Phasing"},{"location":"supplementary/supplementary_4/#methods-for-oxford-nanopore-sequencing","text":"","title":"Methods for Oxford Nanopore Sequencing"}]}